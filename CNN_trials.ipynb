{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtezcan/anaconda3/lib/python3.6/site-packages/torchsample-0.1.2-py3.6.egg/torchsample/datasets.py:16: UserWarning: Cant import nibabel.. Cant load brain images\n"
     ]
    }
   ],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchsample\n",
    "from torchsample import transforms as ts_transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image\n",
    "from tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import importlib\n",
    "\n",
    "from torchsample.transforms import RangeNorm\n",
    "\n",
    "import functions.fine_tune as ft\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "---------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5']\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training \n",
    "# Just normalization for validation\n",
    "#uniform_sampler=False\n",
    "batch_size=32\n",
    "split=1000\n",
    "random_seed=1\n",
    "shuffle=True\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        #ts_transforms.RandomRotate(30)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '../bin_images/0to5'\n",
    "dset_train = datasets.ImageFolder(data_dir, data_transforms['train'])\n",
    "dset_val = datasets.ImageFolder(data_dir, data_transforms['val'])\n",
    "\n",
    "num_train = len(dset_train)\n",
    "indices = list(range(num_train))\n",
    "\n",
    "if shuffle == True:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "sampler_train = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n",
    "sampler_val = torch.utils.data.sampler.SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "dset_loaders = {'train':torch.utils.data.DataLoader(dset_train, batch_size=batch_size,sampler=sampler_train,\n",
    "                                                    num_workers=12),\n",
    "                'val':torch.utils.data.DataLoader(dset_val, batch_size=batch_size,sampler=sampler_val,\n",
    "                                                    num_workers=12)}\n",
    "'''if(uniform_sampler):\n",
    "    weights,wpc = ft.make_weights_for_balanced_classes(dsets['train'].imgs, len(dsets['train'].classes))  \n",
    "    weights = torch.DoubleTensor(weights) \n",
    "    sampler = {'train':torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) ,\n",
    "               'val':None}\n",
    "else:\n",
    "    sampler = {'train':None,\n",
    "               'val':None}'''\n",
    "\n",
    "'''dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "\n",
    "print(dset_classes)'''\n",
    "\n",
    "dset_sizes={'train':len(dset_train)-1000,'val':1000}\n",
    "dset_classes = dset_train.classes\n",
    "print(dset_classes)\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "#use_gpu=False\n",
    "if use_gpu:\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    print('!!!!! NO CUDA GPUS DETECTED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(dset_loaders['train'])*batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a few images\n",
    "^^^^^^^^^^^^^^^^^^^^^^\n",
    "Let's visualize a few training images so as to understand the data\n",
    "augmentations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "inputs, classes = next(iter(dset_loaders['train']))\n",
    "#print(classes.cpu().numpy().reshape(4,4)+1)\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs,nrow=4)\n",
    "print('Size of the input tensors in one batch after grid is  '+str(out.size()))\n",
    "plt.figure(figsize=(12,12))\n",
    "ft.imshow(out, title=[dset_classes[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning the convnet\n",
    "----------------------\n",
    "\n",
    "Load a pretrained model and reset final fully connected layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def writeLog(logname):\n",
    "    f=open('runs/'+logname+'/Network_properties.txt','w')\n",
    "    f.write('Batch size: '+str(batch_size)+'\\n')\n",
    "    f.write('Validation size: '+str(split)+'\\n')\n",
    "    f.write('Random seed: '+str(random_seed)+'\\n')\n",
    "    f.write('Shuffle: '+str(shuffle)+'\\n')\n",
    "    f.write('Validation size: '+str(split)+'\\n')\n",
    "    f.write('Network: '+network+'\\n')\n",
    "    f.write('Criterion: '+criteria+'\\n')\n",
    "    f.write('Learning rate: '+str(lr)+'\\n')\n",
    "    f.write('Momentum: '+str(momentum)+'\\n')\n",
    "    f.write('Leraning Rate Scheduler: '+str(lr_scheduler)+'\\n')\n",
    "    f.write('Leraning Rate Decay Period: '+str(lr_decay_epoch)+'\\n')\n",
    "    f.write('Network is pretrained: '+str(pretrained)+'\\n')\n",
    "    f.write('Network laoded from: '+networkName+'\\n')\n",
    "    f.write('MSE loss function: '+str(mse_loss)+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import time\n",
    "\n",
    "\n",
    "'''iter_loc=12\n",
    "book = openpyxl.load_workbook('logs.xlsx')\n",
    "sheet = book.active\n",
    "sheet.append((1,2,3,4,5,6,7,8,9))\n",
    "n=sheet.max_row\n",
    "sheet.cell(row=n,column=7).value=123\n",
    "\n",
    "book.save('logs.xlsx')'''\n",
    "\n",
    "def writeLog_xlsx(logname='logs.xlsx',iter_loc=14):\n",
    "    book = openpyxl.load_workbook(logname)\n",
    "    sheet = book.active\n",
    "    if network=='loaded':\n",
    "        specs=(datetime.now().strftime('%B%d  %H:%M:%S'),networkName,str(split),str(random_seed),str(shuffle),\n",
    "               optimizer, criteria,str(lr),str(momentum),str(lr_scheduler),str(lr_decay_epoch),str(pretrained),\n",
    "               str(batch_size))\n",
    "    else:\n",
    "        specs=(datetime.now().strftime('%B%d  %H:%M:%S'),network,str(split),str(random_seed),str(shuffle),\n",
    "               optimizer, criteria,str(lr),str(momentum),str(lr_scheduler),str(lr_decay_epoch),str(pretrained),\n",
    "               str(batch_size))\n",
    "    sheet.append(specs)\n",
    "    current_row = sheet.max_row\n",
    "    sheet.cell(row=current_row, column=iter_loc+5).value = comment\n",
    "    #n=sheet.max_row\n",
    "    #sheet.cell(row=n,column=7).value=123\n",
    "    book.save(logname)\n",
    "#writeLog_xlsx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comment=' ' #'Tested for three rooms'\n",
    "network='resnet18'\n",
    "networkName='resnet18_real_sgd_multisoft_August29  19:06:27'\n",
    "optimizer='sgd'\n",
    "criteria='multisoft'\n",
    "iter_loc=14\n",
    "end_to_end=True\n",
    "lr=0.01\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "lr_scheduler=ft.exp_lr_scheduler\n",
    "lr_decay_epoch=10\n",
    "pretrained=True\n",
    "mse_loss=False\n",
    "nclasses=6\n",
    "\n",
    "def network_loader(comment=comment, #'Tested for three rooms'\n",
    "                    network=network,\n",
    "                    networkName=networkName,\n",
    "                    optimizer=optimizer,\n",
    "                    criteria=criteria,\n",
    "                    iter_loc=iter_loc,\n",
    "                    end_to_end=end_to_end,\n",
    "                    lr=lr,\n",
    "                    momentum=momentum,\n",
    "                    weight_decay=weight_decay,\n",
    "                    lr_scheduler=lr_scheduler,\n",
    "                    lr_decay_epoch=lr_decay_epoch,\n",
    "                    pretrained=pretrained,\n",
    "                    mse_loss=mse_loss,\n",
    "                    nclasses=nclasses):\n",
    "\n",
    "    if(criteria=='crossentropy'):\n",
    "        multilabel=False\n",
    "    else:\n",
    "        multilabel=True\n",
    "\n",
    "    if(network=='resnet18'):\n",
    "        model_ft = models.resnet18(pretrained=pretrained)\n",
    "        if not end_to_end:\n",
    "            for param in model_ft.parameters():\n",
    "                param.requires_grad = False \n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        if(mse_loss):\n",
    "            model_ft.fc = nn.Linear(num_ftrs, 1)\n",
    "        else:    \n",
    "            model_ft.fc = nn.Linear(num_ftrs, nclasses)\n",
    "    elif(network=='resnet34'):\n",
    "        model_ft = models.resnet34(pretrained=pretrained)\n",
    "        if not end_to_end:\n",
    "            for param in model_ft.parameters():\n",
    "                param.requires_grad = False \n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        if(mse_loss):\n",
    "            model_ft.fc = nn.Linear(num_ftrs, 1)\n",
    "        else:    \n",
    "            model_ft.fc = nn.Linear(num_ftrs, nclasses)\n",
    "    elif(network=='resnet50'):\n",
    "        model_ft = models.resnet50(pretrained=pretrained)\n",
    "        if not end_to_end:\n",
    "            for param in model_ft.parameters():\n",
    "                param.requires_grad = False \n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        if(mse_loss):\n",
    "            model_ft.fc = nn.Linear(num_ftrs, 1)\n",
    "        else:    \n",
    "            model_ft.fc = nn.Linear(num_ftrs, nclasses)\n",
    "    elif(network=='resnet101'):\n",
    "        model_ft = models.resnet101(pretrained=pretrained)\n",
    "        if not end_to_end:\n",
    "            for param in model_ft.parameters():\n",
    "                param.requires_grad = False \n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        if(mse_loss):\n",
    "            model_ft.fc = nn.Linear(num_ftrs, 1)\n",
    "        else:    \n",
    "            model_ft.fc = nn.Linear(num_ftrs, nclasses)\n",
    "    elif(network=='alexnet'):\n",
    "        model_ft = models.alexnet(pretrained=pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].out_features\n",
    "        setattr(model_ft.classifier, '7', nn.ReLU(inplace=True))\n",
    "        setattr(model_ft.classifier, '8', nn.Dropout())\n",
    "        setattr(model_ft.classifier, '9', nn.Linear(num_ftrs,nclasses))\n",
    "    elif(network=='loaded'):\n",
    "        model_ft = torch.load('./saved_models/'+networkName)\n",
    "        if not end_to_end:\n",
    "            for param in model_ft.parameters():\n",
    "                param.requires_grad = False \n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, nclasses)\n",
    "    else:\n",
    "        raise ValueError('Undefined network '+network)\n",
    "\n",
    "    if use_gpu:\n",
    "        model_ft = model_ft.cuda()\n",
    "\n",
    "    if(criteria=='crossentropy'):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif(criteria=='multisoft'):\n",
    "        criterion=nn.MultiLabelSoftMarginLoss()\n",
    "    else:\n",
    "        raise ValueError('Undefined criteria '+criteria)\n",
    "\n",
    "    if(optimizer=='adam'):\n",
    "        optimizer_ft = optim.Adam(model_ft.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "    elif(optimizer=='sgd'):\n",
    "        if(end_to_end):\n",
    "            optimizer_ft = optim.SGD(model_ft.parameters(), lr=lr, momentum=momentum)#,weight_decay=weight_decay)\n",
    "        else:\n",
    "            optimizer_ft = optim.SGD(model_ft.fc.parameters(), lr=lr, momentum=momentum,weight_decay=weight_decay)\n",
    "    return model_ft, optimizer_ft, criterion, multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(ft)\n",
    "\n",
    "def run_network():\n",
    "    model_ft, optimizer_ft, criterion, multilabel = network_loader(comment=comment, #'Tested for three rooms'\n",
    "                                                                    network=network,\n",
    "                                                                    networkName=networkName,\n",
    "                                                                    optimizer=optimizer,\n",
    "                                                                    criteria=criteria,\n",
    "                                                                    iter_loc=iter_loc,\n",
    "                                                                    end_to_end=end_to_end,\n",
    "                                                                    lr=lr,\n",
    "                                                                    momentum=momentum,\n",
    "                                                                    weight_decay=weight_decay,\n",
    "                                                                    lr_scheduler=lr_scheduler,\n",
    "                                                                    lr_decay_epoch=lr_decay_epoch,\n",
    "                                                                    pretrained=pretrained,\n",
    "                                                                    mse_loss=mse_loss,\n",
    "                                                                    nclasses=nclasses)\n",
    "    logname=network+'_'+'_'+optimizer+'_'+criteria+'_'+datetime.now().strftime('%B%d  %H:%M:%S')\n",
    "    writer = SummaryWriter('runs/'+logname)\n",
    "    writeLog(logname)\n",
    "    writeLog_xlsx()\n",
    "    '''model_ft = ft.train_model(model_ft, criterion, optimizer_ft, lr_scheduler,dset_loaders,dset_sizes,writer,\n",
    "                            use_gpu=use_gpu,num_epochs=50,batch_size=batch_size,num_log=50,multilabel=multilabel,\n",
    "                              multi_prob=False,lr_decay_epoch=lr_decay_epoch,init_lr=lr,mse_loss=False,\n",
    "                              iter_loc=iter_loc)'''\n",
    "\n",
    "    model_ft = ft.train_model(model_ft, criterion, optimizer_ft, lr_scheduler,dset_loaders,dset_sizes,writer,\n",
    "                            use_gpu=use_gpu,num_epochs=30,batch_size=batch_size,num_log=100,multilabel=multilabel,\n",
    "                              multi_prob=False,lr_decay_epoch=lr_decay_epoch,init_lr=lr,mse_loss=mse_loss,\n",
    "                              iter_loc=iter_loc)\n",
    "    torch.save(model_ft,'./saved_models/'+logname)\n",
    "    del model_ft\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "LR is set to 0.01\n",
      "100/182, acc: 0.2284, CIR-1: 0.6238, RMSE: 1.6095\n",
      "train Loss: 0.0569 Acc: 0.2491 CIR-1: 0.6680 RMSE 1.5221\n",
      "val Loss: 0.0515 Acc: 0.2830 CIR-1: 0.7150 RMSE 1.3065\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "100/182, acc: 0.2628, CIR-1: 0.6963, RMSE: 1.4360\n"
     ]
    }
   ],
   "source": [
    "mse_loss=False\n",
    "criteria='crossentropy'\n",
    "optimizer='adam'\n",
    "network='resnet18'\n",
    "momentum=.9\n",
    "lr=0.01\n",
    "run_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer='sgd'\n",
    "network='loaded'\n",
    "networkName='resnet18_real_sgd_multisoft_August29  19:06:27'\n",
    "momentum=.9\n",
    "for learning in [ 0.01, 0.03, 0.05, 0.07, 0.1]:\n",
    "    lr=learning\n",
    "    run_network()\n",
    "    \n",
    "optimizer='sgd'\n",
    "network='resnet18'\n",
    "momentum=.9\n",
    "for learning in [0.005, 0.01, 0.03, 0.05, 0.07, 0.1]:\n",
    "    lr=learning\n",
    "    run_network()\n",
    "    \n",
    "criteria='crossentropy'\n",
    "network='loaded'\n",
    "networkName='resnet18_real_sgd_multisoft_August29  19:06:27'\n",
    "momentum=.9\n",
    "for learning in [0.005, 0.01, 0.03, 0.05, 0.07, 0.1]:\n",
    "    lr=learning\n",
    "    run_network()\n",
    "    \n",
    "optimizer='sgd'\n",
    "network='resnet18'\n",
    "momentum=.9\n",
    "for learning in [0.005, 0.01, 0.03, 0.05, 0.07, 0.1]:\n",
    "    lr=learning\n",
    "    run_network()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer='sgd'\n",
    "network='loaded'\n",
    "networkName='resnet18_real_sgd_multisoft_August29  19:06:27'\n",
    "momentum=.9\n",
    "for learning in [0.005, 0.01, 0.03, 0.05, 0.07, 0.1]:\n",
    "    lr=learning\n",
    "    run_network()\n",
    "    \n",
    "optimizer='sgd'\n",
    "momentum=.9\n",
    "for learning in [0.005, 0.01, 0.03, 0.05, 0.07, 0.1]:\n",
    "    lr=learning\n",
    "    run_network()\n",
    "    \n",
    "    \n",
    "'''\n",
    "optimizer='sgd'\n",
    "network='loaded'\n",
    "networkName='resnet18_real_sgd_multisoft_August29  19:06:27'\n",
    "momentum=.9\n",
    "for learning in [0.005, 0.01, 0.03, 0.05, 0.07, 0.1]:\n",
    "    lr=learning\n",
    "    run_network()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer='sgd'\n",
    "momentum=.9\n",
    "network='loaded'\n",
    "networkName='resnet18_real_sgd_multisoft_August29  19:06:27'\n",
    "\n",
    "for learning in [0.001, 0.005, 0.01, 0.03, 0.05, 0.07, 0.1]:\n",
    "    lr=learning\n",
    "    run_network()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_ft,'./saved_models/'+logname)\n",
    "model_ft_backup=model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Trials\n",
    "model_ft=torch.load('./saved_models/resnet18_real_sgd_multisoft_August29  19:06:27')\n",
    "model_ft.train(False)\n",
    "running_cir1=0\n",
    "for data in dset_loaders['val']:\n",
    "    inputs, labels=data\n",
    "    if use_gpu:\n",
    "        inputs, labels = Variable(inputs.cuda()), \\\n",
    "            Variable(labels.cuda())\n",
    "    else:\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "    outputs = model_ft(inputs)\n",
    "    #tanh_step=torch.nn.Tanh()\n",
    "    #outputs=tanh_step(outputs)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    #print(preds)\n",
    "    running_cir1 += torch.sum(torch.abs(preds - labels.data)<=1)\n",
    "print(running_cir1/dset_sizes['val'])\n",
    "print(dset_sizes['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "imdir='/home/mtezcan/Documents/test_img/1/JPEG.jpg'\n",
    "im=im = Image.open(imdir)\n",
    "im_numpy=data_transforms['val'](im).numpy()\n",
    "#print(im_numpy.shape)\n",
    "dsets = datasets.ImageFolder_mtezcan(['//home//mtezcan//Documents//test_img'], data_transforms['val'])\n",
    "\n",
    "inputs, classes = next(iter(dsets))\n",
    "inputs=inputs.unsqueeze(0)\n",
    "inputs=Variable(inputs.cuda())\n",
    "print(inputs.size())\n",
    "print(inputs)\n",
    "outputs = model_ft(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model_ft=torch.load('./Obsolete/saved_models/resnet18_multi_88_real_7_15_17.mdl')\n",
    "model_ft=torch.load('./saved_models/'+logname)\n",
    "if use_gpu:\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion=nn.MultiLabelSoftMarginLoss()\n",
    "# Observe that all parameters are being optimized\n",
    "#optimizer_ft = optim.Adam(model_ft.parameters())\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "model_ft = model_ft.cuda()\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "# --- to-be-optimized ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(next(iter(dset_loaders['train']))[0])\n",
    "#model_ft = models.alexnet(pretrained=True)\n",
    "\n",
    "model_ft=model_ft.cpu()\n",
    "model_ft.train(False)\n",
    "#new_classifier = nn.Sequential(*list(model_ft.classifier.children())[:-5])\n",
    "#model_ft.classifier = new_classifier\n",
    "#print(model_ft)\n",
    "model_params= list(model_ft.children())\n",
    "#model_params[1]=list(model_params[1])\n",
    "#print(model_params)\n",
    "new_ft = nn.Sequential(*list(model_params)[:-1])\n",
    "#print(new_ft)\n",
    "\n",
    "fvec_tr=np.zeros((20000,512))\n",
    "label_tr=np.zeros((20000))\n",
    "\n",
    "fvec_val=np.zeros((20000,512))\n",
    "label_val=np.zeros((20000))\n",
    "count=0;\n",
    "\n",
    "#inputs_t, classes_t = data=next(iter(dset_loaders['train']))\n",
    "#print(inputs_t.size())\n",
    "#fvec_t=new_ft(Variable(inputs_t))\n",
    "\n",
    "for data in dset_loaders['train']:\n",
    "    inputs_t, classes_t = data\n",
    "    fvec_t=new_ft(Variable(inputs_t))\n",
    "    #print(fvec_t)\n",
    "    fvec_t_cpu=fvec_t.cpu()\n",
    "    if(fvec_t_cpu.data.numpy().shape[0]==4):\n",
    "        fvec_tr[count:count+4,:]=fvec_t_cpu.data.numpy().reshape(4,-1)\n",
    "        label_tr[count:count+4]=classes_t.short().numpy()\n",
    "        count +=4\n",
    "fvec_tr=fvec_tr[:count,:]\n",
    "label_tr=label_tr[:count]\n",
    "\n",
    "\n",
    "count=0;\n",
    "for data in dset_loaders['val']:\n",
    "    inputs_t, classes_t = data\n",
    "    fvec_t=new_ft(Variable(inputs_t))\n",
    "    fvec_t_cpu=fvec_t.cpu()\n",
    "    if(fvec_t_cpu.data.numpy().shape[0]==4):\n",
    "        fvec_val[count:count+4,:]=fvec_t_cpu.data.numpy().reshape(4,-1)\n",
    "        label_val[count:count+4]=classes_t.short().numpy()\n",
    "        count +=4\n",
    "    \n",
    "fvec_val=fvec_val[:count,:]\n",
    "label_val=label_val[:count]\n",
    "\n",
    "'''\n",
    "print('The CNN model is')\n",
    "print(model_ft)\n",
    "num_ftrs = model_ft.classifier[6].in_features\n",
    "print('Number of features in the fine tune layer is '+str(num_ftrs))\n",
    "setattr(model_ft.classifier, '6', nn.Linear(num_ftrs, 9))\n",
    "#model_ft.classifier['6'] = nn.Linear(num_ftrs, 2)\n",
    "print(model_ft)\n",
    "'''\n",
    "\n",
    "print('Size of the input in training is '+str(fvec_tr.shape))\n",
    "print('Size of the labels in training is '+str(label_tr.shape))\n",
    "\n",
    "print('Size of the input in validation is '+str(fvec_val.shape))\n",
    "print('Size of the labels in validation is '+str(label_val.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(fvec_val.shape)\n",
    "print(fvec_val[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "#clf = SVC(C=1,kernel='linear',class_weight='balanced',decision_function_shape='ovo')\n",
    "clf=OneVsOneClassifier(LinearSVC(C=1e-5))\n",
    "clf.fit(fvec_tr,label_tr)\n",
    "label_pred=clf.predict(fvec_val)\n",
    "abs_err=np.abs(label_pred-label_val)\n",
    "cir1=np.mean(abs_err<=1)\n",
    "print(cir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "c_val=np.power(2,np.arange(0.,8.,1.))\n",
    "g_val=np.power(2,np.arange(-10.,-5.,1.))\n",
    "\n",
    "#c_val=[128]\n",
    "#g_val=[np.power(2,-10.)]\n",
    "cir1_max=0.\n",
    "c_max=0\n",
    "g_max=0\n",
    "for c_1 in c_val:\n",
    "    for g in g_val:\n",
    "        \n",
    "        clf_svc = SVC(C=c_1,kernel='rbf',gamma=g,shrinking=False,class_weight='balanced')\n",
    "        clf=OneVsOneClassifier(clf_svc)\n",
    "        #print(label_tr)\n",
    "        clf.fit(fvec_tr,label_tr)\n",
    "\n",
    "        label_pred=clf.predict(fvec_val)\n",
    "        #print(label_pred)\n",
    "        abs_err=np.abs(label_pred-label_val)\n",
    "        #print(abs_err)\n",
    "        cir1=np.mean(abs_err<=1)\n",
    "        print('C = '+str(c_1),', g = ',str(g),', cir-1 = '+str(cir1))\n",
    "        if(cir1>cir1_max):\n",
    "            cir1_max=cir1\n",
    "            c_max=c_1\n",
    "            g_max=g\n",
    "            \n",
    "\n",
    "clf = SVC(C=c_max,kernel='rbf',gamma=g_max)#,class_weight='balanced')\n",
    "clf.fit(fvec_tr,label_tr)\n",
    "label_pred=clf.predict(fvec_val)\n",
    "abs_err=np.abs(label_pred-label_val)\n",
    "print('Max CIR-1 achieved is '+str(np.mean(abs_err<=1)) +' with c='+str(c_max),', g='+str(g_max))\n",
    "print('CIR-0 = '+str(np.mean(abs_err<=0)))\n",
    "print('CIR-2 = '+str(np.mean(abs_err<=2)))\n",
    "tr_pred=clf.predict(fvec_tr)\n",
    "abs_tr=np.abs(tr_pred-label_tr)\n",
    "print('Training CIR-1 is '+str(np.mean(abs_tr<=1)))\n",
    "#print(abs_err)\n",
    "#print(label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "#print('The CNN model is')\n",
    "#print(model_ft)\n",
    "#print(list(model_ft.children())[:-1])\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "print('Number of features in the fine tune layer is '+str(num_ftrs))\n",
    "#setattr(model_ft.classifier, '6', nn.Linear(num_ftrs, 9))\n",
    "model_ft.fc = nn.Linear(num_ftrs, 9)\n",
    "#print(model_ft)\n",
    "\n",
    "#print(next(iter(dset_loaders['train']))[0])\n",
    "new_ft = nn.Sequential(*list(model_ft.children())[:-1])\n",
    "#dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=4,\n",
    "#                                               shuffle=True, num_workers=4)\n",
    "#                for x in ['train', 'val']}\n",
    "\n",
    "fvec_tr=np.zeros((X_tr.shape[0],512))\n",
    "for k in range(0,10,10):#(0,inputs.shape[0],10):\n",
    "    print(k)\n",
    "    fvec_now=new_ft(Variable(torch.from_numpy(X_tr[k:k+10,:,:,:])))\n",
    "    print(fvec_now.numpy())\n",
    "\n",
    "print(fvec)\n",
    "\n",
    "\n",
    "'''\n",
    "print('The CNN model is')\n",
    "print(model_ft)\n",
    "num_ftrs = model_ft.classifier[6].in_features\n",
    "print('Number of features in the fine tune layer is '+str(num_ftrs))\n",
    "setattr(model_ft.classifier, '6', nn.Linear(num_ftrs, 9))\n",
    "#model_ft.classifier['6'] = nn.Linear(num_ftrs, 2)\n",
    "print(model_ft)\n",
    "'''\n",
    "\n",
    "if use_gpu:\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate\n",
    "^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "It should take around 15-25 min on CPU. On GPU though, it takes less than a\n",
    "minute.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNet as fixed feature extractor\n",
    "----------------------------------\n",
    "\n",
    "Here, we need to freeze all the network except the final layer. We need\n",
    "to set ``requires_grad == False`` to freeze the parameters so that the\n",
    "gradients are not computed in ``backward()``.\n",
    "\n",
    "You can read more about this in the documentation\n",
    "`here <http://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward>`__.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    #print(param)\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in model_conv.fc.parameters():\n",
    "    #print(param)\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 9)\n",
    "\n",
    "if use_gpu:\n",
    "    model_conv = model_conv.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opoosed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate\n",
    "^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "On CPU this will take about half the time compared to previous scenario.\n",
    "This is expected as gradients don't need to be computed for most of the\n",
    "network. However, forward does need to be computed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_conv = ft.train_model(model_conv, criterion, optimizer_conv, ft.exp_lr_scheduler,dset_loaders,dset_sizes,writer,\n",
    "                        use_gpu=use_gpu,num_epochs=100,batch_size=32,num_log=1000,multilabel=False,multi_prob=False,\n",
    "                         lr_decay_epoch=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_model(model_conv)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
