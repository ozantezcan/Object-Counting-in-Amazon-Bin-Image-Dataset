{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchsample\n",
    "from torchsample import transforms as ts_transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "import importlib\n",
    "\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "num_classes=5\n",
    "\n",
    "\n",
    "#from torchsample.transforms import RangeNorm\n",
    "\n",
    "import functions.fine_tune as ft\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f0a2e905b00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAD8CAYAAAAylrwMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGAjYAQMGDMiIgYAdMGDAgIz43zi9YYgE0yNZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0aa41cf5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_classes = 10\n",
    "dim = 2\n",
    "\n",
    "def generateLinearData(num_samples = 10000, num_classes = num_classes, dim = dim, bound = 5, sigma_noise = .1,rand_label = False):\n",
    "    \n",
    "    if  rand_label:\n",
    "        rand_classes = np.random.permutation(num_classes)\n",
    "    else:\n",
    "        rand_classes = np.arange(num_classes)\n",
    "        \n",
    "    fvec = np.random.rand(dim, num_samples)*bound*2-bound\n",
    "    label = np.dot((np.random.rand(1,dim)*bound*2-bound).reshape(1,-1),fvec)\n",
    "\n",
    "    sorted_idx = np.argsort(label)\n",
    "    bin_size = label.shape[1]/num_classes\n",
    "\n",
    "    for k in range(0, num_classes):\n",
    "        label[0, sorted_idx[0, np.floor(k*bin_size).astype(int):np.floor((k+1)*bin_size).astype(int)]] = rand_classes[k]\n",
    "\n",
    "    label = label.astype(np.int)\n",
    "    n = sigma_noise * np.random.randn(dim, num_samples)\n",
    "    fvec = fvec + n\n",
    "    \n",
    "    return fvec.T, label.reshape(label.shape[1])\n",
    "\n",
    "fvec, label = generateLinearData(rand_label = False)\n",
    "plt.scatter(fvec[:, 0], fvec[:, 1], c=label)\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateCircularData(num_samples = 10000, num_classes = num_classes, dim = dim,\n",
    "                         bound = 5, sigma_noise = .1, rand_label = False):\n",
    "    \n",
    "    if  rand_label:\n",
    "        rand_classes = np.random.permutation(num_classes)\n",
    "    else:\n",
    "        rand_classes = np.arange(num_classes)\n",
    "        \n",
    "    fvec = np.random.rand(dim, num_samples)*bound*2-bound\n",
    "    \n",
    "    fvec_l = np.sum(fvec**2, axis = 0).reshape(1,-1)\n",
    "    print(fvec_l.shape)\n",
    "    label = fvec_l\n",
    "\n",
    "    sorted_idx = np.argsort(label)\n",
    "    bin_size = label.shape[1]/num_classes\n",
    "\n",
    "    for k in range(0, num_classes):\n",
    "        label[0, sorted_idx[0, np.floor(k*bin_size).astype(int):np.floor((k+1)*bin_size).astype(int)]] = rand_classes[k]\n",
    "\n",
    "    label = label.astype(np.int)\n",
    "    n = sigma_noise * np.random.randn(dim, num_samples)\n",
    "    fvec = fvec + n\n",
    "    \n",
    "    return fvec.T, label.reshape(label.shape[1])\n",
    "\n",
    "fvec, label = generateCircularData(num_classes =5, sigma_noise = 0, rand_label = False)\n",
    "plt.scatter(fvec[:, 0], fvec[:, 1], c=label, cmap = plt.get_cmap('Greys'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generateSpiralData(num_samples = 10000, num_classes = 9, dim = 2,\n",
    "                         bound = 1, sigma_noise = .1, rand_label = False):\n",
    "    \n",
    "    if  rand_label:\n",
    "        rand_classes = np.random.permutation(num_classes)\n",
    "    else:\n",
    "        rand_classes = np.arange(num_classes)\n",
    "    \n",
    "    #rand_classes = [1, 1.5, -1, -1.5]\n",
    "    sample_per_class = int(num_samples/num_classes)\n",
    "    num_samples = sample_per_class*num_classes\n",
    "    fvec = np.zeros((dim, sample_per_class*num_classes))\n",
    "    label = np.zeros((1, sample_per_class*num_classes))\n",
    "    \n",
    "    t = np.linspace(0, 10, sample_per_class)\n",
    "    x = t * np.cos(t)\n",
    "    y = t * np.sin(t)\n",
    "    x = x.reshape(1, -1)\n",
    "    y = y.reshape(1, -1)\n",
    "\n",
    "    cons = .7\n",
    "    for k in range(0, num_classes):\n",
    "        r = np.linspace(0.05, 1, sample_per_class)\n",
    "        t = np.linspace(k*cons, (k+6)*cons, sample_per_class)\n",
    "        x = np.cos(t)\n",
    "        y = np.sin(t)\n",
    "        x = x.reshape(1, -1)\n",
    "        y = y.reshape(1, -1)\n",
    "        label[0, k*sample_per_class:(k+1)*sample_per_class] = rand_classes[k]\n",
    "        fvec[0, k*sample_per_class:(k+1)*sample_per_class] = bound * x * r\n",
    "        fvec[1, k*sample_per_class:(k+1)*sample_per_class] = bound * y * r\n",
    "\n",
    "    label = label.astype(np.int)\n",
    "    n = sigma_noise * np.random.randn(dim, num_samples)\n",
    "    fvec = fvec + n\n",
    "    \n",
    "    return fvec.T, label.reshape(label.shape[1])\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "X, y = generateSpiralData(num_classes = 9, sigma_noise = 0.01, rand_label = False)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap = plt.get_cmap('Set1'))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generateSpiralData(num_samples = 10000, num_classes = num_classes, dim = dim,\n",
    "                         bound = 5, sigma_noise = .1, rand_label = False):\n",
    "    \n",
    "    if  rand_label:\n",
    "        rand_classes = np.random.permutation(num_classes)\n",
    "    else:\n",
    "        rand_classes = np.arange(num_classes)\n",
    "    \n",
    "    #rand_classes = [1, 1.5, -1, -1.5]\n",
    "\n",
    "    cons = 4\n",
    "    N = num_samples # number of points per class\n",
    "    D = dim # dimensionality\n",
    "    K = num_classes # number of classes\n",
    "\n",
    "    X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "    y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "    for j in range(K):\n",
    "      ix = range(N*j,N*(j+1))\n",
    "      r = np.linspace(0.0,1,N) # radius\n",
    "      t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*sigma_noise # theta\n",
    "      X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "      y[ix] = j\n",
    "    \n",
    "    label = y.astype(np.int)\n",
    "    fvec = X\n",
    "    \n",
    "    return fvec, label\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.subplot(121)\n",
    "fvec, label = generateCircularData(num_classes =9, sigma_noise = 0, rand_label = False)\n",
    "plt.scatter(fvec[:, 0], fvec[:, 1], c=label, cmap = plt.get_cmap('Set1'))\n",
    "plt.colorbar()\n",
    "plt.subplot(122)\n",
    "fvec, label = generateSpiralData(num_classes = 3, sigma_noise = 0, rand_label = False)\n",
    "plt.scatter(fvec[:, 0], fvec[:, 1], c=label, cmap = plt.get_cmap('Set1'))\n",
    "plt.colorbar()\n",
    "#plt.savefig('circular_vs_spiral.tiff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinal Regression Benchmark Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "num_bins=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank32nh.data\n",
      "bank8FM.data\n",
      "bostonhousing\n",
      "cal_housing.data\n",
      "cpu_act.data\n",
      "cpu_small.data\n",
      "house_16H.data\n",
      "house_8L.data\n",
      "housing\n",
      "results.csv\n",
      "stock\n",
      "stocksdomain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./dataset/regression\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 0)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-35211db07a8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"feat\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./dataset/regression/housing\", sep=',', header=None)\n",
    "train_df=train_df.drop(train_df.columns[-1],axis=1)\n",
    "print(train_df.shape)\n",
    "\n",
    "columns=[\"feat\"+str(k) for k in range(train_df.shape[1])]\n",
    "columns[-1]=\"label\"\n",
    "train_df.columns=columns\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"./dataset/regression/housing\", sep='\\s+', header=None)\n",
    "train_df = pd.read_csv(\"./dataset/regression/cal_housing.data\", sep=',', header=None)\n",
    "#train_df=train_df.drop(train_df.columns[-1],axis=1)\n",
    "\n",
    "columns=[\"feat\"+str(k) for k in range(train_df.shape[1])]\n",
    "columns[-1]=\"label\"\n",
    "train_df.columns=columns\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat0</th>\n",
       "      <th>feat1</th>\n",
       "      <th>feat2</th>\n",
       "      <th>feat3</th>\n",
       "      <th>feat4</th>\n",
       "      <th>feat5</th>\n",
       "      <th>feat6</th>\n",
       "      <th>feat7</th>\n",
       "      <th>feat8</th>\n",
       "      <th>feat9</th>\n",
       "      <th>feat10</th>\n",
       "      <th>feat11</th>\n",
       "      <th>feat12</th>\n",
       "      <th>feat13</th>\n",
       "      <th>feat14</th>\n",
       "      <th>feat15</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15512.0</td>\n",
       "      <td>0.460869</td>\n",
       "      <td>0.049252</td>\n",
       "      <td>0.226470</td>\n",
       "      <td>0.149827</td>\n",
       "      <td>0.752837</td>\n",
       "      <td>0.010057</td>\n",
       "      <td>0.579729</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.075912</td>\n",
       "      <td>0.625318</td>\n",
       "      <td>0.036613</td>\n",
       "      <td>0.991377</td>\n",
       "      <td>0.260116</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>0.774059</td>\n",
       "      <td>130600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1550.0</td>\n",
       "      <td>0.470968</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>0.137419</td>\n",
       "      <td>0.096341</td>\n",
       "      <td>0.862581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.695142</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.043551</td>\n",
       "      <td>0.064263</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.994975</td>\n",
       "      <td>0.285266</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>40500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4741.0</td>\n",
       "      <td>0.485341</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.189412</td>\n",
       "      <td>0.135656</td>\n",
       "      <td>0.856992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683584</td>\n",
       "      <td>0.004143</td>\n",
       "      <td>0.027965</td>\n",
       "      <td>0.065796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997411</td>\n",
       "      <td>0.315433</td>\n",
       "      <td>0.065116</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>28700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>467.0</td>\n",
       "      <td>0.498929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100642</td>\n",
       "      <td>0.085470</td>\n",
       "      <td>0.907923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.018293</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>310.0</td>\n",
       "      <td>0.474194</td>\n",
       "      <td>0.680645</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.128834</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.756302</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.077519</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.147287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat0     feat1     feat2     feat3     feat4     feat5     feat6  \\\n",
       "0  15512.0  0.460869  0.049252  0.226470  0.149827  0.752837  0.010057   \n",
       "1   1550.0  0.470968  0.002581  0.137419  0.096341  0.862581  0.000000   \n",
       "2   4741.0  0.485341  0.000211  0.189412  0.135656  0.856992  0.000000   \n",
       "3    467.0  0.498929  0.000000  0.100642  0.085470  0.907923  0.000000   \n",
       "4    310.0  0.474194  0.680645  0.225806  0.128834  0.896774  0.000000   \n",
       "\n",
       "      feat7     feat8     feat9    feat10    feat11    feat12    feat13  \\\n",
       "0  0.579729  0.003251  0.075912  0.625318  0.036613  0.991377  0.260116   \n",
       "1  0.695142  0.005025  0.043551  0.064263  0.003350  0.994975  0.285266   \n",
       "2  0.683584  0.004143  0.027965  0.065796  0.000000  0.997411  0.315433   \n",
       "3  0.780488  0.006098  0.018293  0.057471  0.000000  1.000000  0.149425   \n",
       "4  0.756302  0.008403  0.016807  0.077519  0.672269  0.991597  0.147287   \n",
       "\n",
       "     feat14    feat15     label  \n",
       "0  0.052246  0.774059  130600.0  \n",
       "1  0.060606  0.142857   40500.0  \n",
       "2  0.065116  0.687500   28700.0  \n",
       "3  0.139535  1.000000   28500.0  \n",
       "4  0.000000  0.000000   24100.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df['label_ord']=train_df['label']\n",
    "label=train_df.label.values\n",
    "sorted_idx=np.argsort(train_df.label.values)\n",
    "num_samples_per_class=train_df.shape[0]/num_bins\n",
    "print('Number of Samples per class is ' + str(num_samples_per_class))\n",
    "bins=[(k*1e-4+label[sorted_idx[np.round(k*num_samples_per_class-1).astype(np.int)]]) for k in range(1,num_bins+1)]\n",
    "bins.insert(0,0.0)\n",
    "print(bins)\n",
    "bins[-1]=bins[-1]+1\n",
    "print(bins)\n",
    "\n",
    "label_ord=label.copy()\n",
    "k = 10\n",
    "\n",
    "print(label[sorted_idx[np.round(k*num_samples_per_class-1).astype(np.int)]])\n",
    "for k in range(num_bins):\n",
    "    #print(np.all([label>=bins[k], label<bins[k+1]],0))\n",
    "    label_ord[np.all([label>=bins[k], label<bins[k+1]],0)]=k\n",
    "    \n",
    "print('Unique labels are ' + str(np.unique(label_ord)))\n",
    "\n",
    "\n",
    "train_df['label_ord']=label_ord\n",
    "#print(train_df.head())\n",
    "\n",
    "plt.scatter(label,label_ord)\n",
    "plt.xlabel('Original Label')\n",
    "plt.ylabel('Ordinal Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat0</th>\n",
       "      <th>feat1</th>\n",
       "      <th>feat2</th>\n",
       "      <th>feat3</th>\n",
       "      <th>feat4</th>\n",
       "      <th>feat5</th>\n",
       "      <th>feat6</th>\n",
       "      <th>feat7</th>\n",
       "      <th>feat8</th>\n",
       "      <th>feat9</th>\n",
       "      <th>feat10</th>\n",
       "      <th>feat11</th>\n",
       "      <th>feat12</th>\n",
       "      <th>feat13</th>\n",
       "      <th>feat14</th>\n",
       "      <th>feat15</th>\n",
       "      <th>label</th>\n",
       "      <th>label_ord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15512.0</td>\n",
       "      <td>0.460869</td>\n",
       "      <td>0.049252</td>\n",
       "      <td>0.226470</td>\n",
       "      <td>0.149827</td>\n",
       "      <td>0.752837</td>\n",
       "      <td>0.010057</td>\n",
       "      <td>0.579729</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.075912</td>\n",
       "      <td>0.625318</td>\n",
       "      <td>0.036613</td>\n",
       "      <td>0.991377</td>\n",
       "      <td>0.260116</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>0.774059</td>\n",
       "      <td>130600.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1550.0</td>\n",
       "      <td>0.470968</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>0.137419</td>\n",
       "      <td>0.096341</td>\n",
       "      <td>0.862581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.695142</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.043551</td>\n",
       "      <td>0.064263</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.994975</td>\n",
       "      <td>0.285266</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>40500.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4741.0</td>\n",
       "      <td>0.485341</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.189412</td>\n",
       "      <td>0.135656</td>\n",
       "      <td>0.856992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683584</td>\n",
       "      <td>0.004143</td>\n",
       "      <td>0.027965</td>\n",
       "      <td>0.065796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997411</td>\n",
       "      <td>0.315433</td>\n",
       "      <td>0.065116</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>28700.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>467.0</td>\n",
       "      <td>0.498929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100642</td>\n",
       "      <td>0.085470</td>\n",
       "      <td>0.907923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.018293</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28500.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>310.0</td>\n",
       "      <td>0.474194</td>\n",
       "      <td>0.680645</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.128834</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.756302</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.077519</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.147287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24100.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat0     feat1     feat2     feat3     feat4     feat5     feat6  \\\n",
       "0  15512.0  0.460869  0.049252  0.226470  0.149827  0.752837  0.010057   \n",
       "1   1550.0  0.470968  0.002581  0.137419  0.096341  0.862581  0.000000   \n",
       "2   4741.0  0.485341  0.000211  0.189412  0.135656  0.856992  0.000000   \n",
       "3    467.0  0.498929  0.000000  0.100642  0.085470  0.907923  0.000000   \n",
       "4    310.0  0.474194  0.680645  0.225806  0.128834  0.896774  0.000000   \n",
       "\n",
       "      feat7     feat8     feat9    feat10    feat11    feat12    feat13  \\\n",
       "0  0.579729  0.003251  0.075912  0.625318  0.036613  0.991377  0.260116   \n",
       "1  0.695142  0.005025  0.043551  0.064263  0.003350  0.994975  0.285266   \n",
       "2  0.683584  0.004143  0.027965  0.065796  0.000000  0.997411  0.315433   \n",
       "3  0.780488  0.006098  0.018293  0.057471  0.000000  1.000000  0.149425   \n",
       "4  0.756302  0.008403  0.016807  0.077519  0.672269  0.991597  0.147287   \n",
       "\n",
       "     feat14    feat15     label  label_ord  \n",
       "0  0.052246  0.774059  130600.0        9.0  \n",
       "1  0.060606  0.142857   40500.0        6.0  \n",
       "2  0.065116  0.687500   28700.0        4.0  \n",
       "3  0.139535  1.000000   28500.0        4.0  \n",
       "4  0.000000  0.000000   24100.0        3.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(train_df.shape[0]), label[sorted_idx],s=3,\n",
    "            c=np.sort(label_ord[sorted_idx]), cmap = plt.get_cmap('tab10'))\n",
    "plt.colorbar()\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('Label', fontsize=12)\n",
    "\n",
    "plt.savefig('cpu_act.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ulimit = np.percentile(train_df.label.values, 98)\n",
    "llimit = np.percentile(train_df.label.values, 2)\n",
    "train_df['label'].ix[train_df['label']>ulimit] = ulimit\n",
    "train_df['label'].ix[train_df['label']<llimit] = llimit'''\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.distplot(train_df.label.values, bins=50, kde=False)\n",
    "plt.xlabel('label', fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_coeff(n, metric, lmbda = 1):\n",
    "    if metric is 'ccr':\n",
    "        return [1]\n",
    "    elif metric is 'ccr1':\n",
    "        return [1, 1, 1]\n",
    "    elif metric is 'mae':\n",
    "        coeff = np.arange(1,n)/(n-1)\n",
    "    elif metric is 'mse':\n",
    "        coeff = np.zeros(n-1)\n",
    "        coeff[0] = 2*n-3\n",
    "        for k in range(1, n-1):\n",
    "            coeff[k] = coeff[k-1] + 2*n - (2*(k+1)+1)\n",
    "        coeff = coeff /((n-1)**2)\n",
    "    else:\n",
    "        print('Undefined Metric: ' + metric)\n",
    "    coeff = np.concatenate((coeff, coeff[::-1][1:]), axis=0)\n",
    "    coeff = coeff * lmbda\n",
    "    coeff[n-2] = 1\n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_type = 'cal_housing'\n",
    "num_samples = 10000\n",
    "num_classes = 9\n",
    "nclasses = num_classes\n",
    "dim = 2\n",
    "\n",
    "sigma_noise = 0.01\n",
    "optimizer='sgd' #Optimizer function\n",
    "iter_loc=10 #Number of the first column in the excel file for writing the results.\n",
    "lr=.5 #Initial learning rate\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "batch_size = 256\n",
    "lr_scheduler=ft.exp_lr_scheduler #Learning rate scheduler\n",
    "lr_decay_epoch=10 #Number of epoch for learning rate decay\n",
    "hidden_sizes = [50, 50]\n",
    "dropouts = [0, 0]\n",
    "rand_label = False\n",
    "\n",
    "metric = 'ccr'\n",
    "coeff_lmbda =  1\n",
    "multi_coeff = make_coeff(nclasses, metric, coeff_lmbda)\n",
    "KL = False #KL divergence for porbability measure\n",
    "\n",
    "\n",
    "'''Multipliers for loss functions'''\n",
    "single_loss=1.\n",
    "multi_loss=0.\n",
    "\n",
    "comment=' ' #Additional comments if any\n",
    "\n",
    "algo = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = 5\n",
    "random_seed = 1\n",
    "\n",
    "if data_type == 'circular':\n",
    "    fvec, label = generateCircularData(num_samples = num_samples, \n",
    "                                       num_classes = num_classes, dim = dim, bound = 5,\n",
    "                                       sigma_noise = sigma_noise, rand_label = rand_label)\n",
    "elif data_type == 'linear':\n",
    "    fvec, label = generateLinearData(num_samples = num_samples, \n",
    "                                     num_classes = num_classes, dim = dim, bound = 5,\n",
    "                                       sigma_noise = sigma_noise, rand_label = rand_label)\n",
    "elif data_type == 'spiral':\n",
    "    fvec, label = generateSpiralData(num_samples = num_samples, \n",
    "                                     num_classes = num_classes, dim = dim, bound = 5,\n",
    "                                       sigma_noise = sigma_noise, rand_label = rand_label)\n",
    "else:\n",
    "    num_classes = num_bins\n",
    "    nclasses = num_classes\n",
    "    \n",
    "    feat=train_df.values[:,:-2]\n",
    "    #Normalize the features\n",
    "\n",
    "    feat_max = np.amax(feat,axis=0)\n",
    "    feat_min = np.amin(feat,axis=0)\n",
    "\n",
    "    feat=(feat-feat_min)/(feat_max-feat_min)\n",
    "    feat=feat*2-1\n",
    "\n",
    "    '''feat_mean = np.mean(feat,axis=0)\n",
    "    feat_std = np.std(feat,axis=0)\n",
    "\n",
    "    feat=(feat-feat_mean)/feat_std\n",
    "    '''\n",
    "    label_ord=train_df.values[:,-1].astype(np.int)\n",
    "\n",
    "    rand_idx = np.random.permutation(len(label_ord))\n",
    "    feat = feat[rand_idx, :]\n",
    "    label = label_ord[rand_idx]\n",
    "\n",
    "\n",
    "    print(np.mean(feat,axis=0))\n",
    "    print(np.min(feat,axis=0))\n",
    "    print(feat.shape)\n",
    "    print(label)\n",
    "\n",
    "    fvec=feat.copy()\n",
    "    dim = feat.shape[1]\n",
    "    \n",
    "    if not CV == 0: \n",
    "        dset_train= torch.utils.data.TensorDataset(torch.from_numpy(fvec).type(torch.FloatTensor),\n",
    "                                                       torch.from_numpy(label).type(torch.LongTensor))\n",
    "        dset_val= torch.utils.data.TensorDataset(torch.from_numpy(fvec).type(torch.FloatTensor),\n",
    "                                                       torch.from_numpy(label).type(torch.LongTensor))\n",
    "\n",
    "        '''Define dataset loaders''''''\n",
    "        dset_loaders = {'train':torch.utils.data.DataLoader(dsets['train'], batch_size=batch_size,shuffle=True,\n",
    "                                                            num_workers=12),\n",
    "                        'val':torch.utils.data.DataLoader(dsets['val'], batch_size=batch_size,shuffle=False,\n",
    "                                                            num_workers=12)}\n",
    "\n",
    "\n",
    "        dset_sizes={'train':len(dsets['train']),'val':len(dsets['val'])}\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "\n",
    "        print(dset_sizes)\n",
    "\n",
    "        if use_gpu:\n",
    "            print('GPU is available')\n",
    "        else:\n",
    "            print('!!!!! NO CUDA GPUS DETECTED')\n",
    "\n",
    "        inputs, classes = next(iter(dset_loaders['train']))\n",
    "        print(inputs.shape)'''\n",
    "        '''dset_train = datasets.ImageFolder(data_dir+'/train_val', data_transforms['train'])\n",
    "        dset_val = datasets.ImageFolder(data_dir+'/train_val', data_transforms['val'])'''\n",
    "\n",
    "        num_train = len(dset_train)\n",
    "        indices = list(range(num_train))\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        splits = (num_train*np.linspace(0,1,CV+1)).astype(int)\n",
    "\n",
    "        val_idx = [indices[splits[k]:splits[k+1]] for k in range(CV)]\n",
    "        train_idx=[np.setdiff1d(indices,val_idx[k]) for k in range(CV)]\n",
    "        '''Sampler functions for validation and training'''\n",
    "        sampler_train = [torch.utils.data.sampler.SubsetRandomSampler(train_idx[k]) for k in range(CV)]\n",
    "        sampler_val = [torch.utils.data.sampler.SubsetRandomSampler(val_idx[k]) for k in range(CV)]\n",
    "\n",
    "        '''Define dataset loaders'''\n",
    "        dset_loaders_arr = [{'train':torch.utils.data.DataLoader(dset_train, batch_size=batch_size,sampler=sampler_train[k],\n",
    "                                                            num_workers=12),\n",
    "                        'val':torch.utils.data.DataLoader(dset_val, batch_size=batch_size,sampler=sampler_val[k],\n",
    "                                                            num_workers=12)} for k in range(CV)]\n",
    "        dset_sizes={'train':int(len(dset_train)*(1-1/CV)),'val':int(len(dset_train)*(1/CV))}\n",
    "\n",
    "        print(dset_sizes)\n",
    "        print('OR')\n",
    "        print('Number of training images '+str(len(val_idx)))\n",
    "        print('Number of validation images '+str(len(train_idx)))\n",
    "    \n",
    "\n",
    "\n",
    "'''rand_idx = np.random.permutation(len(label))\n",
    "fvec_norm = (fvec)/5\n",
    "mid_point = int(len(label)/2)#100*num_classes\n",
    "fvec_test = fvec_norm[rand_idx[:mid_point],:]\n",
    "fvec_train = fvec_norm[rand_idx[mid_point:],:]\n",
    "\n",
    "label_test = label[rand_idx[:mid_point]]\n",
    "label_train = label[rand_idx[mid_point:]]\n",
    "print(np.max(fvec_train))\n",
    "print(np.min(fvec_train))\n",
    "\n",
    "torch.from_numpy(label_train).type(torch.LongTensor)\n",
    "dsets={'train': torch.utils.data.TensorDataset(torch.from_numpy(fvec_train).type(torch.FloatTensor),\n",
    "                                               torch.from_numpy(label_train).type(torch.LongTensor)),\n",
    "       'val': torch.utils.data.TensorDataset(torch.from_numpy(fvec_test).type(torch.FloatTensor),\n",
    "                                             torch.from_numpy(label_test).type(torch.LongTensor))}\n",
    "\n",
    "''''''\n",
    "dset_loaders = {'train':torch.utils.data.DataLoader(dsets['train'], batch_size=batch_size,shuffle=True,\n",
    "                                                    num_workers=12),\n",
    "                'val':torch.utils.data.DataLoader(dsets['val'], batch_size=batch_size,shuffle=False,\n",
    "                                                    num_workers=12)}\n",
    "\n",
    "\n",
    "dset_sizes={'train':len(dsets['train']),'val':len(dsets['val'])}\n",
    "'''\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "print(dset_sizes)\n",
    "\n",
    "if use_gpu:\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    print('!!!!! NO CUDA GPUS DETECTED')\n",
    "\n",
    "'''inputs, classes = next(iter(dset_loaders['train']))\n",
    "print(inputs.shape)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(feat.astype(np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeLog(logname):\n",
    "    '''\n",
    "    Creates a text file named Network_properties.txt inside runs/'logname'\n",
    "    '''\n",
    "    f=open('runs_regression/'+logname+'/Network_properties.txt','w')\n",
    "    f.write('Feature Length: '+str(dim)+'\\n')\n",
    "    f.write('Number of classes: '+str(num_classes)+'\\n')\n",
    "    f.write('Data type: '+data_type+'\\n')\n",
    "    f.write('Random Noise: '+str(sigma_noise)+'\\n')\n",
    "    \n",
    "    f.write('Hidden sizes: '+ str(hidden_sizes)+'\\n')\n",
    "    f.write('Dropouts: '+str(dropouts)+'\\n')\n",
    "    f.write('Batch size: '+str(batch_size)+'\\n')\n",
    "    f.write('Number of samples: '+str(num_samples)+'\\n')\n",
    "    \n",
    "    f.write('Optimizer: ' + optimizer + '\\n')\n",
    "    crt=str(single_loss)+'xsingle + '+str(multi_loss)+'Xmulti'\n",
    "    f.write('Criterion: '+crt+'\\n')\n",
    "    f.write('Learning rate: '+str(lr)+'\\n')\n",
    "    f.write('Momentum: '+str(momentum)+'\\n')\n",
    "    f.write('Leraning Rate Scheduler: '+str(lr_scheduler)+'\\n')\n",
    "    f.write('Leraning Rate Decay Period: '+str(lr_decay_epoch)+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs_regression.xlsx\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "import time\n",
    "\n",
    "def writeLog_xlsx(logname='logs_regression.xlsx',iter_loc=10):\n",
    "    '''\n",
    "    Adds a line to logs.xlsx with the network properties and outcomes.\n",
    "    :param iter_loc: First column to record the outcomes.\n",
    "    '''\n",
    "    \n",
    "    print(logname)\n",
    "    book = openpyxl.load_workbook(logname)\n",
    "    sheet = book.active\n",
    "    crt=str(single_loss)+'xsingle + '+str(multi_loss)+'Xmulti'\n",
    "    if metric:\n",
    "        m_coeff = make_coeff(nclasses, metric, coeff_lmbda)\n",
    "    else:\n",
    "        m_coeff = multi_coeff\n",
    "    specs=(datetime.now().strftime('%B%d  %H:%M:%S'),data_type,str(hidden_sizes),str(dim),str(num_classes),\n",
    "           crt, str(lr), str(m_coeff), str(algo))\n",
    "    sheet.append(specs)\n",
    "    current_row = sheet.max_row\n",
    "    sheet.cell(row=current_row, column=iter_loc+5).value = comment\n",
    "    book.save(logname)\n",
    "writeLog_xlsx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (fc0): Linear (2 -> 50)\n",
      "  (relu0): ReLU ()\n",
      "  (drop0): Dropout (p = 0)\n",
      "  (fc1): Linear (50 -> 50)\n",
      "  (relu1): ReLU ()\n",
      "  (drop1): Dropout (p = 0)\n",
      "  (fc2): Linear (50 -> 2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, dropouts, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.numHidden=len(hidden_sizes)\n",
    "        setattr(self, 'fc0', nn.Linear(input_size, hidden_sizes[0]))\n",
    "        setattr(self, 'relu0', nn.ReLU())\n",
    "        setattr(self, 'drop0', nn.Dropout(p=dropouts[0]))\n",
    "        for k in range(len(hidden_sizes)-1):\n",
    "            setattr(self, 'fc'+str(k+1), nn.Linear(hidden_sizes[k], hidden_sizes[k+1]))\n",
    "            setattr(self, 'relu'+str(k+1), nn.ReLU())\n",
    "            setattr(self, 'drop'+str(k+1), nn.Dropout(p=dropouts[k+1]))\n",
    "        setattr(self, 'fc'+str(len(hidden_sizes)), nn.Linear(hidden_sizes[-1], num_classes))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out=self.fc0(x)\n",
    "        out = self.relu0(out)\n",
    "        out = self.drop0(out)\n",
    "        for k in range(self.numHidden-1):\n",
    "            fc = getattr(self,'fc'+str(k+1))\n",
    "            relu = getattr(self,'relu'+str(k+1))\n",
    "            drop = getattr(self,'drop'+str(k+1))\n",
    "            out = fc(out)\n",
    "            out = relu(out)\n",
    "            out = drop(out)\n",
    "        fc = getattr(self,'fc'+str(self.numHidden))\n",
    "        out = fc(out)\n",
    "        return out\n",
    "    \n",
    "model=Net(2, [50, 50], [0, 0], 2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network_loader(comment=comment,\n",
    "                    optimizer=optimizer,\n",
    "                    iter_loc=iter_loc,\n",
    "                    lr=lr,\n",
    "                    momentum=momentum,\n",
    "                    weight_decay=weight_decay,\n",
    "                    lr_scheduler=lr_scheduler,\n",
    "                    lr_decay_epoch=lr_decay_epoch,\n",
    "                    nclasses=num_classes,\n",
    "                    hidden_sizes = hidden_sizes,\n",
    "                    dropouts = dropouts):\n",
    "    \n",
    "    '''Load the network from pytorch'''\n",
    "    model_ft = Net(dim, hidden_sizes , dropouts, num_classes)\n",
    "\n",
    "    if use_gpu:\n",
    "        model_ft = model_ft.cuda()\n",
    "\n",
    "    '''Define the optimizer function'''\n",
    "    if(optimizer=='adam'):\n",
    "        optimizer_ft = optim.Adam(model_ft.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "    elif(optimizer=='sgd'):\n",
    "        if(end_to_end):\n",
    "            optimizer_ft = optim.SGD(model_ft.parameters(), lr=lr, momentum=momentum)\n",
    "        else:\n",
    "            optimizer_ft = optim.SGD(model_ft.fc.parameters(), lr=lr, momentum=momentum,weight_decay=weight_decay)\n",
    "    return model_ft, optimizer_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'end_to_end' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6fc291b403a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                             \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                             \u001b[0mlr_decay_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_decay_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                             nclasses=num_classes)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0ma_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-c06a19410825>\u001b[0m in \u001b[0;36mnetwork_loader\u001b[0;34m(comment, optimizer, iter_loc, lr, momentum, weight_decay, lr_scheduler, lr_decay_epoch, nclasses, hidden_sizes, dropouts)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_to_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0moptimizer_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'end_to_end' is not defined"
     ]
    }
   ],
   "source": [
    "model_ft, optimizer_ft = network_loader(comment=comment, #'Tested for three rooms'\n",
    "                                            optimizer=optimizer,\n",
    "                                            iter_loc=iter_loc,\n",
    "                                            lr=lr,\n",
    "                                            momentum=momentum,\n",
    "                                            weight_decay=weight_decay,\n",
    "                                            lr_scheduler=lr_scheduler,\n",
    "                                            lr_decay_epoch=lr_decay_epoch,\n",
    "                                            nclasses=num_classes)\n",
    "a_vec = Variable(torch.randn(10, 1), requires_grad=True)\n",
    "params = optimizer_ft.param_groups\n",
    "params[0]['params'].append(a_vec)\n",
    "optimizer_ft.param_groups = params\n",
    "print(optimizer_ft.param_groups)\n",
    "#optimizer_ft.add_param_group({'params': a_vec})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_epochs(result_log, logname):\n",
    "    print(len(result_log))\n",
    "\n",
    "    wb_tr = openpyxl.Workbook()\n",
    "    ws_tr = wb_tr.active\n",
    "    wb_val = openpyxl.Workbook()\n",
    "    ws_val = wb_val.active\n",
    "    print(logname)\n",
    "\n",
    "    label_arr_tr = np.zeros((100000,1))\n",
    "    probs_arr_tr = np.zeros((100000, num_classes))\n",
    "    label_arr_val = np.zeros((100000,1))\n",
    "    probs_arr_val = np.zeros((100000, num_classes))\n",
    "\n",
    "    prev_epoch = 0\n",
    "    \n",
    "    count_tr = count_val = 0\n",
    "    for result in result_log:\n",
    "        epoch = result[1]\n",
    "        if not epoch == prev_epoch:\n",
    "            label_arr_tr = label_arr_tr[:count_tr]\n",
    "            probs_arr_tr = probs_arr_tr[:count_tr, :]\n",
    "            label_arr_val = label_arr_val[:count_val]\n",
    "            probs_arr_val = probs_arr_val[:count_val, :]\n",
    "            ws_tr.append(['Epoch ' + str(prev_epoch)])\n",
    "            ws_tr.append(label_arr_tr[1:].reshape(-1).tolist())\n",
    "            ws_tr.append(np.argmax(probs_arr_tr[1:,:], axis=1).reshape(-1).tolist())\n",
    "            for probs in probs_arr_tr[1:,:].T.tolist():\n",
    "                ws_tr.append(probs)\n",
    "            #wb_tr.save('./runs_ord/'+logname + '/train.xlsx')\n",
    "            ws_val.append(['Epoch ' + str(prev_epoch)])\n",
    "            ws_val.append(label_arr_val[1:].reshape(-1).tolist())\n",
    "            ws_val.append(np.argmax(probs_arr_val[1:,:], axis=1).reshape(-1).tolist())\n",
    "            for probs in probs_arr_val[1:,:].T.tolist():\n",
    "                ws_val.append(probs)\n",
    "    \n",
    "\n",
    "            label_arr_tr = np.zeros((100000,1))\n",
    "            probs_arr_tr = np.zeros((100000, num_classes))\n",
    "            label_arr_val = np.zeros((100000,1))\n",
    "            probs_arr_val = np.zeros((100000, num_classes)) \n",
    "            count_tr = count_val = 0\n",
    "            prev_epoch = epoch\n",
    "\n",
    "        label = np.asarray(result[2]).reshape(-1,1)\n",
    "        scores = np.asarray(result[3])\n",
    "        exp_scores = np.exp(scores - np.max(scores,axis=1).reshape(-1, 1)*np.ones(num_classes))\n",
    "        probs = np.round(exp_scores/(np.sum(exp_scores,axis=1).reshape(-1, 1)*np.ones(num_classes)), decimals=2)\n",
    "        if result[0] == 'train':\n",
    "            label_arr_tr[count_tr:count_tr + len(label)]  = label\n",
    "            probs_arr_tr[count_tr:count_tr + len(label), :] = probs\n",
    "            count_tr += len(label)\n",
    "        elif result[0] == 'val':\n",
    "            label_arr_val[count_val:count_val + len(label)]  = label\n",
    "            probs_arr_val[count_val:count_val + len(label), :] = probs\n",
    "            count_val += len(label)\n",
    "\n",
    "\n",
    "    \n",
    "    label_arr_tr = label_arr_tr[:count_tr]\n",
    "    probs_arr_tr = probs_arr_tr[:count_tr, :]\n",
    "    label_arr_val = label_arr_val[:count_val]\n",
    "    probs_arr_val = probs_arr_val[:count_val, :]\n",
    "            \n",
    "    ws_tr.append(['Epoch ' + str(epoch)])\n",
    "    ws_tr.append(label_arr_tr[1:].reshape(-1).tolist())\n",
    "    ws_tr.append(np.argmax(probs_arr_tr[1:,:], axis=1).reshape(-1).tolist())\n",
    "    for probs in probs_arr_tr[1:,:].T.tolist():\n",
    "        ws_tr.append(probs)\n",
    "    #wb_tr.save('./runs_ord/'+logname + '/train.xlsx')\n",
    "    ws_val.append(['Epoch ' + str(epoch)])\n",
    "    ws_val.append(label_arr_val[1:].reshape(-1).tolist())\n",
    "    ws_val.append(np.argmax(probs_arr_val[1:,:], axis=1).reshape(-1).tolist())\n",
    "    for probs in probs_arr_val[1:,:].T.tolist():\n",
    "        ws_val.append(probs)\n",
    "    wb_val.save('./runs_regression/'+logname + '/val.xlsx')\n",
    "    label_arr_tr = np.zeros((1,1))\n",
    "    probs_arr_tr = np.zeros((1, num_classes))\n",
    "    label_arr_val = np.zeros((1,1))\n",
    "    probs_arr_val = np.zeros((1, num_classes))\n",
    "    prev_epoch = epoch\n",
    "    print('Finito')\n",
    "    \n",
    "    del label_arr_tr, probs_arr_tr, label_arr_val, probs_arr_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(ft)\n",
    "    \n",
    "def run_network():\n",
    "    '''\n",
    "    Cretaes the log files and starts the training\n",
    "    '''\n",
    "    model_ft, optimizer_ft = network_loader(comment=comment, #'Tested for three rooms'\n",
    "                                            optimizer=optimizer,\n",
    "                                            iter_loc=iter_loc,\n",
    "                                            lr=lr,\n",
    "                                            momentum=momentum,\n",
    "                                            weight_decay=weight_decay,\n",
    "                                            lr_scheduler=lr_scheduler,\n",
    "                                            lr_decay_epoch=lr_decay_epoch,\n",
    "                                            nclasses=num_classes)\n",
    "    \n",
    "    \n",
    "    '''Name of the trial'''\n",
    "    crt=str(single_loss)+'xsingle + '+str(multi_loss)+'Xmulti'\n",
    "    logname='Ordinal_'+datetime.now().strftime('%B%d  %H:%M:%S')\n",
    "    writer = SummaryWriter('runs_regression/'+logname) #For tensorboard\n",
    "    writeLog(logname)\n",
    "    writeLog_xlsx()\n",
    "    \n",
    "    '''Start trianing'''\n",
    "    if metric:\n",
    "        m_coeff = make_coeff(nclasses, metric, coeff_lmbda)\n",
    "    else:\n",
    "        m_coeff = multi_coeff\n",
    "    best_model, last_model, result_log = ft.train_model(model_ft,optimizer_ft, lr_scheduler,dset_loaders,\n",
    "                            dset_sizes,writer,use_gpu=use_gpu,num_epochs=100,batch_size=batch_size,num_log=250,\n",
    "                            lr_decay_epoch=lr_decay_epoch,init_lr=lr,regression=False,\n",
    "                            iter_loc=iter_loc,cross_loss=single_loss,multi_loss=multi_loss,numOut=num_classes,\n",
    "                            logname='logs_regression.xlsx',\n",
    "                            multi_coeff = m_coeff, single_coeff = m_coeff, KL = KL, algo = algo)\n",
    "    \n",
    "    '''Save the models'''\n",
    "    torch.save(best_model,'./saved_models/ord/'+logname+'_best')\n",
    "    torch.save(last_model,'./saved_models/ord/'+logname+'_last')\n",
    "    \n",
    "    '''print('Writing results')\n",
    "    write_epochs(result_log, logname)\n",
    "    print('Wrote results')'''\n",
    "    '''Free up the memory'''\n",
    "    del model_ft, result_log\n",
    "    \n",
    "    writer.close\n",
    "    del writer\n",
    "    return last_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22784, 16)\n"
     ]
    }
   ],
   "source": [
    "'''hidden_sizes = [50, 50]\n",
    "dropouts = [0, 0]\n",
    "end_to_end = True\n",
    "run_network()'''\n",
    "print(fvec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "end_to_end = True\n",
    "optimizer='sgd' #Optimizer function\n",
    "lr=1 #Initial learning rate\n",
    "momentum=0.5\n",
    "weight_decay=0.0005\n",
    "lr_scheduler=ft.exp_lr_scheduler #Learning rate scheduler\n",
    "lr_decay_epoch=15 #Number of epoch for learning rate decay\n",
    "\n",
    "hidden_sizes = [64, 64, 128, 128, 256, 512, 256, 128, 64, 32, 16]\n",
    "dropouts = [0, 0, 0, 0, 0, .5, .5, .5, 0, 0, 0]\n",
    "\n",
    "'''hidden_size = [64, 64, 128, 64, 32]\n",
    "dropouts = [0, 0, 0, 0, 0]'''\n",
    "\n",
    "single_loss=0.\n",
    "multi_loss =1.\n",
    "\n",
    "metric = 'mae'\n",
    "algo = 'cheng'\n",
    "for dset_loaders in dset_loaders_arr:\n",
    "    run_network()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.72625953  0.235544    0.03819638]\n",
      " [ 0.17899252  0.39192492  0.4290826 ]\n",
      " [ 0.68682605  0.26286978  0.05030423]\n",
      " [ 0.69291276  0.25876865  0.04831864]\n",
      " [ 0.50891167  0.36219808  0.12889017]]\n",
      "(5, 1)\n",
      "(5, 1)\n",
      "(5, 1)\n",
      "[[ 0.72625959  0.23554402  0.03819639]\n",
      " [ 0.17899252  0.3919249   0.42908258]\n",
      " [ 0.68682599  0.26286977  0.05030424]\n",
      " [ 0.69291273  0.25876863  0.04831864]\n",
      " [ 0.50891171  0.3621981   0.12889019]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtezcan/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:6: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.3. Note that arange generates values in [start; end), not [start; end].\n"
     ]
    }
   ],
   "source": [
    "numOut = 3\n",
    "numIns = 5\n",
    "\n",
    "log_j_fact = np.log(np.asarray([math.factorial(j) for j in range(numOut)]))\n",
    "ones_vec = Variable(torch.ones(numOut).type(torch.FloatTensor).cuda().view(1, numOut))\n",
    "j_vec = Variable(torch.range(0, numOut-1).type(torch.FloatTensor).cuda().view(1, numOut))\n",
    "log_j_fact = Variable(torch.from_numpy(log_j_fact).type(torch.FloatTensor).cuda().view(1, numOut))\n",
    "\n",
    "\n",
    "preds = Variable(torch.randn(numIns,1).cuda())\n",
    "softplus_step = torch.nn.Softplus()\n",
    "preds = softplus_step(preds)\n",
    "outputs = torch.mm(preds, ones_vec)\n",
    "outputs = j_vec * torch.log(outputs) - outputs - log_j_fact\n",
    "softmax_step = torch.nn.Softmax(dim=1)\n",
    "outputs_softmax = softmax_step(outputs)\n",
    "print(outputs_softmax.data.cpu().numpy())\n",
    "\n",
    "f = preds.data.cpu().numpy()\n",
    "pos_f = np.zeros((numIns,numOut))\n",
    "\n",
    "log_j = np.asarray([(np.log(math.factorial(k))) for k in range(numOut)]).reshape(numOut,1)\n",
    "for k in range(numOut):\n",
    "    print((k*np.log(f) -f - np.log(math.factorial(k))).shape)\n",
    "    pos_f[:, k] = (k*np.log(f) -f - np.log(math.factorial(k))).reshape(numIns)\n",
    "    \n",
    "soft_pos = np.exp(pos_f)\n",
    "soft_pos = soft_pos/(np.sum(soft_pos, axis=1).reshape(-1,1)*np.ones((1, numOut)))\n",
    "print(soft_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 1 1 0 1 1 1]\n",
      " [1 1 0 0 1 1 1 1 1 1]\n",
      " [0 1 1 0 1 0 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 0 1 0 0 1 1]]\n",
      "[ 1  1 -1  9  3]\n"
     ]
    }
   ],
   "source": [
    "preds = (np.random.randint(0,5, size = (5,10))>0).astype(np.int)\n",
    "print(preds)\n",
    "print(np.sum(np.cumprod(preds,axis = 1), axis=1)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "end_to_end = True\n",
    "optimizer='sgd' #Optimizer function\n",
    "lr=.01 #Initial learning rate\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "lr_scheduler=ft.exp_lr_scheduler #Learning rate scheduler\n",
    "lr_decay_epoch=40 #Number of epoch for learning rate decay\n",
    "\n",
    "hidden_sizes = [64, 64, 128, 128, 256, 512, 256, 128, 64, 32, 16]#8, 16, 8, 4, 4]\n",
    "dropouts = [0, 0, .5, .5, .5, .5, .5, .5, .5, 0, 0]#.5, .5, .5]\n",
    "\n",
    "for lr_now in [1, 0.1, 0.01]:\n",
    "    lr = lr_now\n",
    "    for kk in range(3):\n",
    "        \n",
    "        algo = 'None'\n",
    "        single_loss=1.\n",
    "        multi_loss =0.\n",
    "        KL = True\n",
    "        metric = 'mse'\n",
    "        for dset_loaders in dset_loaders_arr:\n",
    "            run_network()\n",
    "\n",
    "        single_loss=0.\n",
    "        multi_loss =1.\n",
    "        metric = 'mse'\n",
    "        for dset_loaders in dset_loaders_arr:\n",
    "            run_network()\n",
    "\n",
    "        algo = 'learn_a_mae'\n",
    "        for dset_loaders in dset_loaders_arr:\n",
    "            run_network()\n",
    "\n",
    "        algo = 'fix_a_mae'\n",
    "        for dset_loaders in dset_loaders_arr:\n",
    "            run_network()\n",
    "            \n",
    "        '''algo = 'None'\n",
    "        single_loss=1.\n",
    "        multi_loss =0.\n",
    "        KL = True\n",
    "        metric = 'ccr'\n",
    "        for dset_loaders in dset_loaders_arr:\n",
    "            run_network()\n",
    "\n",
    "        metric = 'ccr1'\n",
    "        for dset_loaders in dset_loaders_arr:\n",
    "            run_network()\n",
    "\n",
    "        metric = 'mae'\n",
    "        for dset_loaders in dset_loaders_arr:\n",
    "            run_network()\n",
    "\n",
    "        single_loss=0.\n",
    "        multi_loss =1.\n",
    "        metric = 'ccr'\n",
    "        for dset_loaders in dset_loaders_arr:\n",
    "            run_network()\n",
    "\n",
    "        metric = 'ccr1'\n",
    "        for dset_loaders in dset_loaders_arr:\n",
    "            run_network()\n",
    "\n",
    "        metric = 'mae'\n",
    "        for dset_loaders in dset_loaders_arr:\n",
    "            run_network()\n",
    "\n",
    "        algo = 'learn_a'\n",
    "        for dset_loaders in dset_loaders_arr:\n",
    "            run_network()\n",
    "\n",
    "        algo = 'fix_a'\n",
    "        for dset_loaders in dset_loaders_arr:\n",
    "            run_network()\n",
    "\n",
    "        algo = 'cheng'\n",
    "        for dset_loaders in dset_loaders_arr:\n",
    "            run_network()\n",
    "\n",
    "        algo = 'poisson'\n",
    "        for dset_loaders in dset_loaders_arr:\n",
    "            run_network()\n",
    "\n",
    "        algo = 'binomial'\n",
    "        for dset_loaders in dset_loaders_arr:\n",
    "            run_network()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm is learn_a\n",
      "Multi_coef is [ 0.  1.  0.]\n",
      "Epoch 0/99\n",
      "----------\n",
      "LR is set to 0.05\n",
      "Variable containing:\n",
      "-2.0074\n",
      "-0.1140\n",
      "-0.1974\n",
      " 0.3888\n",
      "-0.6736\n",
      " 0.2404\n",
      "-1.6391\n",
      " 0.1642\n",
      "-0.1674\n",
      " 0.6457\n",
      "[torch.cuda.FloatTensor of size 10x1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1025  0.0933  0.0941  ...   0.0983  0.0927  0.1111\n",
      " 0.1047  0.0948  0.0944  ...   0.1025  0.0950  0.1072\n",
      " 0.1021  0.0927  0.0965  ...   0.1045  0.0949  0.1064\n",
      "          ...             ⋱             ...          \n",
      " 0.1040  0.0948  0.0939  ...   0.1008  0.0922  0.1090\n",
      " 0.0991  0.0941  0.0972  ...   0.1027  0.0940  0.1044\n",
      " 0.1043  0.0955  0.0946  ...   0.1007  0.0952  0.1086\n",
      "[torch.cuda.FloatTensor of size 256x10 (GPU 0)]\n",
      "\n",
      "Preds is Variable containing:\n",
      "-0.3485\n",
      "-0.3582\n",
      "-0.3581\n",
      "-0.3557\n",
      "-0.3569\n",
      "-0.3565\n",
      "-0.3537\n",
      "-0.3562\n",
      "-0.3538\n",
      "-0.3593\n",
      "-0.3631\n",
      "-0.3548\n",
      "-0.3626\n",
      "-0.3539\n",
      "-0.3641\n",
      "-0.3569\n",
      "-0.3429\n",
      "-0.3500\n",
      "-0.3584\n",
      "-0.3521\n",
      "-0.3549\n",
      "-0.3500\n",
      "-0.3553\n",
      "-0.3625\n",
      "-0.3547\n",
      "-0.3440\n",
      "-0.3671\n",
      "-0.3595\n",
      "-0.3572\n",
      "-0.3566\n",
      "-0.3550\n",
      "-0.3568\n",
      "-0.3552\n",
      "-0.3466\n",
      "-0.3515\n",
      "-0.3580\n",
      "-0.3507\n",
      "-0.3622\n",
      "-0.3522\n",
      "-0.3573\n",
      "-0.3533\n",
      "-0.3606\n",
      "-0.3545\n",
      "-0.3552\n",
      "-0.3532\n",
      "-0.3560\n",
      "-0.3558\n",
      "-0.3596\n",
      "-0.3607\n",
      "-0.3568\n",
      "-0.3500\n",
      "-0.3619\n",
      "-0.3456\n",
      "-0.3521\n",
      "-0.3497\n",
      "-0.3502\n",
      "-0.3576\n",
      "-0.3547\n",
      "-0.3538\n",
      "-0.3626\n",
      "-0.3592\n",
      "-0.3575\n",
      "-0.3506\n",
      "-0.3596\n",
      "-0.3520\n",
      "-0.3583\n",
      "-0.3605\n",
      "-0.3510\n",
      "-0.3510\n",
      "-0.3536\n",
      "-0.3537\n",
      "-0.3552\n",
      "-0.3535\n",
      "-0.3489\n",
      "-0.3636\n",
      "-0.3608\n",
      "-0.3519\n",
      "-0.3540\n",
      "-0.3575\n",
      "-0.3590\n",
      "-0.3533\n",
      "-0.3562\n",
      "-0.3479\n",
      "-0.3565\n",
      "-0.3520\n",
      "-0.3475\n",
      "-0.3591\n",
      "-0.3483\n",
      "-0.3597\n",
      "-0.3521\n",
      "-0.3544\n",
      "-0.3516\n",
      "-0.3426\n",
      "-0.3565\n",
      "-0.3523\n",
      "-0.3527\n",
      "-0.3632\n",
      "-0.3593\n",
      "-0.3567\n",
      "-0.3530\n",
      "-0.3582\n",
      "-0.3545\n",
      "-0.3480\n",
      "-0.3613\n",
      "-0.3507\n",
      "-0.3571\n",
      "-0.3498\n",
      "-0.3582\n",
      "-0.3532\n",
      "-0.3574\n",
      "-0.3580\n",
      "-0.3658\n",
      "-0.3562\n",
      "-0.3550\n",
      "-0.3553\n",
      "-0.3565\n",
      "-0.3539\n",
      "-0.3554\n",
      "-0.3443\n",
      "-0.3432\n",
      "-0.3557\n",
      "-0.3527\n",
      "-0.3523\n",
      "-0.3609\n",
      "-0.3617\n",
      "-0.3550\n",
      "-0.3551\n",
      "-0.3572\n",
      "-0.3582\n",
      "-0.3643\n",
      "-0.3462\n",
      "-0.3582\n",
      "-0.3487\n",
      "-0.3535\n",
      "-0.3621\n",
      "-0.3536\n",
      "-0.3597\n",
      "-0.3553\n",
      "-0.3498\n",
      "-0.3624\n",
      "-0.3547\n",
      "-0.3630\n",
      "-0.3564\n",
      "-0.3522\n",
      "-0.3460\n",
      "-0.3576\n",
      "-0.3596\n",
      "-0.3566\n",
      "-0.3567\n",
      "-0.3543\n",
      "-0.3546\n",
      "-0.3643\n",
      "-0.3516\n",
      "-0.3510\n",
      "-0.3547\n",
      "-0.3558\n",
      "-0.3575\n",
      "-0.3450\n",
      "-0.3524\n",
      "-0.3457\n",
      "-0.3568\n",
      "-0.3531\n",
      "-0.3567\n",
      "-0.3608\n",
      "-0.3522\n",
      "-0.3484\n",
      "-0.3672\n",
      "-0.3596\n",
      "-0.3602\n",
      "-0.3528\n",
      "-0.3526\n",
      "-0.3599\n",
      "-0.3584\n",
      "-0.3516\n",
      "-0.3520\n",
      "-0.3522\n",
      "-0.3599\n",
      "-0.3585\n",
      "-0.3533\n",
      "-0.3580\n",
      "-0.3579\n",
      "-0.3564\n",
      "-0.3524\n",
      "-0.3535\n",
      "-0.3665\n",
      "-0.3569\n",
      "-0.3565\n",
      "-0.3593\n",
      "-0.3604\n",
      "-0.3484\n",
      "-0.3562\n",
      "-0.3601\n",
      "-0.3568\n",
      "-0.3592\n",
      "-0.3587\n",
      "-0.3563\n",
      "-0.3555\n",
      "-0.3611\n",
      "-0.3581\n",
      "-0.3522\n",
      "-0.3583\n",
      "-0.3568\n",
      "-0.3759\n",
      "-0.3512\n",
      "-0.3576\n",
      "-0.3538\n",
      "-0.3520\n",
      "-0.3598\n",
      "-0.3633\n",
      "-0.3543\n",
      "-0.3587\n",
      "-0.3593\n",
      "-0.3570\n",
      "-0.3562\n",
      "-0.3649\n",
      "-0.3607\n",
      "-0.3645\n",
      "-0.3560\n",
      "-0.3551\n",
      "-0.3641\n",
      "-0.3504\n",
      "-0.3484\n",
      "-0.3557\n",
      "-0.3465\n",
      "-0.3590\n",
      "-0.3532\n",
      "-0.3570\n",
      "-0.3538\n",
      "-0.3473\n",
      "-0.3618\n",
      "-0.3533\n",
      "-0.3521\n",
      "-0.3554\n",
      "-0.3630\n",
      "-0.3565\n",
      "-0.3499\n",
      "-0.3602\n",
      "-0.3556\n",
      "-0.3522\n",
      "-0.3576\n",
      "-0.3543\n",
      "-0.3543\n",
      "-0.3561\n",
      "-0.3528\n",
      "-0.3560\n",
      "-0.3582\n",
      "-0.3500\n",
      "-0.3585\n",
      "-0.3539\n",
      "-0.3585\n",
      "-0.3560\n",
      "-0.3517\n",
      "-0.3571\n",
      "-0.3544\n",
      "-0.3531\n",
      "-0.3544\n",
      "[torch.cuda.FloatTensor of size 256x1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 8\n",
      " 0\n",
      " 6\n",
      " 0\n",
      " 9\n",
      " 0\n",
      " 7\n",
      " 5\n",
      " 9\n",
      " 4\n",
      " 3\n",
      " 6\n",
      " 0\n",
      " 8\n",
      " 6\n",
      " 2\n",
      " 7\n",
      " 8\n",
      " 2\n",
      " 6\n",
      " 2\n",
      " 7\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 9\n",
      " 0\n",
      " 6\n",
      " 5\n",
      " 7\n",
      " 9\n",
      " 6\n",
      " 6\n",
      " 0\n",
      " 8\n",
      " 0\n",
      " 2\n",
      " 8\n",
      " 6\n",
      " 3\n",
      " 6\n",
      " 1\n",
      " 4\n",
      " 2\n",
      " 7\n",
      " 6\n",
      " 0\n",
      " 2\n",
      " 3\n",
      " 8\n",
      " 9\n",
      " 3\n",
      " 4\n",
      " 4\n",
      " 9\n",
      " 8\n",
      " 7\n",
      " 9\n",
      " 1\n",
      " 7\n",
      " 1\n",
      " 6\n",
      " 8\n",
      " 6\n",
      " 9\n",
      " 2\n",
      " 0\n",
      " 7\n",
      " 9\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 9\n",
      " 4\n",
      " 1\n",
      " 3\n",
      " 7\n",
      " 9\n",
      " 2\n",
      " 0\n",
      " 3\n",
      " 2\n",
      " 5\n",
      " 2\n",
      " 3\n",
      " 8\n",
      " 2\n",
      " 3\n",
      " 6\n",
      " 6\n",
      " 4\n",
      " 7\n",
      " 6\n",
      " 5\n",
      " 8\n",
      " 8\n",
      " 4\n",
      " 4\n",
      " 7\n",
      " 6\n",
      " 4\n",
      " 5\n",
      " 2\n",
      " 5\n",
      " 9\n",
      " 0\n",
      " 0\n",
      " 5\n",
      " 8\n",
      " 5\n",
      " 5\n",
      " 4\n",
      " 2\n",
      " 8\n",
      " 9\n",
      " 8\n",
      " 5\n",
      " 0\n",
      " 9\n",
      " 5\n",
      " 5\n",
      " 9\n",
      " 4\n",
      " 3\n",
      " 0\n",
      " 0\n",
      " 8\n",
      " 5\n",
      " 4\n",
      " 1\n",
      " 9\n",
      " 2\n",
      " 5\n",
      " 8\n",
      " 3\n",
      " 8\n",
      " 6\n",
      " 3\n",
      " 7\n",
      " 2\n",
      " 8\n",
      " 1\n",
      " 1\n",
      " 8\n",
      " 8\n",
      " 3\n",
      " 2\n",
      " 0\n",
      " 6\n",
      " 7\n",
      " 5\n",
      " 1\n",
      " 9\n",
      " 4\n",
      " 8\n",
      " 7\n",
      " 0\n",
      " 8\n",
      " 9\n",
      " 0\n",
      " 3\n",
      " 0\n",
      " 5\n",
      " 1\n",
      " 8\n",
      " 7\n",
      " 3\n",
      " 0\n",
      " 4\n",
      " 5\n",
      " 9\n",
      " 3\n",
      " 3\n",
      " 1\n",
      " 6\n",
      " 2\n",
      " 0\n",
      " 3\n",
      " 5\n",
      " 1\n",
      " 3\n",
      " 2\n",
      " 3\n",
      " 3\n",
      " 0\n",
      " 2\n",
      " 2\n",
      " 0\n",
      " 2\n",
      " 6\n",
      " 2\n",
      " 1\n",
      " 6\n",
      " 4\n",
      " 0\n",
      " 4\n",
      " 6\n",
      " 4\n",
      " 6\n",
      " 6\n",
      " 5\n",
      " 6\n",
      " 7\n",
      " 0\n",
      " 8\n",
      " 5\n",
      " 3\n",
      " 2\n",
      " 7\n",
      " 5\n",
      " 0\n",
      " 0\n",
      " 2\n",
      " 0\n",
      " 4\n",
      " 2\n",
      " 5\n",
      " 3\n",
      " 3\n",
      " 2\n",
      " 2\n",
      " 8\n",
      " 6\n",
      " 9\n",
      " 6\n",
      " 2\n",
      " 6\n",
      " 9\n",
      " 7\n",
      " 8\n",
      " 6\n",
      " 8\n",
      " 8\n",
      " 0\n",
      " 8\n",
      " 0\n",
      " 4\n",
      " 1\n",
      " 7\n",
      " 0\n",
      " 3\n",
      " 5\n",
      " 4\n",
      " 7\n",
      " 4\n",
      " 3\n",
      " 1\n",
      " 4\n",
      " 8\n",
      " 3\n",
      " 2\n",
      " 6\n",
      " 6\n",
      " 7\n",
      " 4\n",
      " 2\n",
      "[torch.cuda.FloatTensor of size 256 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Variable' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-5a1e68e99e0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmulti_coeff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_coeff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mrun_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlmbda_mae\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-1d854c7fac37>\u001b[0m in \u001b[0;36mrun_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                             \u001b[0miter_loc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter_loc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcross_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msingle_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmulti_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumOut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                             \u001b[0mlogname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logs_regression.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                             multi_coeff = m_coeff, single_coeff = m_coeff, KL = KL, algo = algo)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;34m'''Save the models'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mtezcan/New Volume/amazon/notebook/functions/fine_tune.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, lr_scheduler, dset_loaders, dset_sizes, writer, use_gpu, num_epochs, batch_size, num_log, init_lr, lr_decay_epoch, regression, learn_a, cross_loss, multi_loss, numOut, logname, iter_loc, multi_coeff, single_coeff, KL, algo)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fallthrough_methods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Variable' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "end_to_end = True\n",
    "optimizer='adam' #Optimizer function\n",
    "lr=0.05 #Initial learning rate\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "lr_scheduler=ft.exp_lr_scheduler #Learning rate scheduler\n",
    "lr_decay_epoch=10 #Number of epoch for learning rate decay\n",
    "\n",
    "\n",
    "hidden_sizes = [16, 16, 32, 32, 16, 16]#8, 16, 8, 4, 4]\n",
    "dropouts = []#.5, .5, .5]\n",
    "single_loss=1.0\n",
    "multi_loss =0.0\n",
    "\n",
    "KL = True\n",
    "metric = None\n",
    "\n",
    "for lmbda_mae in [.1*k for k in range(11)]:\n",
    "    multi_coeff = lmbda_mae * np.asarray(make_coeff(nclasses, 'ccr1', coeff_lmbda))\n",
    "    multi_coeff[int((len(multi_coeff)-1)/2)] = 1.\n",
    "    for k in range(10):\n",
    "        run_network()\n",
    "        \n",
    "for lmbda_mae in [.1*k for k in range(11)]:\n",
    "    multi_coeff = lmbda_mae * np.asarray(make_coeff(nclasses, 'mae', coeff_lmbda))\n",
    "    multi_coeff[int((len(multi_coeff)-1)/2)] = 1.\n",
    "    for k in range(10):\n",
    "        run_network()\n",
    "    \n",
    "'''KL = True\n",
    "metric = 'ccr'\n",
    "for k in range(10):\n",
    "    run_network()\n",
    "    \n",
    "metric = 'ccr1'\n",
    "for k in range(10):\n",
    "    run_network()\n",
    "    \n",
    "metric = 'mae'\n",
    "for k in range(10):\n",
    "    run_network()\n",
    "    \n",
    "metric = 'mse'\n",
    "for k in range(10):\n",
    "    run_network()'''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloader again, this time without shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fvec_norm = (fvec)/5\n",
    "mid_point = int(len(label)/2)#100*num_classes\n",
    "fvec_test = fvec_norm[rand_idx[:mid_point],:]\n",
    "fvec_train = fvec_norm[rand_idx[mid_point:],:]\n",
    "\n",
    "label_test = label[rand_idx[:mid_point]]\n",
    "label_train = label[rand_idx[mid_point:]]\n",
    "print(np.max(fvec_train))\n",
    "print(np.min(fvec_train))\n",
    "\n",
    "torch.from_numpy(label_train).type(torch.LongTensor)\n",
    "dsets={'train': torch.utils.data.TensorDataset(torch.from_numpy(fvec_train).type(torch.FloatTensor),\n",
    "                                               torch.from_numpy(label_train).type(torch.LongTensor)),\n",
    "       'val': torch.utils.data.TensorDataset(torch.from_numpy(fvec_test).type(torch.FloatTensor),\n",
    "                                             torch.from_numpy(label_test).type(torch.LongTensor))}\n",
    "\n",
    "'''Define dataset loaders'''\n",
    "dset_loaders = {'train':torch.utils.data.DataLoader(dsets['train'], batch_size=batch_size,shuffle=False,\n",
    "                                                    num_workers=12),\n",
    "                'val':torch.utils.data.DataLoader(dsets['val'], batch_size=batch_size,shuffle=False,\n",
    "                                                    num_workers=12)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_shape = 'Spiral'\n",
    "data_date = '18_01_31'\n",
    "data_dir = './saved_models_github/' + data_shape + '/' + data_date\n",
    "\n",
    "run_dirs = sorted(os.listdir(data_dir))\n",
    "last_dirs = run_dirs[1::2]\n",
    "ccr1_dirs = last_dirs[:110]\n",
    "mae_dirs = last_dirs[110:]\n",
    "all_dirs = [ccr1_dirs, mae_dirs]\n",
    "\n",
    "\n",
    "'''run_dirs = sorted(os.listdir('./saved_models/test'))\n",
    "last_dirs = run_dirs[1::2]\n",
    "ccr1_dirs = last_dirs[:100]\n",
    "mae_dirs = last_dirs[100:]\n",
    "\n",
    "pure_ccr1 = ['Ordinal_January24  14:10:01_last',\n",
    "             'Ordinal_January24  14:11:08_last',\n",
    "             'Ordinal_January24  14:12:14_last',\n",
    "             'Ordinal_January24  14:13:21_last',\n",
    "             'Ordinal_January24  14:14:27_last',\n",
    "             'Ordinal_January24  14:15:34_last',\n",
    "             'Ordinal_January24  14:16:40_last',\n",
    "             'Ordinal_January24  14:17:47_last',\n",
    "             'Ordinal_January24  14:18:54_last',\n",
    "             'Ordinal_January24  14:20:00_last',]\n",
    "\n",
    "pure_mae = ['Ordinal_January24  14:21:07_last',\n",
    "             'Ordinal_January24  14:22:13_last',\n",
    "             'Ordinal_January24  14:23:19_last',\n",
    "             'Ordinal_January24  14:24:26_last',\n",
    "             'Ordinal_January24  14:25:32_last',\n",
    "             'Ordinal_January24  14:26:38_last',\n",
    "             'Ordinal_January24  14:27:45_last',\n",
    "             'Ordinal_January24  14:28:52_last',\n",
    "             'Ordinal_January24  14:29:58_last',\n",
    "             'Ordinal_January24  14:31:05_last',]'''\n",
    "                \n",
    "len(mae_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.load('./saved_models/ord/Ordinal_January24  10:24:20_last', map_location={'cuda:0': 'cpu'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(model_dir, phase='train'):\n",
    "    if use_gpu:\n",
    "        model = torch.load(model_dir)\n",
    "    else:\n",
    "        model = torch.load(model_dir, map_location={'cuda:0': 'cpu'})\n",
    "    model.train(False)\n",
    "\n",
    "    labels_arr = np.asarray([]);\n",
    "    preds_arr = np.asarray([]);\n",
    "    for data in dset_loaders[phase]:\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            outputs = np.argmax(model(inputs).cpu().data.numpy(), axis=1)\n",
    "            #labels_arr = np.append(labels_arr,labels.cpu().data.numpy())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = np.argmax(model(inputs).data.numpy(), axis=1)\n",
    "            #labels_arr = np.append(labels_arr,labels.data.numpy())\n",
    "          \n",
    "        preds_arr = np.append(preds_arr, outputs)\n",
    "    return preds_arr\n",
    "#pred_tr = validate('./saved_models/ord/Ordinal_January24  10:24:20_last')\n",
    "#print(np.min(label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(18,15))\n",
    "\n",
    "for k in range(5):\n",
    "    plt.subplot(3,5,k+1)\n",
    "    pred_tr = validate(data_dir + '/' + last_dirs[k])\n",
    "    plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "    #plt.scatter(fvec_train[:, 0], fvec_train[:, 1], c=label_train,vmin=-1, vmax=num_classes)\n",
    "    plt.colorbar()\n",
    "    ccr = np.mean(pred_tr==label_train)\n",
    "    ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "    mae = np.mean(np.abs(pred_tr-label_train))\n",
    "    rmse = np.mean((pred_tr-label_train)**2)\n",
    "    plt.title('$CCR$ loss' + \n",
    "              ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "              ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)))\n",
    "    \n",
    "for k in range(5):\n",
    "    plt.subplot(3,5,k+6)\n",
    "    pred_tr = validate(data_dir + '/' + last_dirs[50+k])\n",
    "    plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "    #plt.scatter(fvec_train[:, 0], fvec_train[:, 1], c=label_train,vmin=-1, vmax=num_classes)\n",
    "    plt.colorbar()\n",
    "    ccr = np.mean(pred_tr==label_train)\n",
    "    ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "    mae = np.mean(np.abs(pred_tr-label_train))\n",
    "    rmse = np.mean((pred_tr-label_train)**2)\n",
    "    plt.title('$0.5CCR$ loss + $0.5CCR_1$ loss \\n' + \n",
    "              ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "              ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)))\n",
    "    \n",
    "for k in range(5):\n",
    "    plt.subplot(3,5,k+11)\n",
    "    pred_tr = validate(data_dir + '/' + last_dirs[100+k])\n",
    "    plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "    #plt.scatter(fvec_train[:, 0], fvec_train[:, 1], c=label_train,vmin=-1, vmax=num_classes)\n",
    "    plt.colorbar()\n",
    "    ccr = np.mean(pred_tr==label_train)\n",
    "    ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "    mae = np.mean(np.abs(pred_tr-label_train))\n",
    "    rmse = np.mean((pred_tr-label_train)**2)\n",
    "    plt.title('$CCR_1$ loss ' + \n",
    "              ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "              ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)))\n",
    "    \n",
    "plt.savefig('variance_of_results.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_and_mean(root_dir, sub_dirs, phase='train'):\n",
    "    scores_arr = np.zeros((label_train.shape[0],9))\n",
    "    for sub_dir in sub_dirs:\n",
    "        if use_gpu:\n",
    "            model = torch.load(root_dir + '/' + sub_dir)\n",
    "        else:\n",
    "            model = torch.load(root_dir + '/' + sub_dir, map_location={'cuda:0': 'cpu'})\n",
    "        model.train(False)\n",
    "        #print(model)\n",
    "        score_arr = np.zeros((1,9))\n",
    "        for data in dset_loaders[phase]:\n",
    "            inputs, labels = data\n",
    "            if use_gpu:\n",
    "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                outputs = model(inputs).cpu().data.numpy()\n",
    "                #print(outputs.shape)\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "                outputs = model(inputs).data.numpy()\n",
    "                \n",
    "            score_arr = np.append(score_arr, outputs, axis=0)\n",
    "        scores_arr += score_arr[1:,:]\n",
    "        \n",
    "    return scores_arr\n",
    "#scores_tr = validate_and_mean('./saved_models/test_circular', last_dirs[:10])\n",
    "#pred_tr = np.argmax(scores_tr, axis=1)\n",
    "#print(np.mean(label_train == pred_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dirs = ['./saved_models/ord/Ordinal_January24  10:16:00_last',\n",
    "             './saved_models/ord/Ordinal_January24  10:20:35_last',\n",
    "             './saved_models/ord/Ordinal_January24  10:24:20_last',\n",
    "             './saved_models/ord/Ordinal_January24  10:28:03_last']\n",
    "\n",
    "'''model_dirs = ['./saved_models/ord/Ordinal_January24  13:09:17_last',\n",
    "             './saved_models/ord/Ordinal_January24  13:18:08_last',\n",
    "             './saved_models/ord/Ordinal_January24  13:27:43_last',\n",
    "             './saved_models/ord/Ordinal_January24  13:43:43_last']'''\n",
    "\n",
    "model_dirs = ['./saved_models/ord/Ordinal_January26  00:22:14_last',\n",
    "              './saved_models/ord/Ordinal_January26  00:19:20_last',\n",
    "              './saved_models/ord/Ordinal_January26  00:09:08_last',\n",
    "              './saved_models/ord/Ordinal_January26  00:17:53_last']\n",
    "preds = []\n",
    "\n",
    "for model_dir in model_dirs:\n",
    "    preds.append(validate(model_dir))\n",
    "    \n",
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric = 'CCR1'\n",
    "\n",
    "if metric is 'CCR1':\n",
    "    metric_code = 0\n",
    "elif metric is 'MAE':\n",
    "    metric_code = 1\n",
    "else:\n",
    "    print('Wrong metric')\n",
    "    \n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "plt.subplot(4,3,1)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=label_train, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "plt.title('Ground Truth')\n",
    "\n",
    "metrics = np.zeros((11, 4))\n",
    "for k in range(10):\n",
    "    scores_tr = validate_and_mean(data_dir, all_dirs[metric_code][k*10:(k+1)*10])\n",
    "    pred_tr = np.argmax(scores_tr, axis=1)\n",
    "    plt.subplot(4,3,k+2)\n",
    "    plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "    plt.colorbar()\n",
    "    ccr = np.mean(pred_tr==label_train)\n",
    "    ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "    mae = np.mean(np.abs(pred_tr-label_train))\n",
    "    rmse = np.mean((pred_tr-label_train)**2)\n",
    "    metrics[k,:] = [ccr,ccr1,mae,rmse]\n",
    "    plt.title('$\\lambda$=' + str(np.round(k*.1, decimals=1)) + \n",
    "              ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "              ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)) +\n",
    "              ', $MAE$=' + str(np.round(mae, decimals = 2)) +\n",
    "              ', $RMSE$=' + str(np.round(rmse, decimals = 2)))\n",
    "\n",
    "    \n",
    "#pred_tr = validate('./saved_models/ord/Ordinal_January24  10:20:35_last')\n",
    "#pred_tr = validate('./saved_models/ord/Ordinal_January24  13:18:08_last')\n",
    "\n",
    "scores_tr = validate_and_mean(data_dir, all_dirs[metric_code][100:])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(4,3,12)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "ccr = np.mean(pred_tr==label_train)\n",
    "ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "mae = np.mean(np.abs(pred_tr-label_train))\n",
    "rmse = np.mean((pred_tr-label_train)**2)\n",
    "metrics[10,:] = [ccr,ccr1,mae,rmse]\n",
    "plt.title('$\\lambda$=1.0' + \n",
    "          ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "          ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)) +\n",
    "          ', $MAE$=' + str(np.round(mae, decimals = 2)) +\n",
    "          ', $RMSE$=' + str(np.round(rmse, decimals = 2)))\n",
    "\n",
    "plt_title = data_shape + '_Data_CCR_' + metric + '_tradeoff_' + data_date + '.tiff'\n",
    "plt.savefig(plt_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(metrics)\n",
    "\n",
    "lmbdas = [.1*k for k in range(11)]\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(221)\n",
    "plt.plot(lmbdas, metrics[:,0], 'o-')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('$CCR$')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(lmbdas, metrics[:,1], 'o-')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('$CCR_1$')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(lmbdas, metrics[:,2], 'o-')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('$MAE$')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(lmbdas, metrics[:,3], 'o-')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('$RMSE$')\n",
    "\n",
    "plt.savefig('spiral_plots_ccr1.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=label_train, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "plt.title('Ground Truth')\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/test_circular', ccr1_dirs[:10])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,2)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "ccr = np.mean(pred_tr==label_train)\n",
    "ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "mae = np.mean(np.abs(pred_tr-label_train))\n",
    "rmse = np.mean((pred_tr-label_train)**2)\n",
    "plt.title('$CCR loss$'  + \n",
    "          ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "          ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)) +\n",
    "          ',\\n $MAE$=' + str(np.round(mae, decimals = 2)) +\n",
    "          ', $RMSE$=' + str(np.round(rmse, decimals = 2)))\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/test_circular', ccr1_dirs[100:])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,3)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "ccr = np.mean(pred_tr==label_train)\n",
    "ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "mae = np.mean(np.abs(pred_tr-label_train))\n",
    "rmse = np.mean((pred_tr-label_train)**2)\n",
    "plt.title('$CCR_1 loss$'  + \n",
    "          ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "          ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)) +\n",
    "          ',\\n $MAE$=' + str(np.round(mae, decimals = 2)) +\n",
    "          ', $RMSE$=' + str(np.round(rmse, decimals = 2)))\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/ord', mae_dirs[100:])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,4)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "ccr = np.mean(pred_tr==label_train)\n",
    "ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "mae = np.mean(np.abs(pred_tr-label_train))\n",
    "rmse = np.mean((pred_tr-label_train)**2)\n",
    "plt.title('$MAE loss$'  + \n",
    "          ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "          ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)) +\n",
    "          ',\\n$MAE$=' + str(np.round(mae, decimals = 2)) +\n",
    "          ', $RMSE$=' + str(np.round(rmse, decimals = 2)))\n",
    "\n",
    "plt.savefig('spiral_extreme.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=label_train, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "plt.title('Ground Truth')\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/test_circular', ccr1_dirs[:10])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,2)\n",
    "plt.hist(pred_tr-label_train, bins = np.arange(-4.5,5.5,1))\n",
    "plt.title('$CCR$ loss')\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/test_circular', ccr1_dirs[100:])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,3)\n",
    "plt.hist(pred_tr-label_train, bins = np.arange(-4.5,5.5,1))\n",
    "plt.title('$CCR_1$ loss')\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/test_circular', mae_dirs[100:])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,4)\n",
    "plt.hist(pred_tr-label_train, bins = np.arange(-4.5,5.5,1))\n",
    "plt.title('$MAE$ loss')\n",
    "\n",
    "plt.savefig('circular_extreme_hist.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.subplot(211)\n",
    "img = mpimg.imread('Circular_Data_Extreme_Weights.eps')\n",
    "plt.imshow(img)\n",
    "plt.subplot(212)\n",
    "img = mpimg.imread('Spiral_Data_Extreme_Weights.eps')\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
