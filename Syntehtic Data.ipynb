{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchsample\n",
    "from torchsample import transforms as ts_transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "import importlib\n",
    "\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "num_classes=5\n",
    "\n",
    "\n",
    "#from torchsample.transforms import RangeNorm\n",
    "\n",
    "import functions.fine_tune as ft\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f0a2e905b00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAD8CAYAAAAylrwMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGAjYAQMGDMiIgYAdMGDAgIz43zi9YYgE0yNZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0aa41cf5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_classes = 10\n",
    "dim = 2\n",
    "\n",
    "def generateLinearData(num_samples = 10000, num_classes = num_classes, dim = dim, bound = 5, sigma_noise = .1,rand_label = False):\n",
    "    \n",
    "    if  rand_label:\n",
    "        rand_classes = np.random.permutation(num_classes)\n",
    "    else:\n",
    "        rand_classes = np.arange(num_classes)\n",
    "        \n",
    "    fvec = np.random.rand(dim, num_samples)*bound*2-bound\n",
    "    label = np.dot((np.random.rand(1,dim)*bound*2-bound).reshape(1,-1),fvec)\n",
    "\n",
    "    sorted_idx = np.argsort(label)\n",
    "    bin_size = label.shape[1]/num_classes\n",
    "\n",
    "    for k in range(0, num_classes):\n",
    "        label[0, sorted_idx[0, np.floor(k*bin_size).astype(int):np.floor((k+1)*bin_size).astype(int)]] = rand_classes[k]\n",
    "\n",
    "    label = label.astype(np.int)\n",
    "    n = sigma_noise * np.random.randn(dim, num_samples)\n",
    "    fvec = fvec + n\n",
    "    \n",
    "    return fvec.T, label.reshape(label.shape[1])\n",
    "\n",
    "fvec, label = generateLinearData(rand_label = False)\n",
    "plt.scatter(fvec[:, 0], fvec[:, 1], c=label)\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateCircularData(num_samples = 10000, num_classes = num_classes, dim = dim,\n",
    "                         bound = 5, sigma_noise = .1, rand_label = False):\n",
    "    \n",
    "    if  rand_label:\n",
    "        rand_classes = np.random.permutation(num_classes)\n",
    "    else:\n",
    "        rand_classes = np.arange(num_classes)\n",
    "        \n",
    "    fvec = np.random.rand(dim, num_samples)*bound*2-bound\n",
    "    \n",
    "    fvec_l = np.sum(fvec**2, axis = 0).reshape(1,-1)\n",
    "    print(fvec_l.shape)\n",
    "    label = fvec_l\n",
    "\n",
    "    sorted_idx = np.argsort(label)\n",
    "    bin_size = label.shape[1]/num_classes\n",
    "\n",
    "    for k in range(0, num_classes):\n",
    "        label[0, sorted_idx[0, np.floor(k*bin_size).astype(int):np.floor((k+1)*bin_size).astype(int)]] = rand_classes[k]\n",
    "\n",
    "    label = label.astype(np.int)\n",
    "    n = sigma_noise * np.random.randn(dim, num_samples)\n",
    "    fvec = fvec + n\n",
    "    \n",
    "    return fvec.T, label.reshape(label.shape[1])\n",
    "\n",
    "fvec, label = generateCircularData(num_classes =5, sigma_noise = 0, rand_label = False)\n",
    "plt.scatter(fvec[:, 0], fvec[:, 1], c=label, cmap = plt.get_cmap('Greys'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generateSpiralData(num_samples = 10000, num_classes = 9, dim = 2,\n",
    "                         bound = 1, sigma_noise = .1, rand_label = False):\n",
    "    \n",
    "    if  rand_label:\n",
    "        rand_classes = np.random.permutation(num_classes)\n",
    "    else:\n",
    "        rand_classes = np.arange(num_classes)\n",
    "    \n",
    "    #rand_classes = [1, 1.5, -1, -1.5]\n",
    "    sample_per_class = int(num_samples/num_classes)\n",
    "    num_samples = sample_per_class*num_classes\n",
    "    fvec = np.zeros((dim, sample_per_class*num_classes))\n",
    "    label = np.zeros((1, sample_per_class*num_classes))\n",
    "    \n",
    "    t = np.linspace(0, 10, sample_per_class)\n",
    "    x = t * np.cos(t)\n",
    "    y = t * np.sin(t)\n",
    "    x = x.reshape(1, -1)\n",
    "    y = y.reshape(1, -1)\n",
    "\n",
    "    cons = .7\n",
    "    for k in range(0, num_classes):\n",
    "        r = np.linspace(0.05, 1, sample_per_class)\n",
    "        t = np.linspace(k*cons, (k+6)*cons, sample_per_class)\n",
    "        x = np.cos(t)\n",
    "        y = np.sin(t)\n",
    "        x = x.reshape(1, -1)\n",
    "        y = y.reshape(1, -1)\n",
    "        label[0, k*sample_per_class:(k+1)*sample_per_class] = rand_classes[k]\n",
    "        fvec[0, k*sample_per_class:(k+1)*sample_per_class] = bound * x * r\n",
    "        fvec[1, k*sample_per_class:(k+1)*sample_per_class] = bound * y * r\n",
    "\n",
    "    label = label.astype(np.int)\n",
    "    n = sigma_noise * np.random.randn(dim, num_samples)\n",
    "    fvec = fvec + n\n",
    "    \n",
    "    return fvec.T, label.reshape(label.shape[1])\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "X, y = generateSpiralData(num_classes = 9, sigma_noise = 0.01, rand_label = False)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap = plt.get_cmap('Set1'))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generateSpiralData(num_samples = 10000, num_classes = num_classes, dim = dim,\n",
    "                         bound = 5, sigma_noise = .1, rand_label = False):\n",
    "    \n",
    "    if  rand_label:\n",
    "        rand_classes = np.random.permutation(num_classes)\n",
    "    else:\n",
    "        rand_classes = np.arange(num_classes)\n",
    "    \n",
    "    #rand_classes = [1, 1.5, -1, -1.5]\n",
    "\n",
    "    cons = 4\n",
    "    N = num_samples # number of points per class\n",
    "    D = dim # dimensionality\n",
    "    K = num_classes # number of classes\n",
    "\n",
    "    X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "    y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "    for j in range(K):\n",
    "      ix = range(N*j,N*(j+1))\n",
    "      r = np.linspace(0.0,1,N) # radius\n",
    "      t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*sigma_noise # theta\n",
    "      X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "      y[ix] = j\n",
    "    \n",
    "    label = y.astype(np.int)\n",
    "    fvec = X\n",
    "    \n",
    "    return fvec, label\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.subplot(121)\n",
    "fvec, label = generateCircularData(num_classes =9, sigma_noise = 0, rand_label = False)\n",
    "plt.scatter(fvec[:, 0], fvec[:, 1], c=label, cmap = plt.get_cmap('Set1'))\n",
    "plt.colorbar()\n",
    "plt.subplot(122)\n",
    "fvec, label = generateSpiralData(num_classes = 3, sigma_noise = 0, rand_label = False)\n",
    "plt.scatter(fvec[:, 0], fvec[:, 1], c=label, cmap = plt.get_cmap('Set1'))\n",
    "plt.colorbar()\n",
    "#plt.savefig('circular_vs_spiral.tiff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinal Regression Benchmark Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "num_bins=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank32nh.data\n",
      "bank8FM.data\n",
      "bostonhousing\n",
      "cal_housing.data\n",
      "cpu_act.data\n",
      "cpu_small.data\n",
      "house_16H.data\n",
      "house_8L.data\n",
      "housing\n",
      "results.csv\n",
      "stock\n",
      "stocksdomain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./dataset/regression\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 0)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-35211db07a8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"feat\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./dataset/regression/housing\", sep=',', header=None)\n",
    "train_df=train_df.drop(train_df.columns[-1],axis=1)\n",
    "print(train_df.shape)\n",
    "\n",
    "columns=[\"feat\"+str(k) for k in range(train_df.shape[1])]\n",
    "columns[-1]=\"label\"\n",
    "train_df.columns=columns\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22784, 17)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./dataset/regression/house_16H.data\", sep=',', header=None)\n",
    "#train_df=train_df.drop(train_df.columns[-1],axis=1)\n",
    "\n",
    "columns=[\"feat\"+str(k) for k in range(train_df.shape[1])]\n",
    "columns[-1]=\"label\"\n",
    "train_df.columns=columns\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat0</th>\n",
       "      <th>feat1</th>\n",
       "      <th>feat2</th>\n",
       "      <th>feat3</th>\n",
       "      <th>feat4</th>\n",
       "      <th>feat5</th>\n",
       "      <th>feat6</th>\n",
       "      <th>feat7</th>\n",
       "      <th>feat8</th>\n",
       "      <th>feat9</th>\n",
       "      <th>feat10</th>\n",
       "      <th>feat11</th>\n",
       "      <th>feat12</th>\n",
       "      <th>feat13</th>\n",
       "      <th>feat14</th>\n",
       "      <th>feat15</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15512.0</td>\n",
       "      <td>0.460869</td>\n",
       "      <td>0.049252</td>\n",
       "      <td>0.226470</td>\n",
       "      <td>0.149827</td>\n",
       "      <td>0.752837</td>\n",
       "      <td>0.010057</td>\n",
       "      <td>0.579729</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.075912</td>\n",
       "      <td>0.625318</td>\n",
       "      <td>0.036613</td>\n",
       "      <td>0.991377</td>\n",
       "      <td>0.260116</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>0.774059</td>\n",
       "      <td>130600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1550.0</td>\n",
       "      <td>0.470968</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>0.137419</td>\n",
       "      <td>0.096341</td>\n",
       "      <td>0.862581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.695142</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.043551</td>\n",
       "      <td>0.064263</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.994975</td>\n",
       "      <td>0.285266</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>40500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4741.0</td>\n",
       "      <td>0.485341</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.189412</td>\n",
       "      <td>0.135656</td>\n",
       "      <td>0.856992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683584</td>\n",
       "      <td>0.004143</td>\n",
       "      <td>0.027965</td>\n",
       "      <td>0.065796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997411</td>\n",
       "      <td>0.315433</td>\n",
       "      <td>0.065116</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>28700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>467.0</td>\n",
       "      <td>0.498929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100642</td>\n",
       "      <td>0.085470</td>\n",
       "      <td>0.907923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.018293</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>310.0</td>\n",
       "      <td>0.474194</td>\n",
       "      <td>0.680645</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.128834</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.756302</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.077519</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.147287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat0     feat1     feat2     feat3     feat4     feat5     feat6  \\\n",
       "0  15512.0  0.460869  0.049252  0.226470  0.149827  0.752837  0.010057   \n",
       "1   1550.0  0.470968  0.002581  0.137419  0.096341  0.862581  0.000000   \n",
       "2   4741.0  0.485341  0.000211  0.189412  0.135656  0.856992  0.000000   \n",
       "3    467.0  0.498929  0.000000  0.100642  0.085470  0.907923  0.000000   \n",
       "4    310.0  0.474194  0.680645  0.225806  0.128834  0.896774  0.000000   \n",
       "\n",
       "      feat7     feat8     feat9    feat10    feat11    feat12    feat13  \\\n",
       "0  0.579729  0.003251  0.075912  0.625318  0.036613  0.991377  0.260116   \n",
       "1  0.695142  0.005025  0.043551  0.064263  0.003350  0.994975  0.285266   \n",
       "2  0.683584  0.004143  0.027965  0.065796  0.000000  0.997411  0.315433   \n",
       "3  0.780488  0.006098  0.018293  0.057471  0.000000  1.000000  0.149425   \n",
       "4  0.756302  0.008403  0.016807  0.077519  0.672269  0.991597  0.147287   \n",
       "\n",
       "     feat14    feat15     label  \n",
       "0  0.052246  0.774059  130600.0  \n",
       "1  0.060606  0.142857   40500.0  \n",
       "2  0.065116  0.687500   28700.0  \n",
       "3  0.139535  1.000000   28500.0  \n",
       "4  0.000000  0.000000   24100.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples per class is 2278.4\n",
      "[0.0, 14999.000099999999, 18400.000199999999, 23400.0003, 28100.000400000001, 33200.000500000002, 39800.000599999999, 49600.000699999997, 65300.000800000002, 102600.0009, 500001.00099999999]\n",
      "[0.0, 14999.000099999999, 18400.000199999999, 23400.0003, 28100.000400000001, 33200.000500000002, 39800.000599999999, 49600.000699999997, 65300.000800000002, 102600.0009, 500002.00099999999]\n",
      "500001.0\n",
      "Unique labels are [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFHRJREFUeJzt3XuUnVV5x/Hvk2TQCbeAjC4I2OANFhcRnCpelsWoC0Wq\nYKFqtSpYaesFtN6g1qpddmlFW63VatR6KQoqBLyLiOjyVnQioJBARYhAQBmsgEKQkDz947yBk8nk\nzDsn55055+zvZ61Z85593nP23sms37yz3332jsxEkjT8Fsx3AyRJc8PAl6RCGPiSVAgDX5IKYeBL\nUiEMfEkqhIEvSYUw8CWpEAa+JBVi0Xw3oN0ee+yRy5Ytm+9mSNLAWLVq1S2ZOVbn3L4K/GXLljEx\nMTHfzZCkgRERv6x7rkM6klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRB9NS1T9S079Svz3QQNqAC6\n2eduW6+LgKkb5y1aENyzqbvd9HbcYSGbMlm/YVPHejdbGLCxi6qWLhnl9UfuB8DbvnQFv71zw73P\nLRkd4a3POpBjDl3KeZes4/Tzr+LGW9ez6+gI6zds5A/3bNrqvZ68/xgXXTnJulvXszCCjZn31nHM\noUsBOO+SdVvVNdXadz5z9p2pKfppi8Px8fF0Hv7MDHupN0YWBJuAjdP8chpZEDz3Mftwzqp1rN+w\nses6RkcW8o7nHAzA68++jA01fjvNJvQjYlVmjtc51yt8ScXa0OGvkA2bkjMvvp6N23lRvH7DRk4/\n/6rWe3bzp0gPGfiStA3bG/ab3Xjr+p68z/bypq0kbcPCiJ68z15LRtlryWhP3mt7GPiSijWyIFi4\nYPpQH1kQPP+x+zA6snC76hgdWcjrj9yP1x+5HyMLe/MLpFsG/gBq8i6+hl+3kbOt1013EbxoGyFa\nx447LGR05L5omumdus3QpUtGOf34Q3jP8Yew2+KRLZ5bMjrC6ccfwtuPOZh3POdgli4ZJary+y3a\nOjaXLhnlhYc/mKXVVfzmvwyWLhnlHc85mGMOXcoxhy7l9OO2rmsqZ+lIkqY1m1k6XuFLUiEMfEkq\nhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVotEdryLi\nNcBf0dqD+GfACZl5V5N19pu53H/WZZMlddLYFX5ELAVOBsYz8yBgIfC8purrR3O92bibm0vqpOkh\nnUXAaEQsAhYDNzZcnyRpGxoL/MxcB7wbuA64CbgtM78x9byIOCkiJiJiYnJysqnmSFLxmhzS2Q14\nNrAvsBewY0S8cOp5mbkiM8czc3xsbKyp5khS8Zoc0nkqcG1mTmbmBmAl8PgG65MkddBk4F8HHB4R\niyMigKcAaxqsr+/M9awZZ+lI6qSxaZmZeXFEnA38BLgHuARY0VR9/coQltQvGp2Hn5lvAd7SZB2S\npHr8pK0kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+S\nCmHgS1IhGl0tc5htz4bhLpksaT54hd+F7Qn7Xrxekrph4EtSIQx8SSqEgS9JhTDwJakQBn4XtneW\njbN0JM0Hp2V2ydCWNGi8wpekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY\n+JJUCANfkgph4EtSIQx8SSqEgS9JhWh0eeSIWAJ8FDgISODEzPxhk3X22mw2HHfJZEn9rOkr/PcB\nX8/M/YFDgDUN19dTswn7bs6XpLnU2BV+ROwKPAl4CUBm3g3c3VR9kqTOmrzC3xeYBD4eEZdExEcj\nYsepJ0XESRExERETk5OTDTZHksrWZOAvAg4D/jMzDwXuAE6delJmrsjM8cwcHxsba7A5klS2bQ7p\nRMT7ad1onVZmnjzDe98A3JCZF1ePz2aawJckzY1OY/gT2/PGmfmriLg+IvbLzKuApwCrt+c959ra\ndz7TWTqShsY2Az8zP9n+OCIWZ+ads3z/VwGfjogdgGuAE2bfxPlliEsaFjOO4UfE4yJiNXBl9fiQ\niPhgnTfPzEur8flHZuYxmfnb7WyvJKlLdW7avhc4EvgNQGZeRmu6pSRpgNSapZOZ108p2thAWyRJ\nDarzwavrI+LxQEbECHAKA/aJWUlSvSv8vwFeASwFbgQeVT2WJA2QGa/wM/MW4AVz0BZJUoPqzNJ5\nSER8KSImI+LmiPhCRDxkLhonSeqdOkM6nwE+B+wJ7AV8HjizyUZJknqvTuAvzsz/zsx7qq8zgPs3\n3TBJUm91Wktn9+rwaxFxKnAWrbV1ngt8dQ7aJknqoU43bVfRCvioHv9123MJnNZUoyRJvddpLZ19\n57IhkqRm1drxKiIOAg6gbew+Mz/VVKMkSb03Y+BHxFuAI2gF/leBZwDfAwx8SRogda7wj6O1Afkl\nmXlCRDwIOKPZZs2fOuvfu2SypEFUZ1rm+szcBNwTEbsANwP7NNus+VF3s5PZbIoiSf2izhX+REQs\nAT5Ca+bO74EfNtoqSVLP1VlL5+XV4Yci4uvALsAtjbZKktRztWbpbJaZawEi4jrgwU00SJLUjFob\noEwjZj5FktRPug387Gkr+kTd2TfO0pE0iDqtpfN+pg/2AJY01qJ5ZphLGladxvAnunxOktSHOq2l\n88m5bIgkqVndjuFLkgaMgS9JhTDwJakQ3czSASAzT26kRZKkRnQ7S0eSNGCcpSNJhaizAcoY8Ea2\n3vFqeYPtkiT1WJ2btp8G1gD7Am8D1gI/brBNkqQG1An8B2Tmx4ANmfmdzDwR8OpekgZMneWRN1Tf\nb4qIZwI3Ars31yRJUhPqBP7bI2JX4LXA+2ltgPKaRlslSeq5Ojtefbk6vA148mwriIiFtKZ4rsvM\no2f7eklSb9SdpfMyYFn7+dVYfh2n0Lrpu0sX7ZsTM21K7pLJkoZBnZu2XwB2Bb4JfKXta0YRsTfw\nTOCj3TawaTOFfd1zJKnf1RnDX5yZb+zy/d8LvAHYucvXS5J6pM4V/pcj4qjZvnFEHA3cnJmrZjjv\npIiYiIiJycnJ2VYjSaqpTuCfQiv010fE7RHxu4i4vcbrngA8KyLWAmcByyPijKknZeaKzBzPzPGx\nsbFZNV6SVN+MgZ+ZO2fmgswczcxdqscz3oDNzNMyc+/MXAY8D/hWZr6wB22WJHVhm4EfEftX3w+b\n7mvumtisOjNwnKUjaRh0umn7WlrTMd8zzXPJLJZXyMxvA9+eTcPmkoEuqQSdlkd+WfV91h+2kiT1\nn047Xj2n0wszc2XvmyNJakqnIZ0/rb4/EHg88K3q8ZOBHwAGviQNkE5DOicARMQ3gAMy86bq8Z7A\nJ+akdZKknqkzD3+fzWFf+TXw4IbaI0lqSJ2lFS6MiPOBM6vHz6W1ro4kaYDUWR75lRFxLPCkqmhF\nZp7bbLMkSb3WMfCrtey/WU3NNOQlaYB1HMPPzI3ApmrHK0nSAKszhv974GcRcQFwx+bCzDy5sVZJ\nknquTuCvxDn3kjTw6gT+Z4GHVcdXZ+ZdDbZHktSQTqtlLoqIdwE3AJ8EPgVcHxHvioiRuWqgJKk3\nOt20PR3YHdg3Mx+dmYcBDwWWAO+ei8ZJknqn05DO0cAjMjM3F2Tm7RHxt8CVtHbCGlgzbUzuksmS\nhk2nK/xsD/u2wo201sMfWDOFfd1zJGmQdAr81RHxoqmFEfFCWlf4kqQB0mlI5xXAyog4EVhVlY0D\no8CxTTdMktRbnZZHXgc8NiKWAwdWxV/NzAvnpGWSpJ6qs3jat7hv8xNJ0oCqsx7+0KkzA8dZOpKG\nTZ1P2g4lA11SaYq8wpekEhn4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJU\nCANfkgph4EtSIQx8SSpEY4EfEftExEURsToiroiIgd70XJIGXZPLI98DvDYzfxIROwOrIuKCzFzd\nYJ0dzbQxuUsmSxpmjV3hZ+ZNmfmT6vh3wBpgaVP1zWSmsK97jiQNqjkZw4+IZcChwMVzUZ8kaWuN\nB35E7AScA7w6M2+f5vmTImIiIiYmJyebbo4kFavRwI+IEVph/+nMXDndOZm5IjPHM3N8bGysyeZI\nUtGanKUTwMeANZn5r03VI0mqp8kr/CcAfwksj4hLq6+jGqyvozozcJylI2mYNTYtMzO/B0RT798N\nA11SyfykrSQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgD\nX5IKYeBLUiEMfEkqRGPLI/eTTpuTu2SypFIM/RV+p7Cv87wkDYuhD3xJUouBL0mFMPAlqRAGviQV\nYugDf6ZZOM7SkVSKIqZlGuqSVMAVviSpxcCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQ\nBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEaDfyIeHpEXBURV0fEqU3WJUnqrLHlkSNiIfAB4GnADcCP\nI+KLmbm6l/VMtwn51OWQ65wjScOuySv8xwBXZ+Y1mXk3cBbw7F5WMF2QTy2vc44klaDJwF8KXN/2\n+IaqTJI0D+b9pm1EnBQRExExMTk5Od/NkaSh1WTgrwP2aXu8d1W2hcxckZnjmTk+NjbWYHMkqWxN\nBv6PgYdHxL4RsQPwPOCLDdYnSeqgscDPzHuAVwLnA2uAz2XmFb2sY1szbdrL65wjSSWIzJzvNtxr\nfHw8JyYm5rsZkjQwImJVZo7XOXfeb9pKkuaGgS9JhTDwJakQBr4kFcLAl6RC9NUsnYiYBH7Z5cv3\nAG7pYXMGgX0efqX1F+zzbP1RZtb61GpfBf72iIiJulOThoV9Hn6l9Rfsc5Mc0pGkQhj4klSIYQr8\nFfPdgHlgn4dfaf0F+9yYoRnDlyR1NkxX+JKkDgY+8Adxo/SI+K+IuDkiLm8r2z0iLoiIn1ffd2t7\n7rSqf1dFxJFt5Y+OiJ9Vz/17RERVfr+I+GxVfnFELGt7zYurOn4eES+eo/7uExEXRcTqiLgiIk4p\noM/3j4gfRcRlVZ/fNux9bqt7YURcEhFfrh4PdZ8jYm3V1ksjYqKv+5yZA/sFLAR+ATwE2AG4DDhg\nvttVo91PAg4DLm8rexdwanV8KvAv1fEBVb/uB+xb9Xdh9dyPgMOBAL4GPKMqfznwoer4ecBnq+Pd\ngWuq77tVx7vNQX/3BA6rjncG/rfq1zD3OYCdquMR4OKq3UPb57a+/x3wGeDLw/6zXdW9FthjSllf\n9nlOfgAa/Id+HHB+2+PTgNPmu101276MLQP/KmDP6nhP4Krp+kRrf4HHVedc2Vb+fODD7edUx4to\nfaAj2s+pnvsw8Px56PsXgKeV0mdgMfAT4LHD3mdaO9tdCCznvsAf9j6vZevA78s+D/qQzjBtlP6g\nzLypOv4V8KDqeFt9XFodTy3f4jXZ2ojmNuABHd5rzlR/jh5K64p3qPtcDW1cCtwMXJCZQ99n4L3A\nG4BNbWXD3ucEvhkRqyLipKqsL/u8qH6fNFcyMyNi6KZPRcROwDnAqzPz9mqIEhjOPmfmRuBREbEE\nODciDpry/FD1OSKOBm7OzFURccR05wxbnytPzMx1EfFA4IKIuLL9yX7q86Bf4dfaKH1A/Doi9gSo\nvt9clW+rj+uq46nlW7wmIhYBuwK/6fBejYuIEVph/+nMXFkVD3WfN8vMW4GLgKcz3H1+AvCsiFgL\nnAUsj4gzGO4+k5nrqu83A+cCj6Ff+zwXY1wNjp0tonWjYl/uu2l74Hy3q2bbl7HlGP7pbHmT513V\n8YFseZPnGrZ9k+eoqvwVbHmT53PV8e7AtbRu8OxWHe8+B30N4FPAe6eUD3Ofx4Al1fEo8F3g6GHu\n85T+H8F9Y/hD22dgR2DntuMf0PrF3pd9nrMfgAb/wY+iNevjF8Cb5rs9Ndt8JnATsIHWuNtLaY3J\nXQj8HPhm+38c8Kaqf1dR3bmvyseBy6vn/oP7Pkh3f+DzwNXVD9FD2l5zYlV+NXDCHPX3ibTGOX8K\nXFp9HTXkfX4kcEnV58uBf6zKh7bPU/p/BPcF/tD2mdYMwcuqryuoMqhf++wnbSWpEIM+hi9JqsnA\nl6RCGPiSVAgDX5IKYeBLUiEMfPW1iNg7Ir5QrQb4i4h4X0TssI1z94qIs2u851erT7920563RsTr\n6pZ3eJ/f96JeaTYMfPWtannYlcB5mflw4BHATsA/T3Puosy8MTOPm+l9M/OobH36VSqKga9+thy4\nKzM/DveuTfMa4MSIWBwRL4mIL0bEt4ALI2JZVHsMVM9/Llpr8J9brSM+Xj23NiL2qM5fExEfidaa\n9d+IiNHqnJdFxI+jtZ79ORGxuJsORMR51aJaV7QtrLX5uX+ryi+MiLGq7KER8fXqNd+NiP27/ceT\npjLw1c8OBFa1F2Tm7cB1wMOqosOA4zLzT6a89uXAbzPzAODNwKO3UcfDgQ9k5oHArcCfVeUrM/OP\nM/MQYA2tT0N348TMfDStT1GeHBEPqMp3BCaqer8DvKUqXwG8qnrN64APdlmvtBVXy9SguyAz/2+a\n8icC7wPIzMsj4qfbeP21mXlpdbyK1hpHAAdFxNuBJbSGkc7vsn0nR8Sx1fE+tH7B/IbW8sGfrcrP\nAFZWq4k+Hvh820qi9+uyXmkrBr762WpgizH5iNgFeDCttUMOA+7Yzjr+0Ha8kdZCZwCfAI7JzMsi\n4iW01oaZlWqJ4KfS2rzizoj4Nq11UaaTtP7ivjUzHzXbuqQ6HNJRP7sQWBwRL4LWhiLAe4BPZOad\nM7z2+8CfV687ADh4lnXvDNxULev8glm+drNdaQ0r3VmNxR/e9twC7vtl9hfA96rhqmsj4viq3RER\nh3RZt7QVA199K1sr+x0LHB8RP6e1KupdwN/XePkHgbGIWA28ndZKhrfNovo309qV6/vAlTOcu9k/\nRMQNm7+ArwOLImIN8E7gf9rOvQN4THWTeTnwT1X5C4CXRsTm1RefPYs2Sx25WqaGUvXXwEhm3hUR\nD6W1RO1+mXn3PDdNmjeO4WtYLQYuqoZkAni5Ya/SeYUvSYVwDF+SCmHgS1IhDHxJKoSBL0mFMPAl\nqRAGviQV4v8BuofUIUTgwIAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa8ee919c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_df['label_ord']=train_df['label']\n",
    "label=train_df.label.values\n",
    "sorted_idx=np.argsort(train_df.label.values)\n",
    "num_samples_per_class=train_df.shape[0]/num_bins\n",
    "print('Number of Samples per class is ' + str(num_samples_per_class))\n",
    "bins=[(k*1e-4+label[sorted_idx[np.round(k*num_samples_per_class-1).astype(np.int)]]) for k in range(1,num_bins+1)]\n",
    "bins.insert(0,0.0)\n",
    "print(bins)\n",
    "bins[-1]=bins[-1]+1\n",
    "print(bins)\n",
    "\n",
    "label_ord=label.copy()\n",
    "k = 10\n",
    "\n",
    "print(label[sorted_idx[np.round(k*num_samples_per_class-1).astype(np.int)]])\n",
    "for k in range(num_bins):\n",
    "    #print(np.all([label>=bins[k], label<bins[k+1]],0))\n",
    "    label_ord[np.all([label>=bins[k], label<bins[k+1]],0)]=k\n",
    "    \n",
    "print('Unique labels are ' + str(np.unique(label_ord)))\n",
    "\n",
    "\n",
    "train_df['label_ord']=label_ord\n",
    "#print(train_df.head())\n",
    "\n",
    "plt.scatter(label,label_ord)\n",
    "plt.xlabel('Original Label')\n",
    "plt.ylabel('Ordinal Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat0</th>\n",
       "      <th>feat1</th>\n",
       "      <th>feat2</th>\n",
       "      <th>feat3</th>\n",
       "      <th>feat4</th>\n",
       "      <th>feat5</th>\n",
       "      <th>feat6</th>\n",
       "      <th>feat7</th>\n",
       "      <th>feat8</th>\n",
       "      <th>feat9</th>\n",
       "      <th>feat10</th>\n",
       "      <th>feat11</th>\n",
       "      <th>feat12</th>\n",
       "      <th>feat13</th>\n",
       "      <th>feat14</th>\n",
       "      <th>feat15</th>\n",
       "      <th>label</th>\n",
       "      <th>label_ord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15512.0</td>\n",
       "      <td>0.460869</td>\n",
       "      <td>0.049252</td>\n",
       "      <td>0.226470</td>\n",
       "      <td>0.149827</td>\n",
       "      <td>0.752837</td>\n",
       "      <td>0.010057</td>\n",
       "      <td>0.579729</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.075912</td>\n",
       "      <td>0.625318</td>\n",
       "      <td>0.036613</td>\n",
       "      <td>0.991377</td>\n",
       "      <td>0.260116</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>0.774059</td>\n",
       "      <td>130600.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1550.0</td>\n",
       "      <td>0.470968</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>0.137419</td>\n",
       "      <td>0.096341</td>\n",
       "      <td>0.862581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.695142</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.043551</td>\n",
       "      <td>0.064263</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.994975</td>\n",
       "      <td>0.285266</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>40500.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4741.0</td>\n",
       "      <td>0.485341</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.189412</td>\n",
       "      <td>0.135656</td>\n",
       "      <td>0.856992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683584</td>\n",
       "      <td>0.004143</td>\n",
       "      <td>0.027965</td>\n",
       "      <td>0.065796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997411</td>\n",
       "      <td>0.315433</td>\n",
       "      <td>0.065116</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>28700.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>467.0</td>\n",
       "      <td>0.498929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100642</td>\n",
       "      <td>0.085470</td>\n",
       "      <td>0.907923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.018293</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28500.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>310.0</td>\n",
       "      <td>0.474194</td>\n",
       "      <td>0.680645</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.128834</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.756302</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.077519</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.147287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24100.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat0     feat1     feat2     feat3     feat4     feat5     feat6  \\\n",
       "0  15512.0  0.460869  0.049252  0.226470  0.149827  0.752837  0.010057   \n",
       "1   1550.0  0.470968  0.002581  0.137419  0.096341  0.862581  0.000000   \n",
       "2   4741.0  0.485341  0.000211  0.189412  0.135656  0.856992  0.000000   \n",
       "3    467.0  0.498929  0.000000  0.100642  0.085470  0.907923  0.000000   \n",
       "4    310.0  0.474194  0.680645  0.225806  0.128834  0.896774  0.000000   \n",
       "\n",
       "      feat7     feat8     feat9    feat10    feat11    feat12    feat13  \\\n",
       "0  0.579729  0.003251  0.075912  0.625318  0.036613  0.991377  0.260116   \n",
       "1  0.695142  0.005025  0.043551  0.064263  0.003350  0.994975  0.285266   \n",
       "2  0.683584  0.004143  0.027965  0.065796  0.000000  0.997411  0.315433   \n",
       "3  0.780488  0.006098  0.018293  0.057471  0.000000  1.000000  0.149425   \n",
       "4  0.756302  0.008403  0.016807  0.077519  0.672269  0.991597  0.147287   \n",
       "\n",
       "     feat14    feat15     label  label_ord  \n",
       "0  0.052246  0.774059  130600.0        9.0  \n",
       "1  0.060606  0.142857   40500.0        6.0  \n",
       "2  0.065116  0.687500   28700.0        4.0  \n",
       "3  0.139535  1.000000   28500.0        4.0  \n",
       "4  0.000000  0.000000   24100.0        3.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fa8e8356f98>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAF6CAYAAAAnAED0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XucXHV9//HXe7O5k4TcCIEQEi4/FKjc0gCKVAgSQCvU\nooXfT42USiu0hXor2FbwgpW2glpFjSUQkCoRpeIFMXKnSiBBEEICCbeQhNxDbuS2m8/vj/luMrvZ\n2Z3dnZkzs/N+Ph7zmDPf+Z4znznkwXvPOd/5HkUEZmZmlr2GrAswMzOzHIeymZlZlXAom5mZVQmH\nspmZWZVwKJuZmVUJh7KZmVmVcCibmZmVkaTLJT0rab6kKzrq61A2MzMrE0lHAx8DJgPHAO+VdFih\n/g5lMzOz8nkrMCci3oyIJuAh4P2FOjuUzczMyudZ4J2SRkoaBJwDHFSoc2PFyqoho0aNigkTJmRd\nhpmZJfPmzVsTEaNLvd3+k98Ruzas7/b6TS8smA9sy2uaHhHTW15ExAJJ1wG/BrYATwHNhbbnUG7H\nhAkTmDt3btZlmJlZIunVcmx314b1jPzOf3d7/ZWnH7ctIiZ11CcibgJuApD0ZWBpob4OZTMzszKS\ntF9ErJI0ntz15JMK9XUom5mZldePJY0EdgKXRcQbhTo6lM3MzMooIt5ZbF+PvjYzM6sSDmUzM7Mq\n4VA2MzOrEg5lMzOzKuFQNjMzqxIOZTMzsypRsVCW9IqkZyQ9JWluahshabakRel5eF7/qyQtlvS8\npKl57Sek7SyW9A1JSu39Jd2R2udImpC3zrT0GYskTavUdzYzM+uKSh8pnxYRx+ZNSXYlcF9EHA7c\nl14j6UjgAuAo4CzgRkl90jrfJncbrMPT46zUfjGwPiIOA24ArkvbGgFcDZxI7tZZV+eHv5mZWbXI\n+vT1ucDMtDwTOC+v/YcRsT0iXgYWA5MljQWGRsRjERHArW3WadnWncCUdBQ9FZgdEesiYj0wmz1B\nbmZmZbL/A0+1eljnKhnKAfxG0jxJl6S2MRHxelpeAYxJywcCr+WtuzS1HUjribxb2lutk+5ZuQEY\n2cG2WpF0iaS5kuauXr26e9/QzMysByo5zeYpEbFM0n7AbEkL89+MiJAUFaynlXSrrekAkyZNyqwO\nM7PeSFkXUCMqFsoRsSw9r5J0F7nruysljY2I19Op6VWp+zJa3wR6XGpblpbbtuevs1RSIzAMWJva\n39VmnQdL983MzKw9L7zzj/jRinWcvO8+HLnPwKzLqQkVOX0tabCkIS3LwJnAs8DdQMto6GnAT9Py\n3cAFaUT1RHIDuh5Pp7o3SjopXS/+SJt1WrZ1PnB/uu58L3CmpOFpgNeZqc3MzMroEwuX8LlFyzhn\n3gvsCp+ALEaljpTHAHelXy81Av8dEb+S9AQwS9LFwKvABwEiYr6kWcBzQBO5W101p21dCtwCDATu\nSQ/I3UD6NkmLgXXkRm8TEeskfRF4IvX7QkSsK+eXNTMzmL9pK81A866ACJBPYnemIqEcES8Bx7TT\nvhaYUmCda4Fr22mfCxzdTvs24AMFtjUDmNG1qs3MrCfOGT2M6a+t5oAB/ZADuSi+n7KZmZXFPx96\nAH++/wgOHuhQLpZD2czMykKSB3h1UdaTh5iZmVniUDYzM6sSDmUzMyuLBZu38uE/vMRdK/2Dl2I5\nlM3MrCw+t2gZs9du5O8WLMm6lJrhUDYzs7IIchOGDGhw1BTLe8rMzMqif0OD57zuIv8kyszMyuL6\nt4zn+8vXMmXk0KxLqRkOZTMzK4sx/fvyyYn7Z11GTfHpazMzK4uH1m3ilDkLuHHJqs47G+BQNjOz\nMvncoqUsfnM7X35pedal1AyHspmZlcWybTsBaKrzuzZK+gdJ8yU9K+kHkgYU6utrymZmVhbNNXAP\n5UN4kRvjz7u9/hmdvC/pQODvgSMjYmu6LfEF5G5BvBcfKZuZWVm8ZXDugHB03z4ZV5K5RmCgpEZg\nEFDwfL5D2czMyuIzh4zlqMED+JdDD8y6lMxExDLgP4AlwOvAhoj4daH+DmUzMyuLL7y4nPlbtvH5\nF3v1QK9RkubmPS7Jf1PScOBcYCJwADBY0ocKbczXlM3MrCzePXIoC7ds410jhmRdSjmtiYhJHbx/\nBvByRKwGkPQT4O3A99vr7FA2M7Oy+OyhB3DFhP0Z1KeuT8ouAU6SNAjYCkwB5hbqXNd7yszMyqvO\nA5mImAPcCTwJPEMud6cX6u8jZTMzszKKiKuBq4vpW99/wpiZWVlFDfxWuZo4lM3MrCz+7aXlHPDg\n03xm4WtZl1IzHMpmZlYW33ltNQH894q1WZdSMxzKZmZWFrun2fQZ7KI5lM3MrCyOGDyAPoLDBvfP\nupSa4dHXZmZWFrOOPYwH123i1N49eUhJOZTNzKwshjX24fSRQxnaWPc3pCiaT1+bmVlZfOr51zji\nkWf4lEdfF82hbGZmZfHw+s0E8Mj6TVmXUjMcymZmVhbHDxkIwEnDBmdcSe1wKJuZWVn8fPUGAO5a\n9UbGldQOh7KZmZVFc3re4ak2i+ZQNjOzshiW7hB1yMB+GVdSOxzKZmZWFtt37QJ8U4qucCibmVlZ\nbEtZ/Oq2ndkWUkMcymZmVhaj06QhfzJ8n4wrqR0OZTMzK4vVTbmhXk9sfDPjSmqHQ9nMzEouIlBa\nHt7oqCmW95SZmZXcrrzBXX0bHDXF8p4yM7OSa5Domw6VDx7gn0QVy6FsZmYl1xyww7+E6jKHspmZ\nlVxjg3jPqGEM7dPAReNGZV1OzXAom5lZyW3a2cQv1mxgY/MuPrVwadbl1AyHspmZldyy7dt3L6/d\n2ZRhJbWlMesCzMys99mv357BXYf075thJR3btGkkjzz84R5s4fMlqwV8pGxmZmXQDLt/pzx5X8/o\nVSyHspmZldzAhgb6KhcyY/yTqKI5lM3MrOR27NpFc0AA65t8TblYDmUzMyu5zc3pto0Z15E1SUdI\neirvsVHSFYX6VzSUJfWR9HtJP0+vR0iaLWlReh6e1/cqSYslPS9pal77CZKeSe99Q5JSe39Jd6T2\nOZIm5K0zLX3GIknTKveNzczq04i+jTSki8oH9q/f09cR8XxEHBsRxwInAG8CdxXqX+kj5cuBBXmv\nrwTui4jDgfvSayQdCVwAHAWcBdwoqU9a59vAx4DD0+Os1H4xsD4iDgNuAK5L2xoBXA2cCEwGrs4P\nfzMzK73ntmxlZzpM/s3ajdkWUz2mAC9GxKuFOlQslCWNA94D/Fde87nAzLQ8Ezgvr/2HEbE9Il4G\nFgOTJY0FhkbEYxERwK1t1mnZ1p3AlHQUPRWYHRHrImI9MJs9QW5mZmXw1sEDGdQgGoALx47Iupxy\nGiVpbt7jkg76XgD8oKONVfJ3yl8DPgMMyWsbExGvp+UVwJi0fCDwWF6/paltZ1pu296yzmsAEdEk\naQMwMr+9nXXMzKwMhjT24fIJY1iweStTRw3LupxyWhMRkzrrJKkf8D7gqo76VeRIWdJ7gVURMa9Q\nn3Tkm9mYAEmXtPyls3r16qzKMDPrFV58cxtffXklP1u1gZuWrsm6nGpwNvBkRKzsqFOlTl+/A3if\npFeAHwKnS/o+sDKdkiY9r0r9lwEH5a0/LrUtS8tt21utI6kRGAas7WBbrUTE9IiYFBGTRo8e3f1v\namZmDGgQOyNoJndzCuNCOjl1DRUK5Yi4KiLGRcQEcufU74+IDwF3Ay2joacBP03LdwMXpBHVE8kN\n6Ho8nereKOmkdL34I23WadnW+ekzArgXOFPS8DTA68zUZmZmZbKlac+JzzebmzOsJHuSBgPvBn7S\nWd+s577+CjBL0sXAq8AHASJivqRZwHNAE3BZRLT8V70UuAUYCNyTHgA3AbdJWgysIxf+RMQ6SV8E\nnkj9vhAR68r9xczM6tmW5ubd1yO3N9f3r5UjYgu5MU6dqngoR8SDwINpeS25IeLt9bsWuLad9rnA\n0e20bwM+UGBbM4AZ3a3ZzMy6ZkcEIjdQKOp+CpHieUYvMzMruUMH9qdPupR84rDB2RZTQxzKZmZW\ncrcuX0vLZeVvLvEvWorlUDYzs5L7oyEDdi8fM2RghpXUFoeymZmV3Etv7ti9vC12ZVhJbXEom5lZ\nye2IPYO7wuO8iuZQNjOzkjtk4J47Q00aOijDSmqLQ9nMzErugXWbdi8/tmFLhpXUFoeymZmVXF/t\nmVpzdN++GVZSWxzKZmZWchMG9d+9/JZ9BnTQ0/JlPc2mmZn1Qv937Ehmr9nIgAbxvv2GZ11OzXAo\nm5lZyT20biOPvrEZgIWbt3KcZ/Uqik9fm5lZyd28bO3u5R+v9D2AiuVQNjOzkrtw/z2nrD90wKgM\nK6ktPn1tZmYlt7F5F32BBsH6pvq+n3JXOJTNzKzkThw2mD4NYp8+fThysEdfF8unr83MrOQe37CF\nHbuCN5qaWLJtR+crGOBQNjOzMrht+Vp2AU0Bv/eMXkVzKJuZWcm9snV7u8vWMYeymZmVXt6toQ7P\nm93LOuaBXmZmVnJTRw3jrtUbADht1LCMqylsVAzlr7ZN6fb6n+fzJazGR8pmZlYGHxw7kkbg8IH9\n2K+fb0hRLIeymZmV3N8+9ypNwKKtO3hm45tZl1MzHMpmZlZyQxr3xMsBA3ykXCyHspmZldz4Af12\nLw9scNQUy3vKzMxKbuGWbbuXdxId9LR8DmUzMyu57bv2BHF/KcNKaotD2czMSm5EY5/dy/379Omg\nZ+8naV9Jd0paKGmBpJML9fXvlM3MrOQ27dq150UE1PfR8teBX0XE+ZL6AYMKdfSRspmZldywdKQs\noKGOA1nSMOBU4CaAiNgREW8U6u9QNjOzknt9204AAlDvDuVRkubmPS5p8/5EYDVws6TfS/ovSYML\nbcyhbGZmJfdm1M2I6zURMSnvMb3N+43A8cC3I+I4YAtwZaGNOZTNzKzkRqfT1x64xFJgaUTMSa/v\nJBfS7XIom5lZyY3ol4vjIY31PfI6IlYAr0k6IjVNAZ4r1N9/xJiZWcn1SZeRfeQHwN8Bt6eR1y8B\nFxXq6FA2M7OSO2LwAJ7bsp237jMg61IyFxFPAZOK6es/YszMrOT+Z1XuXsqPvrEl40pqi0PZzMxK\nrm7GXpeYQ9nMzKxKOJTNzKykduVPsWld4lA2M7OSWr5tW+edrF0OZTMzK6lnNm/PuoSa5VA2M7OS\nGtRnT7T0z7COWuRQNjOzkpq1fO3uZR8zd41D2czMSuqNpubdy30zrKMWOZTNzKykjh82aPfy5GED\nM6yk9hScZlPSIcVsICJeKl05ZmZW657etGf09cZmTyPSFR3Nfb2YdH/qDvoEUN+3ADEzs1bOHjWU\nX6/bBMD7R++bcTW1pWAoR4RPbZuZWZct2rpj9/IbzZ5IpCu6FLySDpJ0UrmKMTOz2tdXe06w1vv9\nlLuqqFCWNF7S/wILgd+ktvMl/Vc5izMzs9qzZkfT7uWhDT7p2hXF7q3vAr8AhgA7U9ts4N3lKMrM\nzGrXrBXrdi/fvHxNhpXUnmJDeTLwlYjYRbojV0RsAIYVs7KkAZIel/S0pPmSPp/aR0iaLWlReh6e\nt85VkhZLel7S1Lz2EyQ9k977hpQ7TyKpv6Q7UvscSRPy1pmWPmORpGlFfmczM+uGnXnL4wf4l8pd\n0dHo63wrgcOAF1oaJB0JLCly/e3A6RGxWVJf4FFJ9wDvB+6LiK9IuhK4EvjHtO0LgKOAA4DfSPo/\nEdEMfBv4GDAH+CVwFnAPcDGwPiIOk3QBcB3wF5JGAFcDk8j9QTFP0t0Rsb7I2s3MrAuOGNiX57fm\novnS8WMyrqZj67ev4I6Xr8u6jN2KPVL+D+Dnki4CGiVdCNxBLvg6FTmb08u+6RHAucDM1D4TOC8t\nnwv8MCK2R8TL5H6eNVnSWGBoRDwWEQHc2madlm3dCUxJR9FTgdkRsS4F8WxyQW5mZmWwY9ee3yb3\n9TXlLinqSDkiZkhaC/w18BowDfiXiPifYj9IUh9gHrkj7m9FxBxJYyLi9dRlBdDyJ9WBwGN5qy9N\nbTvTctv2lnVeS/U2SdoAjMxvb2ed/PouAS4BGD9+fLFfy8zM2tiaF8p9OprpwvZS9J8wEfHTiDgn\nIo6KiLO6Eshp/eaIOBYYR+6o9+g27wfpenUWImJ6REyKiEmjR4/Oqgwzs5p39ujccCMBRwz2NJtd\nUXQoS/rLNBhrfnq+uGWQVVdExBvAA+ROIa9Mp6RJz6tSt2XAQXmrjUtty9Jy2/ZW60hqJDcIbW0H\n2zIzszL4/vLc6OsAlrzp+0R1RbG/U/434B+BnwCfTs+foshrypJGS9o3LQ8k91OqhcDd5E6Fk55/\nmpbvBi5II6onAocDj6dT3RslnZT+IPhIm3VatnU+cH86+r4XOFPS8DS6+8zUZmZmZZB/ynNIX19T\n7opiR19/FDg+InZfz5X0c+BJ4DNFrD8WmJmuKzcAsyLi55J+B8ySdDHwKvBBgIiYL2kW8BzQBFyW\nRl4DXArcAgwkN+r6ntR+E3CbpMXAOnKjt4mIdZK+CDyR+n0hIvb8iM7MzErqnfsO4oE33qQB2L9/\nv6zLqSnFhvKm9GjbtrGYlSPiD8Bx7bSvBaYUWOda4Np22ucCR7fTvg34QIFtzQBmFFOrmZn1zMNv\nvAnALmDnrqCfR3sVrdhbN34N+Imkr5AbvXwQudPYN5S3PDMzqzXNect9GxzIXdHVWzee1qbP6cA3\nS12UmZn1Dt0YD1zXfOtGMzOzMpL0CrlLvs1AU0RMKtS32GvKZmZmXeIju1ZOi4hO785RVCin3/1e\nCvwJMIq8U9oRcWp3KzQzs95H5K599vep6y4r9g+ZG8hNsfkwcALwY2A/4P4y1WVmZjWq5XfKWyOz\nSRqrTZC7sdK8NKVzQcWG8vuBsyPi6+TOh3+d3I0g2g78MjOzOratqSnrEiptlKS5eY/2QveUNM30\n2cBlkgqeYS72mvIg9tzUYaukQRGxUNJevz02M7P69cLmrVmXUGlrOhq4BRARy9LzKkl3AZPJnXne\nS7FHyguAP07Lc4FrJP0znkPazMzy/Gylb1WfT9JgSUNalslN9fxsof7FHilfzp7fg38C+DYwBPhY\n90s1M7Pe5qE3tmRdQrUZA9yVfq/dCPx3RPyqUOdi76f8RN7yIuCMdEMIX1M2M7Pd3rbPAP6Q7gw1\n2r+JIiJeAo4ptn9Pdlk/YHYP1jczs17m+a07di/3b/RUGF3V079j/CM0MzPb7clNewZ6rd1RdyOx\ne6ynoewfoZmZ2W6D8w7V/mjwwOwKqVEdnluQ1FFo9ylxLWZmVuM25x2qHTlkQHaF1KjOTvg3Ufho\nWB28Z2ZmdWhX3vL6+ptIpMc6C+WJFanCzMx6nWljR2RdQs3pMJQj4tVKFWJmZrWvL7AzLb991PAs\nS6lJ/hWZmZmVzM7Ou1gHHMpmZmZVwqFsZmZWJRzKZmZWEuH7J/dYwYFekh6hiJ88RUTB+0KamVn9\nWL/dV5R7qqPR1/9VsSrMzKzm/euLS7MuoeYVDOWImFnJQszMrLY9sbH2btuoPmMYMPwTPdjCL0pW\nCxR/P2UkjQEmA6PIuxFFRMwoaUVmZlaThvTtC9uaAfCs191TVChLOg/4PrAIOAqYDxwNPAo4lM3M\njK1NeybZ3K+vb4/QHcWOvv4ScFFEHAdsSc+XAPPKVpmZmdWUV/MGejV5IHa3FBvK4yPiR23aZgIf\nKXE9ZmZWo3bt2pPEBw/ql2EltavYUF6VrikDvCLpZOBQfPtGMzNL8od57dvoeOiOYkP5e8ApafkG\n4AHgaeDGchRlZma1rZ98/ro7ihroFRHX5S3fKulBYHBELChXYWZmVru+csT4rEuoSUX/JKqFpAZg\nactyROzqZBUzM6sz+/bvn3UJNamo09eSjpf0O0lbyN2ZayfQhO/SZWZmVjLFHinPBH4G/CXwZvnK\nMTMzq1/FhvLBwD+FbwFiZmbt+MXyVVmX0CsUO/r6LuDMchZiZma16wuLl2ddQq9Q7JHyAOAuSY8C\nK/LfiAhPIGJmVucGNfaB5uasy6h5xYbyc+lhZma2lwXbHcgdkdQHmAssi4j3FupX7O+UP1+qwszM\nrHcblHUB1elyYAEwtKNOBUNZ0qkR8XBaPr1Qv4i4v7sVmplZ7/PX40ZmXUJVkTQOeA9wLdDhzZs7\nOlK+kdztGQFuKtAngEO6WqCZmfVeHz94/6xLqDZfAz4DDOmsY8FQjoij85YnlqYuMzPr7Yb265t1\nCZU0StLcvNfTI2J6ywtJ7wVWRcQ8Se/qbGNdnmbTzMws3/MbNmVdQpbWRMSkDt5/B/A+SeeQ+yXT\nUEnfj4gPtde5o2vKr5E7Pd2hiPCs42ZmdeycJ1/MuoSqFRFXAVcBpCPlTxUKZOj4SDl/pT8GpgHf\nAF4lN8PX3wK39rBeMzOrcVs672JF6uia8kMty5K+BUyNiGV5bfcAvwK+WtYKzcysZvjnUIVFxIPA\ngx31KXaazQOAzW3aNgMHdrkqMzPrtf5p4pisS6hpxYby3cDdkt4t6a2SziQ3H/bd5SvNzMyq3arN\nrY/Xpo0bnVElvUOxofw3wO+A7wBPAt8G5qR2MzOrUyc/sbjV68ZG/6inJzoN5TRf5/8FromIQyNi\nYHq+MiK2FvMhkg6S9ICk5yTNl3R5ah8habakRel5eN46V0laLOl5SVPz2k+Q9Ex67xuSlNr7S7oj\ntc+RNCFvnWnpMxZJmlbszjEzs455kFdpdRrKEdEMXB8R23rwOU3AJyPiSOAk4DJJRwJXAvdFxOHA\nfek16b0LgKOAs4Ab0x8HkDtK/xhweHqcldovBtZHxGHADcB1aVsjgKuBE4HJwNX54W9mZqVR7KlX\nK6zYffgzSX/a3Q+JiNcj4sm0vIncpNwHAucCM1O3mcB5aflc4IcRsT0iXgYWA5MljQWGRsRjERHk\nfpKVv07Ltu4EpqSj6KnA7IhYFxHrgdnsCXIzMyuRqw4akXUJNa8r91O+U9LvgFaTinT1fsrptPJx\n5K5Jj4mI19NbK4CWYXsHAo/lrbY0te1My23bW9Z5LdXUJGkDMDK/vZ118uu6BLgEYPx4z4diZtZV\nf3eY/9/ZU8WG8rPp0SOS9gF+DFwRERvT5WAAIiIkdTqDWLmkuUqnA0yaNCmzOszMasW/v/Bq1iX0\nOhW7n7KkvuQC+faI+ElqXilpbES8nk5Nr0rty4CD8lYfl9qWpeW27fnrLJXUCAwD1qb2d7VZ58Ge\nfh8zs3p347L1WZfQ6xQz+rpR0kcl3S7p3vR8UQrZoqRruzcBCyLi+ry37iY3fSfp+ad57RekEdUT\nyQ3oejyd6t4o6aS0zY+0WadlW+cD96frzvcCZ0oangZ4nZnazMysB4r6+Y11SYdHypKGkRsYNQH4\nJbnfKI8FvgJcKumMiNhQxOe8A/gw8Iykp1LbZ9N2Zkm6mNyc2h8EiIj5kmYBz5EbuX1ZGgUOcClw\nCzAQuCc9IBf6t0laDKwjN3qbiFgn6YvAE6nfFyJiXRE1m5lZkQZkXUA3Dd20hNMfvCzrMnbr7PT1\nvwKrgdMiYvfP0dK14TvS+5d29iER8SigAm9PKbDOtcC17bTPBY5up30b8IEC25oBzOisTjMz6557\nj5mYdQm9Qmenr88DPp4fyAARsRm4DPizchVmZma144gRw7IuoVfoLJSHsWcgVVtLgaGlLcfMzGrB\nfy56rfNO1mWdhfKLwOkF3psCvFTacszMrBZcu3Rt1iX0Sp2F8vXArZL+XFIDgKQGSeeTG2x1fUcr\nm5mZWfE6HOgVEbdIGkkugH8gaQ0wCthObhTzzeUv0czMqtmFIwdmXUKv0enkIRHxVUnTgbeTC+Q1\nwO8iYmO5izMzs+pzyANPtXp9w9uOyKiS3qfYGb024Qk3zMwMeDPrAnox32nLzMy6zffBLS2HspmZ\nFe2SJ59v9XrBacdmVEnv5FA2M7Oi3b3BM16Xk0PZzMysSjiUzcysW84e0i/rEnodh7KZmRXl3xe2\nnsTx5klHZlRJ7+VQNjOzonz1dU9PUW4OZTMzsyrhUDYzs05t27691evvHbZfRpX0bg5lMzPr1ITf\nLmj1+k8POiCjSmqLpAGSHpf0tKT5kj7fUf+iptk0MzOzbtkOnB4RmyX1BR6VdE9EPNZeZ4eymZl1\n6IvPvNDq9V8MHZBRJbUnIgLYnF72TY8o1N+nr83MrEPfWtP6FhRfP+EtGVVSlUZJmpv3uKRtB0l9\nJD0FrAJmR8ScQhvzkbKZmRX0+PLlWZdQ7dZExKSOOkREM3CspH2BuyQdHRHPttfXR8pmZlbQ+55f\n1er1Ct+Aotsi4g3gAeCsQn0cymZmZmUiaXQ6QkbSQODdwMJC/X362szM2rX/A0+1eu2j5G4ZC8yU\n1IfcgfCsiPh5oc4OZTMzszKJiD8AxxXb36evzcxsL5vfbD3i+oy+yqiS+uJQNjOzvRw2p/Vvk79/\nyjEZVVJfHMpmZmZVwqFsZmatHNtmgNfMQ0ZnVEn9cSibmVkrK9q8nnrwgZnUUY88+trMzHbLTdW8\nR5+M6qiUF8fCB6/qQRR+tGSlAD5SNjOzPGMffLrV62X+bXJFOZTNzAyAx557LusS6p5D2czMADhv\n5Y5Wr392yIiMKqlfDmUzM9trSk2APz54fAaV1DeHsplZnTvv4b0D2fNcZ8OhbGZWx9atW8djza3b\nFk06NJtizKFsZlbPjnx6yV5tQ4YMyaASA4eymVnd+sDvnt6rzaets+VQNjOrU49saz1RiAM5ew5l\nM7M69M52Rltb9hzKZmZ1aFGb1z5Krg4OZTOzOjPZR8lVy6FsZlZn2o639lFy9XAom5nVkfZm7rLq\n4VA2M6sTNz73/F5tPkquLg5lM7M68YWVW1u9vmhwRoVYQQ5lM7M60N5p63+d7KPkauNQNjPr5d7a\nTiD7tHV1ciibmfViv160iPVZF2FFq0goS5ohaZWkZ/PaRkiaLWlReh6e995VkhZLel7S1Lz2EyQ9\nk977hiSl9v6S7kjtcyRNyFtnWvqMRZKmVeL7mplVi48s3QR4Os1aUakj5VuAs9q0XQncFxGHA/el\n10g6EriEM4TFAAAPaklEQVQAOCqtc6OkPmmdbwMfAw5Pj5ZtXgysj4jDgBuA69K2RgBXAycCk4Gr\n88PfzKw32//+xyAEiJZgdiBXt4qEckQ8DKxr03wuMDMtzwTOy2v/YURsj4iXgcXAZEljgaER8VhE\nBHBrm3VatnUnMCUdRU8FZkfEuohYD8xm7z8OzMx6nfvuPxTYkXsRAYQDOQOSDpL0gKTnJM2XdHlH\n/bO8pjwmIl5PyyuAMWn5QOC1vH5LU9uBablte6t1IqIJ2ACM7GBbZma91sOPXAbAbVxEA5uAbaw4\n7fhsi6pfTcAnI+JI4CTgsnRGuF1VMdArHflGpx3LSNIlkuZKmrt69eosSzEz65GdO38F5P4HfxsX\n8fwJBTPAyiwiXo+IJ9PyJmABHRwcZhnKK9MpadLzqtS+DDgor9+41LYsLbdtb7WOpEZgGLC2g23t\nJSKmR8SkiJg0evToHnwtM7Ps5E5btzZs2LAMKqkbo1oO6NLjkkId0yDk44A5hfpkGcp3Ay2joacB\nP81rvyCNqJ5IbkDX4+lU90ZJJ6XrxR9ps07Lts4H7k9H3/cCZ0oangZ4nZnazMx6nfYCecrpL2ZQ\nSV1Z03JAlx7T2+skaR/gx8AVEbGx0MYay1Vlm2J+ALyL3F8US8mNiP4KMEvSxcCrwAcBImK+pFnA\nc+TOxV8WEc1pU5eSG8k9ELgnPQBuAm6TtJjcgLIL0rbWSfoi8ETq94WIaDvgzMys5rUXyKe847kM\nKrG2JPUlF8i3R8RPOupbkVCOiAsLvDWlQP9rgWvbaZ8LHN1O+zbgAwW2NQOYUXSxZmY1pr1Aftsf\nPUb//v0zqMbypTO7NwELIuL6zvpXxUAvMzPrujVrVrcbyGP2+zweG1M13gF8GDhd0lPpcU6hzhU5\nUjYzs9JqL4wBhuzzzxx99IcqXI0VEhGPkpu9pSgOZTOzGlMokI8+6hHGjDmgwtVYKTmUzcxqRKEw\nBo+y7i0cymZmVW7JkhdYtPjsgu87kLvvqO07mPvykm6vX/R56SI5lM3MqlhHR8fjDvwFRxzxlgpW\nY+XmUDYzqzJNTU089PARHfbx0XHv5FA2M6sSCxfOZdnyv+iwz9tPfpaBAwdWqCKrNIeymVkV6Og0\nNUCDzuG00/6zQtVYVhzKZmYZeuihT9LU/D8d9JjMlNN/ULF6LFsOZTOzjHR0dLzP4E9z4ol/U8Fq\nrBo4lM3MKqyjMB6+75c4/vhCtwuw3s6hbGZWAcuXL2PBwlM77OMR1eZQNjMrk8WLX2Dxi2fTt2/H\n/Q4/7AHGjx9fmaKsqjmUzcxK7JprrgHglHfeRmMjRIDanfpJTDl9cSVLsyrnUDYzK4GWIC7GwAGX\n8va3f7J8xVjNciibmXVDMSG8fj0MH77nSNnXjK0zDmUzsyLdcsstvPLKK0X3n//sh7t0BG3mUDYz\n68CiRYu4/fbbu7TOpz/9aQYPHlymiqw3cyibmbXxpS99iaampqL79+vXj89+9rNlrMjqhUPZzOre\nkiVLmDFjRpfWOeWUUzjjjDPKVJHVK4eymdWd7l7nbWho4HOf+1xpizHL41A2s17t17/+Nb/97W97\ntA0P1rJKcSibWa+y9JVX4DuvsYtdzOj/QK6x3Yk7Crvqqqvo379/yWsz64xD2cxq1q5du1j+2f9t\n970lrC46jC+//HKGDx9ewsrMusehbGY1YemSJXDjq0X3H89oiLyGFNBTp07l5JNPLm1xZiXiUDaz\nqrN0zrNw1/oebaOBBv5q+xT463GMmzixRJWZlZdD2cwys/TfH4G1pd3m8H853hN3WM1yKJtZ2Sz9\n8iOwsXzbH3n1JAYOHFi+DzCrMIeymfXI7x+6j/tvvGGv9vPHf4qGhgYA1P59C7uk8YrD2X///Xu8\nHbNKkjQDeC+wKiKO7qy/Q9nMOvSNyy9j54riB1i16HYgHw3jPvTOLn+eWZW6BfgmcGsxnR3KZnVs\n7q/u4aGbv1WWbW/Yuo5hA0cQEXsH88Ew7uMOXuv9IuJhSROK7e9QNuulfnzz/bzyq+sz+/yDvnga\nw4YNy+zzzWqRQ9mshjQ1NfHdv324qL7b1pc3kDV6DJ/45k1l/QyzGjBK0ty819MjYnp3N+ZQNsvY\nbdffz8YXsq5ibxd96xZGjBqVdRlm1W5NREwq1cYcymZlsGXLFm654nboc1iGVQwGtrT7zsTTzuD9\nf3NFZcsxs045lM2KsHPnTha/7RiI6LwzcP/JX4V+h+b6l+DnQF3x9o8O57iTjgNOr+jnmtneJP0A\neBe509xLgasjouB1H4ey1aWIYOE7ToF168rzAX375X8aXb5NURsXXX8SgwYN6tE2zGxvz8QhTNj2\ntR5s4b0dvhsRF3Zlaw5l6zWampqYdPskmmnmyv9o4pid0CejWg76wwxeO+bi9Kr9QL7sOz6SNbPW\nHMpWtbZs2cL1v7qeWZtndW3FXbs4bmdusefHqF0wehSjf/QjRu2/P2+t1GeaWa/iULaKO2PmGaxk\nZdm2X7IQvvzveevHP16qrZmZdcqhbKWzahXceDg7gBMOOgAaGnKPCouGBlawizHkjpTzT2H3/+53\nOeRPTq14TWZmxXAoW+f+7XR4c17R3U8Yf2DFwvjUsafyrTPbmSZyWkU+3syspBzKZfDVn83jP/93\nRdZldMFOXmicRp/G7AZG5fvUWz/FtMlOVTOrPw7lMqitQIYX+02jQUBAqOfXZOctWZY7Wgbok4v5\nfxj2D/zleX/Zwy2bmfVuDmXrYQgfANcsaNXSD3imR9s0M6tPDuUyGABsy7qILjhzx2f5db8vA+SO\nmFv81TwYl+U0kWZm9cWhXAYLv/KerEvoovcA/5h1EWZmda/yv1cxMzOzdjmUzczMqoRD2czMrErU\nTShLOkvS85IWS7oy63rMzMzaqotQltQH+BZwNnAkcKGkI8vxWb97fjkTrvwFE678BdMfWND5CmZm\nZkldhDIwGVgcES9FxA7gh8C55figC2/+/e7lL9/7Ujk+wszMeql6CeUDgdfyXi9NbbtJukTSXElz\nV69eXdHizMzMoH5CuVMRMT0iJkXEpNGjR3d7Ow/+w8m7l2d86JhSlGZmZnWiXiYPWQYclPd6XGor\nuQljRvBKzU0eYmZm1aBejpSfAA6XNFFSP+AC4O6MazIzM2ulLo6UI6JJ0t8C95K7O+GMiJifcVlm\nZmat1EUoA0TEL4FfZl2HmZlZIfVy+trMzKzqOZTNzMyqhEPZzMysSjiUzczMqoRD2czMrEo4lM3M\nzMqoK3cpdCibmZmVSVfvUuhQNjMzK58u3aXQoWxmZlY+nd6lMF/dzOjVFfPmzVsj6dUebmYUsKYU\n9fRC3jeFed8U5n3Tsd6+fw4ux0Z3rFh876vXvXdUDzYxQNLcvNfTI2J6dzfmUG5HRHT/3o2JpLkR\nMakU9fQ23jeFed8U5n3TMe+f7omIs8r8EV26S6FPX5uZmZVPl+5S6CNlMzOzMunqXQodyuXT7WsK\ndcD7pjDvm8K8bzrm/VOlunKXQkVEmcsxMzOzYviaspmZWZVwKJdYV6ZT600kvSLpGUlPtfw8QNII\nSbMlLUrPw/P6X5X20fOSpua1n5C2s1jSNyQpi+/TE5JmSFol6dm8tpLtC0n9Jd2R2udImlDJ79dT\nBfbPNZKWpX8/T0k6J++9utg/kg6S9ICk5yTNl3R5ave/nXoSEX6U6EHuIv6LwCFAP+Bp4Mis66rQ\nd38FGNWm7d+AK9PylcB1afnItG/6AxPTPuuT3nscOAkQcA9wdtbfrRv74lTgeODZcuwL4FLgO2n5\nAuCOrL9zCfbPNcCn2ulbN/sHGAscn5aHAC+k7+9/O3X08JFyaXVpOrU6cC4wMy3PBM7La/9hRGyP\niJeBxcBkSWOBoRHxWOT+r3Fr3jo1IyIeBta1aS7lvsjf1p3AlFo6o1Bg/xRSN/snIl6PiCfT8iZg\nAbmZn/xvp444lEurS9Op9TIB/EbSPEmXpLYxEfF6Wl4BjEnLhfbTgWm5bXtvUMp9sXudiGgCNgAj\ny1N2Rf2dpD+k09stp2jrcv+k08rHAXPwv5264lC2UjklIo4ldyeUyySdmv9m+ovdQ/3xvijg2+Qu\n+xwLvA58NdtysiNpH+DHwBURsTH/Pf/b6f0cyqXVpenUepOIWJaeVwF3kTuVvzKdSiM9r0rdC+2n\nZWm5bXtvUMp9sXsdSY3AMGBt2SqvgIhYGRHNEbEL+B65fz9QZ/tHUl9ygXx7RPwkNfvfTh1xKJdW\nl6ZT6y0kDZY0pGUZOBN4ltx3n5a6TQN+mpbvBi5II0EnAocDj6dTdBslnZSuc30kb51aV8p9kb+t\n84H70xFUzWoJneTPyP37gTraP+l73AQsiIjr897yv516kvVIs972AM4hN2ryReCfsq6nQt/5EHKj\nQJ8G5rd8b3LXqu4DFgG/AUbkrfNPaR89T94Ia2ASuf8hvwh8kzTBTS09gB+QOwW7k9z1vItLuS+A\nAcCPyA3seRw4JOvvXIL9cxvwDPAHcsExtt72D3AKuVPTfwCeSo9z/G+nvh6e0cvMzKxK+PS1mZlZ\nlXAom5mZVQmHspmZWZVwKJuZmVUJh7KZmVmVcCibVbl0x6B3dWO9WyR9qQwlmVmZNGZdgJl1LCKO\nyroGM6sMHymbmZlVCYeyWZWT9IqkMyRdI2mWpFslbUqntSfl9TtO0pPpvTvIzd6Uv533SnpK0huS\nfivpban9UEnrJB2fXh8gaXV3TpmbWc84lM1qy/vI3ad7X3LTUX4TIM21/j/kpqscQW4qxT9vWUnS\nccAM4K/JTdv4XeBuSf0j4kXgH4HvSxoE3AzMjIgHK/SdzCxxKJvVlkcj4pcR0UwugI9J7ScBfYGv\nRcTOiLiT3A1SWlwCfDci5kTubkwzge1pPSLie+TmQ54DjCU3p7KZVZhD2ay2rMhbfhMYkG7BdwCw\nLFpPZv9q3vLBwCfTqes3JL1B7hZ+B+T1+R5wNPCfEbG9POWbWUccyma9w+vAgelWfS3G5y2/Blwb\nEfvmPQZFxA8AJO0DfI3crQOvkTSiYpWb2W4OZbPe4XdAE/D3kvpKej8wOe/97wF/I+lE5QyW9J6W\n+2ADXwfmRsRfAb8AvlPR6s0McCib9QoRsQN4P/BRYB3wF8BP8t6fC3yM3MCw9eSuH38UQNK5wFnA\nx1P3TwDHS/p/lanezFr4fspmZmZVwkfKZmZmVcKhbGZmViUcymZmZlXCoWxmZlYlHMpmZmZVwqFs\nZmZWJRzKZmZmVcKhbGZmViUcymZmZlXi/wMssGgyAM21/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa8ee9190f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(train_df.shape[0]), label[sorted_idx],s=3,\n",
    "            c=np.sort(label_ord[sorted_idx]), cmap = plt.get_cmap('tab10'))\n",
    "plt.colorbar()\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('Ordinal Label', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fa8e828bdd8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAHmCAYAAABanLmxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa950d48470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''ulimit = np.percentile(train_df.label.values, 98)\n",
    "llimit = np.percentile(train_df.label.values, 2)\n",
    "train_df['label'].ix[train_df['label']>ulimit] = ulimit\n",
    "train_df['label'].ix[train_df['label']<llimit] = llimit'''\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.distplot(train_df.label.values, bins=50, kde=False)\n",
    "plt.xlabel('label', fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_coeff(n, metric, lmbda = 1):\n",
    "    if metric is 'ccr':\n",
    "        return [1]\n",
    "    elif metric is 'ccr1':\n",
    "        return [1, 1, 1]\n",
    "    elif metric is 'mae':\n",
    "        coeff = np.arange(1,n)/(n-1)\n",
    "    elif metric is 'mse':\n",
    "        coeff = np.zeros(n-1)\n",
    "        coeff[0] = 2*n-3\n",
    "        for k in range(1, n-1):\n",
    "            coeff[k] = coeff[k-1] + 2*n - (2*(k+1)+1)\n",
    "        coeff = coeff /((n-1)**2)\n",
    "    else:\n",
    "        print('Undefined Metric: ' + metric)\n",
    "    coeff = np.concatenate((coeff, coeff[::-1][1:]), axis=0)\n",
    "    coeff = coeff * lmbda\n",
    "    coeff[n-2] = 1\n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_type = 'house_16H'\n",
    "num_samples = 10000\n",
    "num_classes = 9\n",
    "nclasses = num_classes\n",
    "dim = 2\n",
    "\n",
    "sigma_noise = 0.01\n",
    "optimizer='sgd' #Optimizer function\n",
    "iter_loc=10 #Number of the first column in the excel file for writing the results.\n",
    "lr=.5 #Initial learning rate\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "batch_size = 256\n",
    "lr_scheduler=ft.exp_lr_scheduler #Learning rate scheduler\n",
    "lr_decay_epoch=10 #Number of epoch for learning rate decay\n",
    "hidden_sizes = [50, 50]\n",
    "dropouts = [0, 0]\n",
    "rand_label = False\n",
    "\n",
    "metric = 'ccr'\n",
    "coeff_lmbda =  1\n",
    "multi_coeff = make_coeff(nclasses, metric, coeff_lmbda)\n",
    "KL = False #KL divergence for porbability measure\n",
    "\n",
    "\n",
    "'''Multipliers for loss functions'''\n",
    "single_loss=1.\n",
    "multi_loss=0.\n",
    "\n",
    "comment=' ' #Additional comments if any\n",
    "\n",
    "algo = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.99786778 -0.1046049  -0.87203641 -0.64262862 -0.52512336  0.68509746\n",
      " -0.95656432  0.25911488 -0.94257411 -0.90573663 -0.7733197  -0.88512628\n",
      "  0.93387451 -0.3900144  -0.78524665 -0.01674871]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "(22784, 16)\n",
      "[9 4 0 ..., 3 8 0]\n",
      "{'train': 18227, 'val': 4556}\n",
      "OR\n",
      "Number of training images 5\n",
      "Number of validation images 5\n",
      "{'train': 18227, 'val': 4556}\n",
      "GPU is available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"inputs, classes = next(iter(dset_loaders['train']))\\nprint(inputs.shape)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV = 5\n",
    "random_seed = 1\n",
    "\n",
    "if data_type == 'circular':\n",
    "    fvec, label = generateCircularData(num_samples = num_samples, \n",
    "                                       num_classes = num_classes, dim = dim, bound = 5,\n",
    "                                       sigma_noise = sigma_noise, rand_label = rand_label)\n",
    "elif data_type == 'linear':\n",
    "    fvec, label = generateLinearData(num_samples = num_samples, \n",
    "                                     num_classes = num_classes, dim = dim, bound = 5,\n",
    "                                       sigma_noise = sigma_noise, rand_label = rand_label)\n",
    "elif data_type == 'spiral':\n",
    "    fvec, label = generateSpiralData(num_samples = num_samples, \n",
    "                                     num_classes = num_classes, dim = dim, bound = 5,\n",
    "                                       sigma_noise = sigma_noise, rand_label = rand_label)\n",
    "else:\n",
    "    num_classes = num_bins\n",
    "    nclasses = num_classes\n",
    "    \n",
    "    feat=train_df.values[:,:-2]\n",
    "    #Normalize the features\n",
    "\n",
    "    feat_max = np.amax(feat,axis=0)\n",
    "    feat_min = np.amin(feat,axis=0)\n",
    "\n",
    "    feat=(feat-feat_min)/(feat_max-feat_min)\n",
    "    feat=feat*2-1\n",
    "\n",
    "    '''feat_mean = np.mean(feat,axis=0)\n",
    "    feat_std = np.std(feat,axis=0)\n",
    "\n",
    "    feat=(feat-feat_mean)/feat_std\n",
    "    '''\n",
    "    label_ord=train_df.values[:,-1].astype(np.int)\n",
    "\n",
    "    rand_idx = np.random.permutation(len(label_ord))\n",
    "    feat = feat[rand_idx, :]\n",
    "    label = label_ord[rand_idx]\n",
    "\n",
    "\n",
    "    print(np.mean(feat,axis=0))\n",
    "    print(np.min(feat,axis=0))\n",
    "    print(feat.shape)\n",
    "    print(label)\n",
    "\n",
    "    fvec=feat.copy()\n",
    "    dim = feat.shape[1]\n",
    "    \n",
    "    if not CV == 0: \n",
    "        dset_train= torch.utils.data.TensorDataset(torch.from_numpy(fvec).type(torch.FloatTensor),\n",
    "                                                       torch.from_numpy(label).type(torch.LongTensor))\n",
    "        dset_val= torch.utils.data.TensorDataset(torch.from_numpy(fvec).type(torch.FloatTensor),\n",
    "                                                       torch.from_numpy(label).type(torch.LongTensor))\n",
    "\n",
    "        '''Define dataset loaders''''''\n",
    "        dset_loaders = {'train':torch.utils.data.DataLoader(dsets['train'], batch_size=batch_size,shuffle=True,\n",
    "                                                            num_workers=12),\n",
    "                        'val':torch.utils.data.DataLoader(dsets['val'], batch_size=batch_size,shuffle=False,\n",
    "                                                            num_workers=12)}\n",
    "\n",
    "\n",
    "        dset_sizes={'train':len(dsets['train']),'val':len(dsets['val'])}\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "\n",
    "        print(dset_sizes)\n",
    "\n",
    "        if use_gpu:\n",
    "            print('GPU is available')\n",
    "        else:\n",
    "            print('!!!!! NO CUDA GPUS DETECTED')\n",
    "\n",
    "        inputs, classes = next(iter(dset_loaders['train']))\n",
    "        print(inputs.shape)'''\n",
    "        '''dset_train = datasets.ImageFolder(data_dir+'/train_val', data_transforms['train'])\n",
    "        dset_val = datasets.ImageFolder(data_dir+'/train_val', data_transforms['val'])'''\n",
    "\n",
    "        num_train = len(dset_train)\n",
    "        indices = list(range(num_train))\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        splits = (num_train*np.linspace(0,1,CV+1)).astype(int)\n",
    "\n",
    "        val_idx = [indices[splits[k]:splits[k+1]] for k in range(CV)]\n",
    "        train_idx=[np.setdiff1d(indices,val_idx[k]) for k in range(CV)]\n",
    "        '''Sampler functions for validation and training'''\n",
    "        sampler_train = [torch.utils.data.sampler.SubsetRandomSampler(train_idx[k]) for k in range(CV)]\n",
    "        sampler_val = [torch.utils.data.sampler.SubsetRandomSampler(val_idx[k]) for k in range(CV)]\n",
    "\n",
    "        '''Define dataset loaders'''\n",
    "        dset_loaders_arr = [{'train':torch.utils.data.DataLoader(dset_train, batch_size=batch_size,sampler=sampler_train[k],\n",
    "                                                            num_workers=12),\n",
    "                        'val':torch.utils.data.DataLoader(dset_val, batch_size=batch_size,sampler=sampler_val[k],\n",
    "                                                            num_workers=12)} for k in range(CV)]\n",
    "        dset_sizes={'train':int(len(dset_train)*(1-1/CV)),'val':int(len(dset_train)*(1/CV))}\n",
    "\n",
    "        print(dset_sizes)\n",
    "        print('OR')\n",
    "        print('Number of training images '+str(len(val_idx)))\n",
    "        print('Number of validation images '+str(len(train_idx)))\n",
    "    \n",
    "\n",
    "\n",
    "'''rand_idx = np.random.permutation(len(label))\n",
    "fvec_norm = (fvec)/5\n",
    "mid_point = int(len(label)/2)#100*num_classes\n",
    "fvec_test = fvec_norm[rand_idx[:mid_point],:]\n",
    "fvec_train = fvec_norm[rand_idx[mid_point:],:]\n",
    "\n",
    "label_test = label[rand_idx[:mid_point]]\n",
    "label_train = label[rand_idx[mid_point:]]\n",
    "print(np.max(fvec_train))\n",
    "print(np.min(fvec_train))\n",
    "\n",
    "torch.from_numpy(label_train).type(torch.LongTensor)\n",
    "dsets={'train': torch.utils.data.TensorDataset(torch.from_numpy(fvec_train).type(torch.FloatTensor),\n",
    "                                               torch.from_numpy(label_train).type(torch.LongTensor)),\n",
    "       'val': torch.utils.data.TensorDataset(torch.from_numpy(fvec_test).type(torch.FloatTensor),\n",
    "                                             torch.from_numpy(label_test).type(torch.LongTensor))}\n",
    "\n",
    "''''''\n",
    "dset_loaders = {'train':torch.utils.data.DataLoader(dsets['train'], batch_size=batch_size,shuffle=True,\n",
    "                                                    num_workers=12),\n",
    "                'val':torch.utils.data.DataLoader(dsets['val'], batch_size=batch_size,shuffle=False,\n",
    "                                                    num_workers=12)}\n",
    "\n",
    "\n",
    "dset_sizes={'train':len(dsets['train']),'val':len(dsets['val'])}\n",
    "'''\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "print(dset_sizes)\n",
    "\n",
    "if use_gpu:\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    print('!!!!! NO CUDA GPUS DETECTED')\n",
    "\n",
    "'''inputs, classes = next(iter(dset_loaders['train']))\n",
    "print(inputs.shape)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(feat.astype(np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeLog(logname):\n",
    "    '''\n",
    "    Creates a text file named Network_properties.txt inside runs/'logname'\n",
    "    '''\n",
    "    f=open('runs_regression/'+logname+'/Network_properties.txt','w')\n",
    "    f.write('Feature Length: '+str(dim)+'\\n')\n",
    "    f.write('Number of classes: '+str(num_classes)+'\\n')\n",
    "    f.write('Data type: '+data_type+'\\n')\n",
    "    f.write('Random Noise: '+str(sigma_noise)+'\\n')\n",
    "    \n",
    "    f.write('Hidden sizes: '+ str(hidden_sizes)+'\\n')\n",
    "    f.write('Dropouts: '+str(dropouts)+'\\n')\n",
    "    f.write('Batch size: '+str(batch_size)+'\\n')\n",
    "    f.write('Number of samples: '+str(num_samples)+'\\n')\n",
    "    \n",
    "    f.write('Optimizer: ' + optimizer + '\\n')\n",
    "    crt=str(single_loss)+'xsingle + '+str(multi_loss)+'Xmulti'\n",
    "    f.write('Criterion: '+crt+'\\n')\n",
    "    f.write('Learning rate: '+str(lr)+'\\n')\n",
    "    f.write('Momentum: '+str(momentum)+'\\n')\n",
    "    f.write('Leraning Rate Scheduler: '+str(lr_scheduler)+'\\n')\n",
    "    f.write('Leraning Rate Decay Period: '+str(lr_decay_epoch)+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs_regression.xlsx\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "import time\n",
    "\n",
    "def writeLog_xlsx(logname='logs_regression.xlsx',iter_loc=10):\n",
    "    '''\n",
    "    Adds a line to logs.xlsx with the network properties and outcomes.\n",
    "    :param iter_loc: First column to record the outcomes.\n",
    "    '''\n",
    "    \n",
    "    print(logname)\n",
    "    book = openpyxl.load_workbook(logname)\n",
    "    sheet = book.active\n",
    "    crt=str(single_loss)+'xsingle + '+str(multi_loss)+'Xmulti'\n",
    "    if metric:\n",
    "        m_coeff = make_coeff(nclasses, metric, coeff_lmbda)\n",
    "    else:\n",
    "        m_coeff = multi_coeff\n",
    "    specs=(datetime.now().strftime('%B%d  %H:%M:%S'),data_type,str(hidden_sizes),str(dim),str(num_classes),\n",
    "           crt, str(lr), str(m_coeff), str(KL))\n",
    "    sheet.append(specs)\n",
    "    current_row = sheet.max_row\n",
    "    sheet.cell(row=current_row, column=iter_loc+5).value = comment\n",
    "    book.save(logname)\n",
    "writeLog_xlsx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc0): Linear(in_features=2, out_features=50, bias=True)\n",
      "  (relu0): ReLU()\n",
      "  (drop0): Dropout(p=0)\n",
      "  (fc1): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (drop1): Dropout(p=0)\n",
      "  (fc2): Linear(in_features=50, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, dropouts, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.numHidden=len(hidden_sizes)\n",
    "        setattr(self, 'fc0', nn.Linear(input_size, hidden_sizes[0]))\n",
    "        setattr(self, 'relu0', nn.ReLU())\n",
    "        setattr(self, 'drop0', nn.Dropout(p=dropouts[0]))\n",
    "        for k in range(len(hidden_sizes)-1):\n",
    "            setattr(self, 'fc'+str(k+1), nn.Linear(hidden_sizes[k], hidden_sizes[k+1]))\n",
    "            setattr(self, 'relu'+str(k+1), nn.ReLU())\n",
    "            setattr(self, 'drop'+str(k+1), nn.Dropout(p=dropouts[k+1]))\n",
    "        setattr(self, 'fc'+str(len(hidden_sizes)), nn.Linear(hidden_sizes[-1], num_classes))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out=self.fc0(x)\n",
    "        out = self.relu0(out)\n",
    "        out = self.drop0(out)\n",
    "        for k in range(self.numHidden-1):\n",
    "            fc = getattr(self,'fc'+str(k+1))\n",
    "            relu = getattr(self,'relu'+str(k+1))\n",
    "            drop = getattr(self,'drop'+str(k+1))\n",
    "            out = fc(out)\n",
    "            out = relu(out)\n",
    "            out = drop(out)\n",
    "        fc = getattr(self,'fc'+str(self.numHidden))\n",
    "        out = fc(out)\n",
    "        return out\n",
    "    \n",
    "model=Net(2, [50, 50], [0, 0], 2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network_loader(comment=comment,\n",
    "                    optimizer=optimizer,\n",
    "                    iter_loc=iter_loc,\n",
    "                    lr=lr,\n",
    "                    momentum=momentum,\n",
    "                    weight_decay=weight_decay,\n",
    "                    lr_scheduler=lr_scheduler,\n",
    "                    lr_decay_epoch=lr_decay_epoch,\n",
    "                    nclasses=num_classes,\n",
    "                    hidden_sizes = hidden_sizes,\n",
    "                    dropouts = dropouts):\n",
    "    \n",
    "    '''Load the network from pytorch'''\n",
    "    model_ft = Net(dim, hidden_sizes , dropouts, num_classes)\n",
    "\n",
    "    if use_gpu:\n",
    "        model_ft = model_ft.cuda()\n",
    "\n",
    "    '''Define the optimizer function'''\n",
    "    if(optimizer=='adam'):\n",
    "        optimizer_ft = optim.Adam(model_ft.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "    elif(optimizer=='sgd'):\n",
    "        if(end_to_end):\n",
    "            optimizer_ft = optim.SGD(model_ft.parameters(), lr=lr, momentum=momentum)\n",
    "        else:\n",
    "            optimizer_ft = optim.SGD(model_ft.fc.parameters(), lr=lr, momentum=momentum,weight_decay=weight_decay)\n",
    "    return model_ft, optimizer_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'params': [Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.0989  0.0276 -0.2002  0.2364  0.2187  0.2119 -0.1518  0.2126  0.1886  0.2376\n",
      "-0.1236 -0.0579 -0.1949 -0.0178  0.1627  0.1492 -0.0042 -0.1609  0.1058 -0.0118\n",
      " 0.1857  0.0867  0.0413 -0.1223 -0.0814 -0.0646 -0.0002  0.1864 -0.2206 -0.0385\n",
      "-0.0685 -0.1728 -0.0526 -0.0080 -0.1281  0.1239 -0.0421 -0.1162  0.0698  0.1712\n",
      "-0.0983 -0.2350  0.0183  0.2309  0.0525 -0.2041 -0.1387  0.0061 -0.1531  0.0568\n",
      " 0.0293  0.2072 -0.2436  0.1574 -0.1800 -0.0822 -0.0655  0.0933  0.1481  0.2364\n",
      " 0.1060 -0.1060  0.1533  0.1535  0.2004 -0.1161  0.2167  0.0250  0.2023  0.2357\n",
      " 0.2167 -0.1310 -0.0959  0.2040  0.0319 -0.1085 -0.0613 -0.0487 -0.1800 -0.1431\n",
      " 0.1159 -0.2102  0.2266  0.0710 -0.1200  0.0339  0.1792  0.2377 -0.1234 -0.1082\n",
      " 0.0269  0.1652 -0.0606 -0.1809  0.2315  0.2342 -0.0817  0.0290  0.0692  0.1348\n",
      " 0.1353 -0.0651 -0.1571 -0.0406 -0.0071  0.1899 -0.2087  0.1806 -0.2238 -0.1225\n",
      " 0.0041 -0.1201 -0.1526 -0.0879  0.2117  0.0301 -0.0848  0.1570  0.2225  0.1040\n",
      " 0.0044  0.0884  0.0006 -0.0837  0.2484 -0.0302  0.1120 -0.0892 -0.2069 -0.1711\n",
      "-0.0636 -0.1598 -0.0135  0.2430 -0.0612  0.2340 -0.1557 -0.1298  0.0057 -0.1470\n",
      " 0.0774  0.0692 -0.0242  0.0976 -0.1367 -0.0146  0.2042 -0.0059 -0.1841 -0.0959\n",
      "-0.1298  0.2260  0.1972  0.2155  0.1731 -0.2115 -0.0299 -0.0353 -0.1853 -0.2073\n",
      "-0.0652  0.1427 -0.0188  0.0413 -0.1361 -0.1657 -0.0024  0.1943 -0.2295  0.2378\n",
      "-0.1369 -0.2071  0.0168  0.0732 -0.2115 -0.1529  0.0452  0.0652 -0.0743 -0.0384\n",
      " 0.1028 -0.0009  0.2359  0.1414 -0.1017  0.1953 -0.0417  0.1544  0.1082  0.1863\n",
      " 0.0926 -0.1671  0.1624 -0.1546 -0.0672  0.0470  0.0118 -0.2347  0.2206  0.0155\n",
      " 0.1554  0.1408 -0.1131 -0.0592 -0.1664 -0.2056 -0.0665 -0.0382  0.1811 -0.0145\n",
      " 0.0498 -0.0383 -0.2477  0.0681 -0.1656  0.1675 -0.0394  0.0733 -0.0005  0.1387\n",
      " 0.0151  0.1172  0.0021  0.0922  0.0146 -0.1371  0.1280 -0.0325 -0.0354  0.0380\n",
      "-0.2092 -0.1545  0.1426  0.1091  0.0068  0.1498 -0.0564 -0.0414  0.1172  0.0790\n",
      " 0.1503 -0.1921  0.0962 -0.2005 -0.0115  0.0155 -0.2156 -0.1854  0.2457  0.2197\n",
      " 0.1098 -0.0019  0.0623  0.0798 -0.1071  0.1742 -0.1357  0.1624  0.0186 -0.1578\n",
      " 0.1053 -0.1892 -0.1622 -0.1695  0.2300 -0.1978  0.1877  0.1014  0.2192  0.0249\n",
      " 0.2480 -0.1156 -0.0972 -0.0010 -0.0176 -0.0939 -0.1494  0.1571 -0.0534  0.2195\n",
      " 0.1754  0.0876  0.2005 -0.0160 -0.0149 -0.1751  0.2163 -0.1065  0.1430  0.0003\n",
      " 0.0668 -0.0159  0.1540  0.0677  0.0311 -0.1255  0.1869 -0.0321 -0.0599  0.1473\n",
      " 0.1683 -0.0351  0.2150 -0.1712  0.2363 -0.1155  0.2142 -0.2007 -0.0479  0.1703\n",
      "-0.0699  0.0459 -0.0311  0.0507  0.1554  0.0264 -0.2226 -0.1146  0.2181  0.0635\n",
      " 0.2048 -0.1004  0.0066 -0.2418 -0.1587  0.2275 -0.0973  0.0902  0.0193  0.1579\n",
      " 0.1889 -0.2334 -0.0725 -0.2359 -0.0517  0.1283  0.0469 -0.2365  0.0494  0.1518\n",
      " 0.1833 -0.0060 -0.1548  0.0163 -0.1366 -0.1949 -0.0388  0.2319 -0.0506  0.0871\n",
      "-0.0421  0.0737  0.0395 -0.0236  0.0559 -0.1180  0.2287  0.0233 -0.2478 -0.2346\n",
      "-0.1182  0.1226  0.2062 -0.1295 -0.0816 -0.0213 -0.1873 -0.2272  0.0163  0.2242\n",
      " 0.2167 -0.1196 -0.1373  0.0051  0.1649  0.1820  0.0149  0.0522  0.1973 -0.0352\n",
      "-0.2026 -0.1902  0.2030 -0.0290 -0.0880  0.0201  0.2428  0.2104 -0.2392  0.2255\n",
      "-0.0836 -0.0252  0.1226 -0.2338  0.0760 -0.1333  0.2079 -0.0738 -0.1824  0.1276\n",
      " 0.1725 -0.2347  0.0541  0.1941 -0.0509  0.0512  0.2191  0.1534 -0.0165  0.1709\n",
      "-0.0822  0.0645 -0.0136  0.2034 -0.1123  0.0683 -0.0846 -0.2060 -0.1441 -0.0469\n",
      "-0.1413 -0.0763  0.0769 -0.0791  0.2144 -0.1624 -0.1504  0.0477 -0.0414 -0.2176\n",
      " 0.0370 -0.2181  0.1895  0.0073  0.2260 -0.1891 -0.0075  0.1460 -0.2483 -0.0684\n",
      "-0.0535 -0.2102 -0.1583 -0.0232 -0.0934 -0.0967 -0.0547 -0.2240 -0.0899  0.0199\n",
      " 0.0356 -0.0880 -0.1574 -0.2122 -0.0679  0.1584  0.0501 -0.2080 -0.0324  0.0372\n",
      " 0.1478  0.0137 -0.0117 -0.2069 -0.1578 -0.2110 -0.1547  0.1794 -0.0072 -0.1382\n",
      "-0.1353  0.0416 -0.1670 -0.0110 -0.0508  0.2151  0.1410 -0.0949 -0.1421 -0.1459\n",
      "-0.1170 -0.2012  0.0152 -0.0856  0.1820  0.0809  0.1680  0.1725  0.1622 -0.0532\n",
      " 0.0594  0.0355 -0.1572  0.1911  0.1561  0.1226  0.2198 -0.1320  0.1013 -0.2433\n",
      "\n",
      "Columns 10 to 15 \n",
      " 0.2120 -0.0118  0.2485 -0.2296  0.0838  0.0148\n",
      "-0.0254  0.2176  0.1639  0.0925  0.1090 -0.2227\n",
      "-0.2220 -0.0783  0.1464  0.1307 -0.2139 -0.0587\n",
      "-0.1827  0.0425 -0.1780  0.0759 -0.1802  0.2023\n",
      "-0.0499  0.0631 -0.0470  0.1671  0.0675  0.2407\n",
      "-0.2303  0.1110  0.1969 -0.1182 -0.2352 -0.1834\n",
      "-0.2069  0.0097 -0.1959 -0.1907  0.1193  0.2416\n",
      " 0.1294  0.0248 -0.0660 -0.1881 -0.1173  0.1539\n",
      " 0.2133 -0.1134 -0.2253 -0.1696  0.0935  0.1714\n",
      "-0.0813 -0.1398  0.2138  0.1647 -0.1725  0.1607\n",
      "-0.1893  0.2189 -0.0887  0.0378 -0.1480 -0.0388\n",
      " 0.2199  0.1947  0.0709  0.2221 -0.0370  0.1546\n",
      "-0.1906  0.0130  0.1742 -0.0223  0.0300 -0.1473\n",
      " 0.1885  0.0700  0.1706  0.0461 -0.0945  0.0191\n",
      " 0.1735 -0.0598 -0.0102  0.0124 -0.1049  0.0384\n",
      " 0.0020 -0.0560  0.0457  0.1884  0.2219 -0.1218\n",
      "-0.0167 -0.1600 -0.1056 -0.2369  0.1886 -0.1016\n",
      " 0.2154  0.0043 -0.1271  0.1077 -0.1045 -0.1245\n",
      "-0.0419 -0.0590  0.1792  0.1909 -0.1497  0.0731\n",
      "-0.1290 -0.0334 -0.1334 -0.0612  0.2071 -0.1711\n",
      " 0.0244  0.2409 -0.2101 -0.1064 -0.0316 -0.2069\n",
      " 0.1621  0.1301  0.0089  0.0391  0.0247 -0.0021\n",
      " 0.1654  0.1039 -0.2092  0.2359  0.0720  0.0543\n",
      " 0.0575  0.1522  0.0536 -0.1691 -0.1473  0.0859\n",
      " 0.0738 -0.0973  0.2138  0.1608 -0.0174  0.1651\n",
      "-0.2049 -0.2369 -0.2407 -0.1517  0.1637  0.2193\n",
      " 0.1748  0.1255  0.1106 -0.1538  0.1558 -0.1312\n",
      "-0.1913  0.1029 -0.1505 -0.2168  0.0350  0.1013\n",
      " 0.2473 -0.0222 -0.2041  0.0816  0.1470  0.0464\n",
      "-0.1545 -0.1341 -0.1036  0.0575  0.1872  0.0736\n",
      "-0.1864  0.0842 -0.1093  0.0114  0.1089  0.0125\n",
      " 0.0276 -0.1681 -0.2439  0.1569 -0.0474 -0.2453\n",
      "-0.2342 -0.1593  0.0431 -0.1215  0.0838  0.1008\n",
      " 0.0208  0.0934 -0.0054  0.1601 -0.0374 -0.0238\n",
      "-0.0043  0.2384  0.0591  0.1185 -0.0331 -0.1013\n",
      "-0.1834  0.0699 -0.1774  0.1372  0.0828 -0.0379\n",
      "-0.1828 -0.2320 -0.2061  0.1867 -0.1746 -0.0875\n",
      " 0.2214  0.1672 -0.1937 -0.0088 -0.0202  0.0380\n",
      "-0.1328 -0.1460  0.0235  0.0644 -0.0579 -0.1087\n",
      " 0.1324 -0.0831 -0.2499  0.0646 -0.0033  0.0880\n",
      " 0.1304 -0.2090 -0.0020 -0.1187  0.1515  0.2494\n",
      " 0.0411 -0.0081  0.2064  0.1012  0.1308 -0.1166\n",
      "-0.1001 -0.1028 -0.1331  0.2048 -0.2402 -0.2021\n",
      "-0.1771 -0.0595 -0.0574  0.1788 -0.2390  0.1947\n",
      "-0.2253  0.1967 -0.1812 -0.1400  0.1513  0.2245\n",
      "-0.0441 -0.1947  0.2260  0.1255  0.2308  0.1388\n",
      " 0.0088 -0.0308 -0.1196  0.0121 -0.1286  0.2343\n",
      "-0.1825  0.1320 -0.2325 -0.1694 -0.1471  0.1491\n",
      " 0.2442 -0.1418  0.1031  0.0742 -0.0525  0.0802\n",
      " 0.0806 -0.0788 -0.2176  0.1438  0.0943  0.1706\n",
      "[torch.FloatTensor of size 50x16]\n",
      ", Parameter containing:\n",
      " 0.1433\n",
      " 0.0208\n",
      "-0.2030\n",
      "-0.0310\n",
      "-0.1504\n",
      "-0.0258\n",
      " 0.0003\n",
      "-0.1570\n",
      "-0.1882\n",
      "-0.0672\n",
      " 0.0926\n",
      "-0.1561\n",
      "-0.1968\n",
      "-0.1081\n",
      "-0.1404\n",
      " 0.1744\n",
      " 0.0434\n",
      " 0.1500\n",
      " 0.1923\n",
      "-0.1241\n",
      " 0.0928\n",
      " 0.1018\n",
      " 0.0183\n",
      " 0.1133\n",
      " 0.0253\n",
      "-0.2073\n",
      " 0.1337\n",
      " 0.0910\n",
      "-0.0460\n",
      "-0.2370\n",
      "-0.1089\n",
      " 0.0449\n",
      "-0.0235\n",
      " 0.0644\n",
      " 0.0730\n",
      "-0.0784\n",
      "-0.1528\n",
      "-0.1097\n",
      " 0.2224\n",
      "-0.0892\n",
      " 0.0792\n",
      " 0.1041\n",
      "-0.0326\n",
      "-0.1505\n",
      "-0.1074\n",
      "-0.1631\n",
      "-0.2356\n",
      " 0.0687\n",
      " 0.1900\n",
      " 0.1416\n",
      "[torch.FloatTensor of size 50]\n",
      ", Parameter containing:\n",
      "-0.0608 -0.0624  0.0883  ...   0.0584  0.0786 -0.1001\n",
      "-0.1384 -0.0784 -0.1021  ...   0.0682  0.0986 -0.0628\n",
      " 0.0829  0.0019 -0.0245  ...   0.0375 -0.0496 -0.0888\n",
      "          ...             ⋱             ...          \n",
      " 0.0325  0.1208 -0.0907  ...  -0.0532 -0.0787  0.1147\n",
      " 0.0009 -0.0655 -0.0532  ...  -0.0045  0.1292  0.0785\n",
      " 0.0014  0.1203  0.0273  ...  -0.0020  0.0652 -0.1213\n",
      "[torch.FloatTensor of size 50x50]\n",
      ", Parameter containing:\n",
      "-0.0315\n",
      " 0.1000\n",
      " 0.0854\n",
      " 0.0965\n",
      "-0.1019\n",
      " 0.0209\n",
      "-0.0639\n",
      " 0.0733\n",
      " 0.1332\n",
      "-0.1314\n",
      " 0.1371\n",
      " 0.0352\n",
      " 0.1204\n",
      "-0.0090\n",
      "-0.1324\n",
      " 0.0906\n",
      "-0.0503\n",
      "-0.0376\n",
      " 0.1206\n",
      " 0.0589\n",
      "-0.1275\n",
      " 0.0736\n",
      "-0.1344\n",
      " 0.0091\n",
      " 0.0939\n",
      "-0.1002\n",
      " 0.0538\n",
      "-0.1308\n",
      " 0.1086\n",
      " 0.1189\n",
      "-0.0369\n",
      " 0.0519\n",
      "-0.1114\n",
      "-0.0194\n",
      " 0.1388\n",
      " 0.0651\n",
      " 0.0466\n",
      " 0.1288\n",
      "-0.1378\n",
      "-0.0273\n",
      "-0.1385\n",
      " 0.0848\n",
      "-0.0562\n",
      " 0.1042\n",
      "-0.0150\n",
      " 0.1042\n",
      "-0.0201\n",
      " 0.1397\n",
      " 0.1247\n",
      " 0.0806\n",
      "[torch.FloatTensor of size 50]\n",
      ", Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0329 -0.0067 -0.1156 -0.0211 -0.0736  0.0763 -0.1131  0.0383 -0.1252 -0.1185\n",
      " 0.0066 -0.0286 -0.0016 -0.1046 -0.0123  0.0177 -0.0970  0.1029  0.0949 -0.1335\n",
      " 0.1084  0.0392  0.0392 -0.0120  0.0826  0.0193 -0.1142  0.0784 -0.0565  0.1114\n",
      " 0.1332 -0.1128  0.0823  0.1326 -0.1049 -0.0455  0.0362 -0.0552 -0.0745  0.0679\n",
      " 0.0525  0.1040 -0.0751  0.0566  0.0482  0.0652 -0.0102  0.0608  0.0520  0.0654\n",
      " 0.0334  0.0902 -0.1070 -0.0670  0.0849 -0.1386 -0.1073 -0.0226 -0.1144  0.1197\n",
      " 0.0047  0.0800 -0.0335  0.0672 -0.0694 -0.1339 -0.0835  0.0823  0.0580  0.0683\n",
      "-0.1338 -0.0113  0.0406  0.0394 -0.0261  0.1104 -0.1127  0.0014  0.1333 -0.1221\n",
      " 0.0444 -0.0158  0.0983  0.0034  0.0140 -0.1167 -0.0748 -0.0015 -0.1003  0.0821\n",
      " 0.0248  0.0019  0.0335  0.0274  0.1172  0.0062  0.0983 -0.0545 -0.0804 -0.1378\n",
      "\n",
      "Columns 10 to 19 \n",
      "-0.0473 -0.1316  0.0646  0.1066  0.0969  0.0317  0.0980  0.0815  0.0151  0.0410\n",
      "-0.0645  0.0844  0.0746  0.0581 -0.0282 -0.0538  0.1065 -0.1304 -0.1385 -0.0281\n",
      " 0.0965 -0.1310  0.1071 -0.0482  0.0830 -0.0826 -0.0062  0.0510 -0.0875  0.0951\n",
      "-0.0056  0.0457  0.0225  0.1377  0.1346  0.1358 -0.0294 -0.1073 -0.0775  0.1031\n",
      "-0.0019  0.1176  0.1087 -0.0435 -0.0705  0.0804 -0.0422  0.0833  0.0279  0.0080\n",
      " 0.0767 -0.1378  0.0473  0.0245 -0.0265 -0.0596  0.0541  0.0903 -0.1180 -0.0066\n",
      " 0.1031  0.0286  0.1018  0.0048  0.0887 -0.0016  0.0184 -0.1056 -0.0407  0.0388\n",
      "-0.0182 -0.1193 -0.1115  0.0371 -0.0548  0.1263  0.0216  0.1057 -0.1288 -0.0386\n",
      "-0.1324 -0.0845 -0.1346  0.1350  0.0354  0.0440 -0.1354  0.1143  0.1118 -0.0911\n",
      "-0.0488 -0.0418  0.1413 -0.0730  0.0847 -0.1411 -0.1233 -0.0726 -0.0952 -0.0604\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.0586  0.1282  0.1289 -0.1072  0.1000 -0.0063 -0.1025 -0.1381  0.1292  0.0635\n",
      "-0.0319  0.1015  0.0844  0.1102  0.1319  0.0980  0.0027 -0.0175 -0.0738 -0.0592\n",
      " 0.0869  0.0900  0.1031  0.0425 -0.0759 -0.0945  0.0258  0.0176  0.1043  0.0843\n",
      " 0.1102 -0.1054 -0.1293 -0.0958 -0.0266  0.0201  0.1131 -0.1332  0.0847  0.0043\n",
      "-0.1258  0.1129 -0.1219  0.0019 -0.1409 -0.0625  0.1179  0.0958  0.0945 -0.1258\n",
      " 0.0523 -0.0790 -0.0653 -0.0219 -0.0661 -0.0263 -0.0089  0.1134 -0.0413  0.0322\n",
      "-0.1061  0.0722  0.0554  0.1294 -0.0645  0.0486 -0.0977  0.0702  0.0190 -0.1407\n",
      " 0.1107 -0.0247  0.1400  0.0485 -0.0084 -0.0094  0.1123 -0.0776 -0.1212  0.0107\n",
      "-0.0633  0.1133 -0.1238  0.0891  0.0318  0.0849  0.1303 -0.0547  0.0739 -0.0149\n",
      " 0.0734 -0.0613  0.0439 -0.1096  0.0991 -0.0441  0.1010  0.0717  0.1122 -0.0913\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.0442 -0.0446  0.0142 -0.1102  0.0610  0.0744 -0.0003 -0.1013  0.0097  0.0844\n",
      "-0.0330  0.0163  0.0453 -0.0826 -0.0153 -0.1174  0.0233 -0.0747  0.0591 -0.0348\n",
      "-0.0607  0.0996 -0.1086  0.1327  0.0064  0.0676 -0.0233  0.0879  0.1059  0.1063\n",
      "-0.1025 -0.0930  0.0837  0.0930  0.0270 -0.0416 -0.0052 -0.0001 -0.0942 -0.0470\n",
      "-0.0444  0.0167 -0.0594 -0.0572  0.0406  0.1165 -0.0752 -0.0976  0.0186 -0.0161\n",
      "-0.0085 -0.0237  0.0286 -0.1252 -0.0706  0.0522  0.0668  0.1257  0.0031 -0.1115\n",
      "-0.0557 -0.0363 -0.0251 -0.0227  0.1212  0.1379  0.0435 -0.1401  0.0407  0.1234\n",
      "-0.0405 -0.0803 -0.1266  0.1220 -0.0342  0.0727  0.0314  0.1050 -0.0256 -0.1323\n",
      "-0.0260 -0.1011 -0.0025 -0.0967 -0.0493 -0.1027 -0.0802 -0.0528 -0.0008  0.0130\n",
      "-0.1185 -0.0633  0.0962 -0.0036  0.0389  0.0476 -0.0044  0.0816  0.0440 -0.0651\n",
      "\n",
      "Columns 40 to 49 \n",
      "-0.1016  0.1302 -0.1249  0.0534  0.0700  0.0494 -0.0053 -0.0677  0.0096  0.0540\n",
      " 0.1303 -0.0309 -0.1397  0.0490 -0.0409  0.0742  0.0453 -0.1399 -0.0779  0.1362\n",
      " 0.0376 -0.1228  0.1356 -0.1321  0.0718  0.0727  0.0653  0.0204 -0.0057  0.1053\n",
      " 0.0402 -0.0656 -0.0801 -0.0057  0.0665  0.1224 -0.1101 -0.0102 -0.0164  0.0389\n",
      "-0.0698  0.0847 -0.0488  0.1341 -0.0455 -0.0014  0.0326 -0.0545 -0.0184 -0.0598\n",
      " 0.0481  0.1314 -0.0800  0.1409 -0.0907 -0.0937 -0.1073 -0.0683 -0.0717 -0.0848\n",
      "-0.0328  0.0712  0.0848  0.0993  0.0045 -0.0528 -0.0533 -0.0460 -0.0864  0.0827\n",
      " 0.0061  0.0632 -0.1116  0.0387 -0.0427  0.0265  0.0380  0.1236  0.0115 -0.0628\n",
      " 0.0214 -0.1179 -0.0863  0.0952  0.0033  0.1208  0.0125 -0.1249  0.1239 -0.0882\n",
      " 0.0315  0.0164 -0.0085 -0.0450 -0.0789  0.0394 -0.1072 -0.0888  0.0963 -0.0940\n",
      "[torch.FloatTensor of size 10x50]\n",
      ", Parameter containing:\n",
      " 0.0892\n",
      "-0.0820\n",
      " 0.1061\n",
      "-0.0322\n",
      " 0.0517\n",
      "-0.0110\n",
      "-0.0266\n",
      " 0.0679\n",
      " 0.0184\n",
      " 0.1307\n",
      "[torch.FloatTensor of size 10]\n",
      ", Variable containing:\n",
      " 0.7866\n",
      "-0.6991\n",
      " 0.5702\n",
      "-0.5417\n",
      " 0.3867\n",
      "-1.0279\n",
      " 0.0241\n",
      " 0.8079\n",
      "-1.8278\n",
      "-1.6017\n",
      "[torch.FloatTensor of size 10x1]\n",
      "], 'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}]\n"
     ]
    }
   ],
   "source": [
    "model_ft, optimizer_ft = network_loader(comment=comment, #'Tested for three rooms'\n",
    "                                            optimizer=optimizer,\n",
    "                                            iter_loc=iter_loc,\n",
    "                                            lr=lr,\n",
    "                                            momentum=momentum,\n",
    "                                            weight_decay=weight_decay,\n",
    "                                            lr_scheduler=lr_scheduler,\n",
    "                                            lr_decay_epoch=lr_decay_epoch,\n",
    "                                            nclasses=num_classes)\n",
    "a_vec = Variable(torch.randn(10, 1), requires_grad=True)\n",
    "params = optimizer_ft.param_groups\n",
    "params[0]['params'].append(a_vec)\n",
    "optimizer_ft.param_groups = params\n",
    "print(optimizer_ft.param_groups)\n",
    "#optimizer_ft.add_param_group({'params': a_vec})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_epochs(result_log, logname):\n",
    "    print(len(result_log))\n",
    "\n",
    "    wb_tr = openpyxl.Workbook()\n",
    "    ws_tr = wb_tr.active\n",
    "    wb_val = openpyxl.Workbook()\n",
    "    ws_val = wb_val.active\n",
    "    print(logname)\n",
    "\n",
    "    label_arr_tr = np.zeros((100000,1))\n",
    "    probs_arr_tr = np.zeros((100000, num_classes))\n",
    "    label_arr_val = np.zeros((100000,1))\n",
    "    probs_arr_val = np.zeros((100000, num_classes))\n",
    "\n",
    "    prev_epoch = 0\n",
    "    \n",
    "    count_tr = count_val = 0\n",
    "    for result in result_log:\n",
    "        epoch = result[1]\n",
    "        if not epoch == prev_epoch:\n",
    "            label_arr_tr = label_arr_tr[:count_tr]\n",
    "            probs_arr_tr = probs_arr_tr[:count_tr, :]\n",
    "            label_arr_val = label_arr_val[:count_val]\n",
    "            probs_arr_val = probs_arr_val[:count_val, :]\n",
    "            ws_tr.append(['Epoch ' + str(prev_epoch)])\n",
    "            ws_tr.append(label_arr_tr[1:].reshape(-1).tolist())\n",
    "            ws_tr.append(np.argmax(probs_arr_tr[1:,:], axis=1).reshape(-1).tolist())\n",
    "            for probs in probs_arr_tr[1:,:].T.tolist():\n",
    "                ws_tr.append(probs)\n",
    "            #wb_tr.save('./runs_ord/'+logname + '/train.xlsx')\n",
    "            ws_val.append(['Epoch ' + str(prev_epoch)])\n",
    "            ws_val.append(label_arr_val[1:].reshape(-1).tolist())\n",
    "            ws_val.append(np.argmax(probs_arr_val[1:,:], axis=1).reshape(-1).tolist())\n",
    "            for probs in probs_arr_val[1:,:].T.tolist():\n",
    "                ws_val.append(probs)\n",
    "    \n",
    "\n",
    "            label_arr_tr = np.zeros((100000,1))\n",
    "            probs_arr_tr = np.zeros((100000, num_classes))\n",
    "            label_arr_val = np.zeros((100000,1))\n",
    "            probs_arr_val = np.zeros((100000, num_classes)) \n",
    "            count_tr = count_val = 0\n",
    "            prev_epoch = epoch\n",
    "\n",
    "        label = np.asarray(result[2]).reshape(-1,1)\n",
    "        scores = np.asarray(result[3])\n",
    "        exp_scores = np.exp(scores - np.max(scores,axis=1).reshape(-1, 1)*np.ones(num_classes))\n",
    "        probs = np.round(exp_scores/(np.sum(exp_scores,axis=1).reshape(-1, 1)*np.ones(num_classes)), decimals=2)\n",
    "        if result[0] == 'train':\n",
    "            label_arr_tr[count_tr:count_tr + len(label)]  = label\n",
    "            probs_arr_tr[count_tr:count_tr + len(label), :] = probs\n",
    "            count_tr += len(label)\n",
    "        elif result[0] == 'val':\n",
    "            label_arr_val[count_val:count_val + len(label)]  = label\n",
    "            probs_arr_val[count_val:count_val + len(label), :] = probs\n",
    "            count_val += len(label)\n",
    "\n",
    "\n",
    "    \n",
    "    label_arr_tr = label_arr_tr[:count_tr]\n",
    "    probs_arr_tr = probs_arr_tr[:count_tr, :]\n",
    "    label_arr_val = label_arr_val[:count_val]\n",
    "    probs_arr_val = probs_arr_val[:count_val, :]\n",
    "            \n",
    "    ws_tr.append(['Epoch ' + str(epoch)])\n",
    "    ws_tr.append(label_arr_tr[1:].reshape(-1).tolist())\n",
    "    ws_tr.append(np.argmax(probs_arr_tr[1:,:], axis=1).reshape(-1).tolist())\n",
    "    for probs in probs_arr_tr[1:,:].T.tolist():\n",
    "        ws_tr.append(probs)\n",
    "    #wb_tr.save('./runs_ord/'+logname + '/train.xlsx')\n",
    "    ws_val.append(['Epoch ' + str(epoch)])\n",
    "    ws_val.append(label_arr_val[1:].reshape(-1).tolist())\n",
    "    ws_val.append(np.argmax(probs_arr_val[1:,:], axis=1).reshape(-1).tolist())\n",
    "    for probs in probs_arr_val[1:,:].T.tolist():\n",
    "        ws_val.append(probs)\n",
    "    wb_val.save('./runs_regression/'+logname + '/val.xlsx')\n",
    "    label_arr_tr = np.zeros((1,1))\n",
    "    probs_arr_tr = np.zeros((1, num_classes))\n",
    "    label_arr_val = np.zeros((1,1))\n",
    "    probs_arr_val = np.zeros((1, num_classes))\n",
    "    prev_epoch = epoch\n",
    "    print('Finito')\n",
    "    \n",
    "    del label_arr_tr, probs_arr_tr, label_arr_val, probs_arr_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(ft)\n",
    "    \n",
    "def run_network():\n",
    "    '''\n",
    "    Cretaes the log files and starts the training\n",
    "    '''\n",
    "    model_ft, optimizer_ft = network_loader(comment=comment, #'Tested for three rooms'\n",
    "                                            optimizer=optimizer,\n",
    "                                            iter_loc=iter_loc,\n",
    "                                            lr=lr,\n",
    "                                            momentum=momentum,\n",
    "                                            weight_decay=weight_decay,\n",
    "                                            lr_scheduler=lr_scheduler,\n",
    "                                            lr_decay_epoch=lr_decay_epoch,\n",
    "                                            nclasses=num_classes)\n",
    "    \n",
    "    \n",
    "    '''Name of the trial'''\n",
    "    crt=str(single_loss)+'xsingle + '+str(multi_loss)+'Xmulti'\n",
    "    logname='Ordinal_'+datetime.now().strftime('%B%d  %H:%M:%S')\n",
    "    writer = SummaryWriter('runs_regression/'+logname) #For tensorboard\n",
    "    writeLog(logname)\n",
    "    writeLog_xlsx()\n",
    "    \n",
    "    '''Start trianing'''\n",
    "    if metric:\n",
    "        m_coeff = make_coeff(nclasses, metric, coeff_lmbda)\n",
    "    else:\n",
    "        m_coeff = multi_coeff\n",
    "    best_model, last_model, result_log = ft.train_model(model_ft,optimizer_ft, lr_scheduler,dset_loaders,\n",
    "                            dset_sizes,writer,use_gpu=use_gpu,num_epochs=100,batch_size=batch_size,num_log=250,\n",
    "                            lr_decay_epoch=lr_decay_epoch,init_lr=lr,regression=False,\n",
    "                            iter_loc=iter_loc,cross_loss=single_loss,multi_loss=multi_loss,numOut=num_classes,\n",
    "                            logname='logs_regression.xlsx',\n",
    "                            multi_coeff = m_coeff, single_coeff = m_coeff, KL = KL, algo = algo)\n",
    "    \n",
    "    '''Save the models'''\n",
    "    torch.save(best_model,'./saved_models/ord/'+logname+'_best')\n",
    "    torch.save(last_model,'./saved_models/ord/'+logname+'_last')\n",
    "    \n",
    "    '''print('Writing results')\n",
    "    write_epochs(result_log, logname)\n",
    "    print('Wrote results')'''\n",
    "    '''Free up the memory'''\n",
    "    del model_ft, result_log\n",
    "    \n",
    "    writer.close\n",
    "    del writer\n",
    "    return last_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22784, 16)\n"
     ]
    }
   ],
   "source": [
    "'''hidden_sizes = [50, 50]\n",
    "dropouts = [0, 0]\n",
    "end_to_end = True\n",
    "run_network()'''\n",
    "print(fvec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs_regression.xlsx\n",
      "Multi_coef is [1]\n",
      "Epoch 0/99\n",
      "----------\n",
      "LR is set to 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mtezcan/New Volume/amazon/notebook/functions/fine_tune.py:113: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.3. Note that arange generates values in [start; end), not [start; end].\n",
      "  j_vec = Variable(torch.range(0, numOut-1).type(torch.FloatTensor).cuda().view(1, numOut))\n",
      "/media/mtezcan/New Volume/amazon/notebook/functions/fine_tune.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  outputs_log_softmax = log_soft(outputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.1486 Acc: 0.1092 CIR-1: 0.2978 RMSE 3.7726 MAE 3.0185\n",
      "val Loss: 1.4041 Acc: 0.1027 CIR-1: 0.2026 RMSE 5.4132 MAE 4.5426\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 1.0255 Acc: 0.1326 CIR-1: 0.3692 RMSE 3.1163 MAE 2.4580\n",
      "val Loss: 0.9519 Acc: 0.1629 CIR-1: 0.4467 RMSE 2.6771 MAE 2.0869\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.9049 Acc: 0.1633 CIR-1: 0.4606 RMSE 2.5153 MAE 1.9545\n",
      "val Loss: 0.9156 Acc: 0.1690 CIR-1: 0.5022 RMSE 2.3876 MAE 1.8393\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.8834 Acc: 0.1721 CIR-1: 0.4742 RMSE 2.4733 MAE 1.9058\n",
      "val Loss: 0.9527 Acc: 0.1455 CIR-1: 0.3604 RMSE 3.0459 MAE 2.4151\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.8472 Acc: 0.1835 CIR-1: 0.5089 RMSE 2.3301 MAE 1.7791\n",
      "val Loss: 0.8341 Acc: 0.2019 CIR-1: 0.5514 RMSE 2.1398 MAE 1.6223\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.8336 Acc: 0.1909 CIR-1: 0.5256 RMSE 2.2890 MAE 1.7376\n",
      "val Loss: 0.8951 Acc: 0.1694 CIR-1: 0.5162 RMSE 2.3297 MAE 1.7917\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.8144 Acc: 0.1961 CIR-1: 0.5491 RMSE 2.1983 MAE 1.6617\n",
      "val Loss: 0.9394 Acc: 0.1407 CIR-1: 0.3461 RMSE 3.0018 MAE 2.4131\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.8107 Acc: 0.1976 CIR-1: 0.5456 RMSE 2.2215 MAE 1.6757\n",
      "val Loss: 0.7911 Acc: 0.2098 CIR-1: 0.6069 RMSE 2.0362 MAE 1.5134\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.8095 Acc: 0.2000 CIR-1: 0.5556 RMSE 2.2032 MAE 1.6579\n",
      "val Loss: 0.9201 Acc: 0.1583 CIR-1: 0.3828 RMSE 3.0283 MAE 2.3545\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.7946 Acc: 0.2070 CIR-1: 0.5727 RMSE 2.1392 MAE 1.5999\n",
      "val Loss: 0.8338 Acc: 0.1953 CIR-1: 0.5663 RMSE 2.1170 MAE 1.6133\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.7979 Acc: 0.2049 CIR-1: 0.5693 RMSE 2.1418 MAE 1.6036\n",
      "val Loss: 0.7850 Acc: 0.2175 CIR-1: 0.6010 RMSE 2.0071 MAE 1.4952\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.7916 Acc: 0.2100 CIR-1: 0.5689 RMSE 2.1267 MAE 1.5928\n",
      "val Loss: 0.8617 Acc: 0.1984 CIR-1: 0.5854 RMSE 2.1805 MAE 1.6245\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.7876 Acc: 0.2147 CIR-1: 0.5790 RMSE 2.1236 MAE 1.5788\n",
      "val Loss: 0.8812 Acc: 0.1927 CIR-1: 0.5628 RMSE 2.2353 MAE 1.6811\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.7856 Acc: 0.2118 CIR-1: 0.5762 RMSE 2.1046 MAE 1.5723\n",
      "val Loss: 0.7895 Acc: 0.2103 CIR-1: 0.5358 RMSE 2.1689 MAE 1.6387\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.7778 Acc: 0.2249 CIR-1: 0.5925 RMSE 2.0767 MAE 1.5305\n",
      "val Loss: 0.8282 Acc: 0.1859 CIR-1: 0.4638 RMSE 2.5468 MAE 1.9377\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "LR is set to 0.1\n",
      "train Loss: 0.7598 Acc: 0.2277 CIR-1: 0.6129 RMSE 1.9941 MAE 1.4682\n",
      "val Loss: 0.7634 Acc: 0.2349 CIR-1: 0.6137 RMSE 1.9980 MAE 1.4609\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.7540 Acc: 0.2336 CIR-1: 0.6259 RMSE 1.9577 MAE 1.4349\n",
      "val Loss: 0.7636 Acc: 0.2430 CIR-1: 0.6227 RMSE 1.9745 MAE 1.4348\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.7533 Acc: 0.2377 CIR-1: 0.6271 RMSE 1.9492 MAE 1.4251\n",
      "val Loss: 0.7637 Acc: 0.2344 CIR-1: 0.5994 RMSE 2.0303 MAE 1.4884\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.7523 Acc: 0.2372 CIR-1: 0.6280 RMSE 1.9562 MAE 1.4290\n",
      "val Loss: 0.7618 Acc: 0.2471 CIR-1: 0.6214 RMSE 1.9713 MAE 1.4300\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.7521 Acc: 0.2383 CIR-1: 0.6276 RMSE 1.9488 MAE 1.4242\n",
      "val Loss: 0.7668 Acc: 0.2471 CIR-1: 0.6317 RMSE 1.9579 MAE 1.4188\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.7511 Acc: 0.2420 CIR-1: 0.6282 RMSE 1.9473 MAE 1.4200\n",
      "val Loss: 0.7604 Acc: 0.2406 CIR-1: 0.6159 RMSE 1.9867 MAE 1.4475\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.7501 Acc: 0.2436 CIR-1: 0.6280 RMSE 1.9464 MAE 1.4184\n",
      "val Loss: 0.7597 Acc: 0.2450 CIR-1: 0.6245 RMSE 1.9621 MAE 1.4263\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.7507 Acc: 0.2434 CIR-1: 0.6287 RMSE 1.9425 MAE 1.4158\n",
      "val Loss: 0.7602 Acc: 0.2434 CIR-1: 0.6218 RMSE 1.9594 MAE 1.4267\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.7492 Acc: 0.2422 CIR-1: 0.6290 RMSE 1.9427 MAE 1.4175\n",
      "val Loss: 0.7623 Acc: 0.2467 CIR-1: 0.6293 RMSE 1.9503 MAE 1.4153\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.7492 Acc: 0.2417 CIR-1: 0.6308 RMSE 1.9413 MAE 1.4148\n",
      "val Loss: 0.7586 Acc: 0.2478 CIR-1: 0.6306 RMSE 1.9424 MAE 1.4100\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.7489 Acc: 0.2448 CIR-1: 0.6304 RMSE 1.9341 MAE 1.4095\n",
      "val Loss: 0.7568 Acc: 0.2474 CIR-1: 0.6225 RMSE 1.9625 MAE 1.4243\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.7470 Acc: 0.2449 CIR-1: 0.6325 RMSE 1.9333 MAE 1.4075\n",
      "val Loss: 0.7573 Acc: 0.2441 CIR-1: 0.6152 RMSE 1.9970 MAE 1.4504\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.7476 Acc: 0.2435 CIR-1: 0.6315 RMSE 1.9378 MAE 1.4118\n",
      "val Loss: 0.7557 Acc: 0.2403 CIR-1: 0.6207 RMSE 1.9745 MAE 1.4388\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.7471 Acc: 0.2429 CIR-1: 0.6291 RMSE 1.9373 MAE 1.4131\n",
      "val Loss: 0.7561 Acc: 0.2414 CIR-1: 0.6207 RMSE 1.9754 MAE 1.4363\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.7466 Acc: 0.2440 CIR-1: 0.6326 RMSE 1.9301 MAE 1.4069\n",
      "val Loss: 0.7576 Acc: 0.2498 CIR-1: 0.6295 RMSE 1.9497 MAE 1.4094\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "LR is set to 0.010000000000000002\n",
      "train Loss: 0.7465 Acc: 0.2468 CIR-1: 0.6347 RMSE 1.9301 MAE 1.4026\n",
      "val Loss: 0.7553 Acc: 0.2430 CIR-1: 0.6179 RMSE 1.9685 MAE 1.4339\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.7452 Acc: 0.2455 CIR-1: 0.6335 RMSE 1.9286 MAE 1.4037\n",
      "val Loss: 0.7551 Acc: 0.2430 CIR-1: 0.6234 RMSE 1.9618 MAE 1.4269\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.7450 Acc: 0.2462 CIR-1: 0.6361 RMSE 1.9213 MAE 1.3975\n",
      "val Loss: 0.7548 Acc: 0.2441 CIR-1: 0.6223 RMSE 1.9654 MAE 1.4291\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.7444 Acc: 0.2457 CIR-1: 0.6338 RMSE 1.9267 MAE 1.4021\n",
      "val Loss: 0.7548 Acc: 0.2445 CIR-1: 0.6253 RMSE 1.9577 MAE 1.4221\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.7445 Acc: 0.2459 CIR-1: 0.6350 RMSE 1.9239 MAE 1.3997\n",
      "val Loss: 0.7541 Acc: 0.2443 CIR-1: 0.6209 RMSE 1.9630 MAE 1.4284\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.7449 Acc: 0.2469 CIR-1: 0.6346 RMSE 1.9238 MAE 1.3991\n",
      "val Loss: 0.7542 Acc: 0.2447 CIR-1: 0.6214 RMSE 1.9613 MAE 1.4269\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.7445 Acc: 0.2463 CIR-1: 0.6335 RMSE 1.9265 MAE 1.4014\n",
      "val Loss: 0.7550 Acc: 0.2469 CIR-1: 0.6266 RMSE 1.9486 MAE 1.4151\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.7441 Acc: 0.2471 CIR-1: 0.6359 RMSE 1.9193 MAE 1.3963\n",
      "val Loss: 0.7547 Acc: 0.2434 CIR-1: 0.6205 RMSE 1.9676 MAE 1.4326\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.7449 Acc: 0.2457 CIR-1: 0.6342 RMSE 1.9247 MAE 1.4008\n",
      "val Loss: 0.7545 Acc: 0.2428 CIR-1: 0.6198 RMSE 1.9663 MAE 1.4326\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.7448 Acc: 0.2452 CIR-1: 0.6346 RMSE 1.9263 MAE 1.4018\n",
      "val Loss: 0.7541 Acc: 0.2441 CIR-1: 0.6198 RMSE 1.9638 MAE 1.4298\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.7450 Acc: 0.2479 CIR-1: 0.6343 RMSE 1.9221 MAE 1.3980\n",
      "val Loss: 0.7551 Acc: 0.2423 CIR-1: 0.6201 RMSE 1.9660 MAE 1.4328\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.7452 Acc: 0.2471 CIR-1: 0.6332 RMSE 1.9256 MAE 1.4009\n",
      "val Loss: 0.7550 Acc: 0.2419 CIR-1: 0.6229 RMSE 1.9642 MAE 1.4304\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.7449 Acc: 0.2468 CIR-1: 0.6346 RMSE 1.9211 MAE 1.3977\n",
      "val Loss: 0.7549 Acc: 0.2439 CIR-1: 0.6238 RMSE 1.9586 MAE 1.4243\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.7442 Acc: 0.2481 CIR-1: 0.6363 RMSE 1.9183 MAE 1.3945\n",
      "val Loss: 0.7545 Acc: 0.2423 CIR-1: 0.6209 RMSE 1.9640 MAE 1.4309\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.7444 Acc: 0.2475 CIR-1: 0.6336 RMSE 1.9250 MAE 1.3997\n",
      "val Loss: 0.7543 Acc: 0.2425 CIR-1: 0.6238 RMSE 1.9595 MAE 1.4263\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "LR is set to 0.0010000000000000002\n",
      "train Loss: 0.7443 Acc: 0.2472 CIR-1: 0.6350 RMSE 1.9208 MAE 1.3969\n",
      "val Loss: 0.7542 Acc: 0.2430 CIR-1: 0.6234 RMSE 1.9598 MAE 1.4265\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.7448 Acc: 0.2469 CIR-1: 0.6347 RMSE 1.9212 MAE 1.3978\n",
      "val Loss: 0.7545 Acc: 0.2430 CIR-1: 0.6231 RMSE 1.9600 MAE 1.4267\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.7450 Acc: 0.2477 CIR-1: 0.6353 RMSE 1.9204 MAE 1.3963\n",
      "val Loss: 0.7547 Acc: 0.2423 CIR-1: 0.6227 RMSE 1.9610 MAE 1.4280\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-1741:\n",
      "Process Process-1745:\n",
      "Process Process-1749:\n",
      "Process Process-1744:\n",
      "Process Process-1743:\n",
      "Process Process-1751:\n",
      "Process Process-1750:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "Process Process-1747:\n",
      "Process Process-1746:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "Process Process-1752:\n",
      "Process Process-1748:\n",
      "Traceback (most recent call last):\n",
      "Process Process-1742:\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-dc5d790c7d9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0malgo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'poisson'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdset_loaders\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdset_loaders_arr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mrun_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-1d854c7fac37>\u001b[0m in \u001b[0;36mrun_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                             \u001b[0miter_loc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter_loc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcross_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msingle_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmulti_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumOut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                             \u001b[0mlogname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logs_regression.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                             multi_coeff = m_coeff, single_coeff = m_coeff, KL = KL, algo = algo)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;34m'''Save the models'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mtezcan/New Volume/amazon/notebook/functions/fine_tune.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, lr_scheduler, dset_loaders, dset_sizes, writer, use_gpu, num_epochs, batch_size, num_log, init_lr, lr_decay_epoch, regression, learn_a, cross_loss, multi_loss, numOut, logname, iter_loc, multi_coeff, single_coeff, KL, poisson, algo)\u001b[0m\n\u001b[1;32m    194\u001b[0m                             \u001b[0mextend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_coeff\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                             \u001b[0mlabel_multi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumOut\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mextend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                             \u001b[0mlabel_multi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mextend\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingle_coeff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mextend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                                 \u001b[0mlabel_multi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_multi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7fa8e842e780>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 319, in _shutdown_workers\n",
      "    self.data_queue.get()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 345, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mtezcan/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "end_to_end = True\n",
    "optimizer='sgd' #Optimizer function\n",
    "lr=1 #Initial learning rate\n",
    "momentum=0.5\n",
    "weight_decay=0.0005\n",
    "lr_scheduler=ft.exp_lr_scheduler #Learning rate scheduler\n",
    "lr_decay_epoch=15 #Number of epoch for learning rate decay\n",
    "\n",
    "hidden_sizes = [64, 64, 128, 128, 256, 512, 256, 128, 64, 32, 16]\n",
    "dropouts = [0, 0, 0, 0, 0, .5, .5, .5, 0, 0, 0]\n",
    "\n",
    "'''hidden_size = [64, 64, 128, 64, 32]\n",
    "dropouts = [0, 0, 0, 0, 0]'''\n",
    "\n",
    "single_loss=0.\n",
    "multi_loss =1.\n",
    "\n",
    "metric = 'mae'\n",
    "algo = 'poisson'\n",
    "for dset_loaders in dset_loaders_arr:\n",
    "    run_network()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.72625953  0.235544    0.03819638]\n",
      " [ 0.17899252  0.39192492  0.4290826 ]\n",
      " [ 0.68682605  0.26286978  0.05030423]\n",
      " [ 0.69291276  0.25876865  0.04831864]\n",
      " [ 0.50891167  0.36219808  0.12889017]]\n",
      "(5, 1)\n",
      "(5, 1)\n",
      "(5, 1)\n",
      "[[ 0.72625959  0.23554402  0.03819639]\n",
      " [ 0.17899252  0.3919249   0.42908258]\n",
      " [ 0.68682599  0.26286977  0.05030424]\n",
      " [ 0.69291273  0.25876863  0.04831864]\n",
      " [ 0.50891171  0.3621981   0.12889019]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtezcan/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:6: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.3. Note that arange generates values in [start; end), not [start; end].\n"
     ]
    }
   ],
   "source": [
    "numOut = 3\n",
    "numIns = 5\n",
    "\n",
    "log_j_fact = np.log(np.asarray([math.factorial(j) for j in range(numOut)]))\n",
    "ones_vec = Variable(torch.ones(numOut).type(torch.FloatTensor).cuda().view(1, numOut))\n",
    "j_vec = Variable(torch.range(0, numOut-1).type(torch.FloatTensor).cuda().view(1, numOut))\n",
    "log_j_fact = Variable(torch.from_numpy(log_j_fact).type(torch.FloatTensor).cuda().view(1, numOut))\n",
    "\n",
    "\n",
    "preds = Variable(torch.randn(numIns,1).cuda())\n",
    "softplus_step = torch.nn.Softplus()\n",
    "preds = softplus_step(preds)\n",
    "outputs = torch.mm(preds, ones_vec)\n",
    "outputs = j_vec * torch.log(outputs) - outputs - log_j_fact\n",
    "softmax_step = torch.nn.Softmax(dim=1)\n",
    "outputs_softmax = softmax_step(outputs)\n",
    "print(outputs_softmax.data.cpu().numpy())\n",
    "\n",
    "f = preds.data.cpu().numpy()\n",
    "pos_f = np.zeros((numIns,numOut))\n",
    "\n",
    "log_j = np.asarray([(np.log(math.factorial(k))) for k in range(numOut)]).reshape(numOut,1)\n",
    "for k in range(numOut):\n",
    "    print((k*np.log(f) -f - np.log(math.factorial(k))).shape)\n",
    "    pos_f[:, k] = (k*np.log(f) -f - np.log(math.factorial(k))).reshape(numIns)\n",
    "    \n",
    "soft_pos = np.exp(pos_f)\n",
    "soft_pos = soft_pos/(np.sum(soft_pos, axis=1).reshape(-1,1)*np.ones((1, numOut)))\n",
    "print(soft_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.36134355]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f21414d28d0>]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f21414d2470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math \n",
    "f= np.random.randn(1)*5\n",
    "f = np.log(1+np.exp(f))\n",
    "print(f)\n",
    "kk = 10\n",
    "pp = [(k*np.log(f) - f - np.log(math.factorial(k)))[0] for k in range(kk)]\n",
    "pp = np.exp(pp)\n",
    "pp = pp/np.sum(pp)\n",
    "plt.plot(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "end_to_end = True\n",
    "optimizer='sgd' #Optimizer function\n",
    "lr=1 #Initial learning rate\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "lr_scheduler=ft.exp_lr_scheduler #Learning rate scheduler\n",
    "lr_decay_epoch=40 #Number of epoch for learning rate decay\n",
    "\n",
    "hidden_sizes = [64, 64, 128, 128, 256, 512, 256, 128, 64, 32, 16]#8, 16, 8, 4, 4]\n",
    "dropouts = [0, 0, .5, .5, .5, .5, .5, .5, .5, 0, 0]#.5, .5, .5]\n",
    "\n",
    "for kk in range(3):\n",
    "    single_loss=1.\n",
    "    multi_loss =0.\n",
    "    KL = True\n",
    "    metric = 'ccr'\n",
    "    for dset_loaders in dset_loaders_arr:\n",
    "        run_network()\n",
    "\n",
    "    metric = 'ccr1'\n",
    "    for dset_loaders in dset_loaders_arr:\n",
    "        run_network()\n",
    "\n",
    "    metric = 'mae'\n",
    "    for dset_loaders in dset_loaders_arr:\n",
    "        run_network()\n",
    "\n",
    "    single_loss=0.\n",
    "    multi_loss =1.\n",
    "    metric = 'ccr'\n",
    "    for dset_loaders in dset_loaders_arr:\n",
    "        run_network()\n",
    "\n",
    "    metric = 'ccr1'\n",
    "    for dset_loaders in dset_loaders_arr:\n",
    "        run_network()\n",
    "\n",
    "    metric = 'mae'\n",
    "    for dset_loaders in dset_loaders_arr:\n",
    "        run_network()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm is learn_a\n",
      "Multi_coef is [ 0.  1.  0.]\n",
      "Epoch 0/99\n",
      "----------\n",
      "LR is set to 0.05\n",
      "Variable containing:\n",
      "-2.0074\n",
      "-0.1140\n",
      "-0.1974\n",
      " 0.3888\n",
      "-0.6736\n",
      " 0.2404\n",
      "-1.6391\n",
      " 0.1642\n",
      "-0.1674\n",
      " 0.6457\n",
      "[torch.cuda.FloatTensor of size 10x1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1025  0.0933  0.0941  ...   0.0983  0.0927  0.1111\n",
      " 0.1047  0.0948  0.0944  ...   0.1025  0.0950  0.1072\n",
      " 0.1021  0.0927  0.0965  ...   0.1045  0.0949  0.1064\n",
      "          ...             ⋱             ...          \n",
      " 0.1040  0.0948  0.0939  ...   0.1008  0.0922  0.1090\n",
      " 0.0991  0.0941  0.0972  ...   0.1027  0.0940  0.1044\n",
      " 0.1043  0.0955  0.0946  ...   0.1007  0.0952  0.1086\n",
      "[torch.cuda.FloatTensor of size 256x10 (GPU 0)]\n",
      "\n",
      "Preds is Variable containing:\n",
      "-0.3485\n",
      "-0.3582\n",
      "-0.3581\n",
      "-0.3557\n",
      "-0.3569\n",
      "-0.3565\n",
      "-0.3537\n",
      "-0.3562\n",
      "-0.3538\n",
      "-0.3593\n",
      "-0.3631\n",
      "-0.3548\n",
      "-0.3626\n",
      "-0.3539\n",
      "-0.3641\n",
      "-0.3569\n",
      "-0.3429\n",
      "-0.3500\n",
      "-0.3584\n",
      "-0.3521\n",
      "-0.3549\n",
      "-0.3500\n",
      "-0.3553\n",
      "-0.3625\n",
      "-0.3547\n",
      "-0.3440\n",
      "-0.3671\n",
      "-0.3595\n",
      "-0.3572\n",
      "-0.3566\n",
      "-0.3550\n",
      "-0.3568\n",
      "-0.3552\n",
      "-0.3466\n",
      "-0.3515\n",
      "-0.3580\n",
      "-0.3507\n",
      "-0.3622\n",
      "-0.3522\n",
      "-0.3573\n",
      "-0.3533\n",
      "-0.3606\n",
      "-0.3545\n",
      "-0.3552\n",
      "-0.3532\n",
      "-0.3560\n",
      "-0.3558\n",
      "-0.3596\n",
      "-0.3607\n",
      "-0.3568\n",
      "-0.3500\n",
      "-0.3619\n",
      "-0.3456\n",
      "-0.3521\n",
      "-0.3497\n",
      "-0.3502\n",
      "-0.3576\n",
      "-0.3547\n",
      "-0.3538\n",
      "-0.3626\n",
      "-0.3592\n",
      "-0.3575\n",
      "-0.3506\n",
      "-0.3596\n",
      "-0.3520\n",
      "-0.3583\n",
      "-0.3605\n",
      "-0.3510\n",
      "-0.3510\n",
      "-0.3536\n",
      "-0.3537\n",
      "-0.3552\n",
      "-0.3535\n",
      "-0.3489\n",
      "-0.3636\n",
      "-0.3608\n",
      "-0.3519\n",
      "-0.3540\n",
      "-0.3575\n",
      "-0.3590\n",
      "-0.3533\n",
      "-0.3562\n",
      "-0.3479\n",
      "-0.3565\n",
      "-0.3520\n",
      "-0.3475\n",
      "-0.3591\n",
      "-0.3483\n",
      "-0.3597\n",
      "-0.3521\n",
      "-0.3544\n",
      "-0.3516\n",
      "-0.3426\n",
      "-0.3565\n",
      "-0.3523\n",
      "-0.3527\n",
      "-0.3632\n",
      "-0.3593\n",
      "-0.3567\n",
      "-0.3530\n",
      "-0.3582\n",
      "-0.3545\n",
      "-0.3480\n",
      "-0.3613\n",
      "-0.3507\n",
      "-0.3571\n",
      "-0.3498\n",
      "-0.3582\n",
      "-0.3532\n",
      "-0.3574\n",
      "-0.3580\n",
      "-0.3658\n",
      "-0.3562\n",
      "-0.3550\n",
      "-0.3553\n",
      "-0.3565\n",
      "-0.3539\n",
      "-0.3554\n",
      "-0.3443\n",
      "-0.3432\n",
      "-0.3557\n",
      "-0.3527\n",
      "-0.3523\n",
      "-0.3609\n",
      "-0.3617\n",
      "-0.3550\n",
      "-0.3551\n",
      "-0.3572\n",
      "-0.3582\n",
      "-0.3643\n",
      "-0.3462\n",
      "-0.3582\n",
      "-0.3487\n",
      "-0.3535\n",
      "-0.3621\n",
      "-0.3536\n",
      "-0.3597\n",
      "-0.3553\n",
      "-0.3498\n",
      "-0.3624\n",
      "-0.3547\n",
      "-0.3630\n",
      "-0.3564\n",
      "-0.3522\n",
      "-0.3460\n",
      "-0.3576\n",
      "-0.3596\n",
      "-0.3566\n",
      "-0.3567\n",
      "-0.3543\n",
      "-0.3546\n",
      "-0.3643\n",
      "-0.3516\n",
      "-0.3510\n",
      "-0.3547\n",
      "-0.3558\n",
      "-0.3575\n",
      "-0.3450\n",
      "-0.3524\n",
      "-0.3457\n",
      "-0.3568\n",
      "-0.3531\n",
      "-0.3567\n",
      "-0.3608\n",
      "-0.3522\n",
      "-0.3484\n",
      "-0.3672\n",
      "-0.3596\n",
      "-0.3602\n",
      "-0.3528\n",
      "-0.3526\n",
      "-0.3599\n",
      "-0.3584\n",
      "-0.3516\n",
      "-0.3520\n",
      "-0.3522\n",
      "-0.3599\n",
      "-0.3585\n",
      "-0.3533\n",
      "-0.3580\n",
      "-0.3579\n",
      "-0.3564\n",
      "-0.3524\n",
      "-0.3535\n",
      "-0.3665\n",
      "-0.3569\n",
      "-0.3565\n",
      "-0.3593\n",
      "-0.3604\n",
      "-0.3484\n",
      "-0.3562\n",
      "-0.3601\n",
      "-0.3568\n",
      "-0.3592\n",
      "-0.3587\n",
      "-0.3563\n",
      "-0.3555\n",
      "-0.3611\n",
      "-0.3581\n",
      "-0.3522\n",
      "-0.3583\n",
      "-0.3568\n",
      "-0.3759\n",
      "-0.3512\n",
      "-0.3576\n",
      "-0.3538\n",
      "-0.3520\n",
      "-0.3598\n",
      "-0.3633\n",
      "-0.3543\n",
      "-0.3587\n",
      "-0.3593\n",
      "-0.3570\n",
      "-0.3562\n",
      "-0.3649\n",
      "-0.3607\n",
      "-0.3645\n",
      "-0.3560\n",
      "-0.3551\n",
      "-0.3641\n",
      "-0.3504\n",
      "-0.3484\n",
      "-0.3557\n",
      "-0.3465\n",
      "-0.3590\n",
      "-0.3532\n",
      "-0.3570\n",
      "-0.3538\n",
      "-0.3473\n",
      "-0.3618\n",
      "-0.3533\n",
      "-0.3521\n",
      "-0.3554\n",
      "-0.3630\n",
      "-0.3565\n",
      "-0.3499\n",
      "-0.3602\n",
      "-0.3556\n",
      "-0.3522\n",
      "-0.3576\n",
      "-0.3543\n",
      "-0.3543\n",
      "-0.3561\n",
      "-0.3528\n",
      "-0.3560\n",
      "-0.3582\n",
      "-0.3500\n",
      "-0.3585\n",
      "-0.3539\n",
      "-0.3585\n",
      "-0.3560\n",
      "-0.3517\n",
      "-0.3571\n",
      "-0.3544\n",
      "-0.3531\n",
      "-0.3544\n",
      "[torch.cuda.FloatTensor of size 256x1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 8\n",
      " 0\n",
      " 6\n",
      " 0\n",
      " 9\n",
      " 0\n",
      " 7\n",
      " 5\n",
      " 9\n",
      " 4\n",
      " 3\n",
      " 6\n",
      " 0\n",
      " 8\n",
      " 6\n",
      " 2\n",
      " 7\n",
      " 8\n",
      " 2\n",
      " 6\n",
      " 2\n",
      " 7\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 9\n",
      " 0\n",
      " 6\n",
      " 5\n",
      " 7\n",
      " 9\n",
      " 6\n",
      " 6\n",
      " 0\n",
      " 8\n",
      " 0\n",
      " 2\n",
      " 8\n",
      " 6\n",
      " 3\n",
      " 6\n",
      " 1\n",
      " 4\n",
      " 2\n",
      " 7\n",
      " 6\n",
      " 0\n",
      " 2\n",
      " 3\n",
      " 8\n",
      " 9\n",
      " 3\n",
      " 4\n",
      " 4\n",
      " 9\n",
      " 8\n",
      " 7\n",
      " 9\n",
      " 1\n",
      " 7\n",
      " 1\n",
      " 6\n",
      " 8\n",
      " 6\n",
      " 9\n",
      " 2\n",
      " 0\n",
      " 7\n",
      " 9\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 9\n",
      " 4\n",
      " 1\n",
      " 3\n",
      " 7\n",
      " 9\n",
      " 2\n",
      " 0\n",
      " 3\n",
      " 2\n",
      " 5\n",
      " 2\n",
      " 3\n",
      " 8\n",
      " 2\n",
      " 3\n",
      " 6\n",
      " 6\n",
      " 4\n",
      " 7\n",
      " 6\n",
      " 5\n",
      " 8\n",
      " 8\n",
      " 4\n",
      " 4\n",
      " 7\n",
      " 6\n",
      " 4\n",
      " 5\n",
      " 2\n",
      " 5\n",
      " 9\n",
      " 0\n",
      " 0\n",
      " 5\n",
      " 8\n",
      " 5\n",
      " 5\n",
      " 4\n",
      " 2\n",
      " 8\n",
      " 9\n",
      " 8\n",
      " 5\n",
      " 0\n",
      " 9\n",
      " 5\n",
      " 5\n",
      " 9\n",
      " 4\n",
      " 3\n",
      " 0\n",
      " 0\n",
      " 8\n",
      " 5\n",
      " 4\n",
      " 1\n",
      " 9\n",
      " 2\n",
      " 5\n",
      " 8\n",
      " 3\n",
      " 8\n",
      " 6\n",
      " 3\n",
      " 7\n",
      " 2\n",
      " 8\n",
      " 1\n",
      " 1\n",
      " 8\n",
      " 8\n",
      " 3\n",
      " 2\n",
      " 0\n",
      " 6\n",
      " 7\n",
      " 5\n",
      " 1\n",
      " 9\n",
      " 4\n",
      " 8\n",
      " 7\n",
      " 0\n",
      " 8\n",
      " 9\n",
      " 0\n",
      " 3\n",
      " 0\n",
      " 5\n",
      " 1\n",
      " 8\n",
      " 7\n",
      " 3\n",
      " 0\n",
      " 4\n",
      " 5\n",
      " 9\n",
      " 3\n",
      " 3\n",
      " 1\n",
      " 6\n",
      " 2\n",
      " 0\n",
      " 3\n",
      " 5\n",
      " 1\n",
      " 3\n",
      " 2\n",
      " 3\n",
      " 3\n",
      " 0\n",
      " 2\n",
      " 2\n",
      " 0\n",
      " 2\n",
      " 6\n",
      " 2\n",
      " 1\n",
      " 6\n",
      " 4\n",
      " 0\n",
      " 4\n",
      " 6\n",
      " 4\n",
      " 6\n",
      " 6\n",
      " 5\n",
      " 6\n",
      " 7\n",
      " 0\n",
      " 8\n",
      " 5\n",
      " 3\n",
      " 2\n",
      " 7\n",
      " 5\n",
      " 0\n",
      " 0\n",
      " 2\n",
      " 0\n",
      " 4\n",
      " 2\n",
      " 5\n",
      " 3\n",
      " 3\n",
      " 2\n",
      " 2\n",
      " 8\n",
      " 6\n",
      " 9\n",
      " 6\n",
      " 2\n",
      " 6\n",
      " 9\n",
      " 7\n",
      " 8\n",
      " 6\n",
      " 8\n",
      " 8\n",
      " 0\n",
      " 8\n",
      " 0\n",
      " 4\n",
      " 1\n",
      " 7\n",
      " 0\n",
      " 3\n",
      " 5\n",
      " 4\n",
      " 7\n",
      " 4\n",
      " 3\n",
      " 1\n",
      " 4\n",
      " 8\n",
      " 3\n",
      " 2\n",
      " 6\n",
      " 6\n",
      " 7\n",
      " 4\n",
      " 2\n",
      "[torch.cuda.FloatTensor of size 256 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Variable' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-5a1e68e99e0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmulti_coeff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_coeff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mrun_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlmbda_mae\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-1d854c7fac37>\u001b[0m in \u001b[0;36mrun_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                             \u001b[0miter_loc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter_loc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcross_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msingle_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmulti_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumOut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                             \u001b[0mlogname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logs_regression.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                             multi_coeff = m_coeff, single_coeff = m_coeff, KL = KL, algo = algo)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;34m'''Save the models'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mtezcan/New Volume/amazon/notebook/functions/fine_tune.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, lr_scheduler, dset_loaders, dset_sizes, writer, use_gpu, num_epochs, batch_size, num_log, init_lr, lr_decay_epoch, regression, learn_a, cross_loss, multi_loss, numOut, logname, iter_loc, multi_coeff, single_coeff, KL, algo)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fallthrough_methods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Variable' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "end_to_end = True\n",
    "optimizer='adam' #Optimizer function\n",
    "lr=0.05 #Initial learning rate\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "lr_scheduler=ft.exp_lr_scheduler #Learning rate scheduler\n",
    "lr_decay_epoch=10 #Number of epoch for learning rate decay\n",
    "\n",
    "\n",
    "hidden_sizes = [16, 16, 32, 32, 16, 16]#8, 16, 8, 4, 4]\n",
    "dropouts = []#.5, .5, .5]\n",
    "single_loss=1.0\n",
    "multi_loss =0.0\n",
    "\n",
    "KL = True\n",
    "metric = None\n",
    "\n",
    "for lmbda_mae in [.1*k for k in range(11)]:\n",
    "    multi_coeff = lmbda_mae * np.asarray(make_coeff(nclasses, 'ccr1', coeff_lmbda))\n",
    "    multi_coeff[int((len(multi_coeff)-1)/2)] = 1.\n",
    "    for k in range(10):\n",
    "        run_network()\n",
    "        \n",
    "for lmbda_mae in [.1*k for k in range(11)]:\n",
    "    multi_coeff = lmbda_mae * np.asarray(make_coeff(nclasses, 'mae', coeff_lmbda))\n",
    "    multi_coeff[int((len(multi_coeff)-1)/2)] = 1.\n",
    "    for k in range(10):\n",
    "        run_network()\n",
    "    \n",
    "'''KL = True\n",
    "metric = 'ccr'\n",
    "for k in range(10):\n",
    "    run_network()\n",
    "    \n",
    "metric = 'ccr1'\n",
    "for k in range(10):\n",
    "    run_network()\n",
    "    \n",
    "metric = 'mae'\n",
    "for k in range(10):\n",
    "    run_network()\n",
    "    \n",
    "metric = 'mse'\n",
    "for k in range(10):\n",
    "    run_network()'''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloader again, this time without shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fvec_norm = (fvec)/5\n",
    "mid_point = int(len(label)/2)#100*num_classes\n",
    "fvec_test = fvec_norm[rand_idx[:mid_point],:]\n",
    "fvec_train = fvec_norm[rand_idx[mid_point:],:]\n",
    "\n",
    "label_test = label[rand_idx[:mid_point]]\n",
    "label_train = label[rand_idx[mid_point:]]\n",
    "print(np.max(fvec_train))\n",
    "print(np.min(fvec_train))\n",
    "\n",
    "torch.from_numpy(label_train).type(torch.LongTensor)\n",
    "dsets={'train': torch.utils.data.TensorDataset(torch.from_numpy(fvec_train).type(torch.FloatTensor),\n",
    "                                               torch.from_numpy(label_train).type(torch.LongTensor)),\n",
    "       'val': torch.utils.data.TensorDataset(torch.from_numpy(fvec_test).type(torch.FloatTensor),\n",
    "                                             torch.from_numpy(label_test).type(torch.LongTensor))}\n",
    "\n",
    "'''Define dataset loaders'''\n",
    "dset_loaders = {'train':torch.utils.data.DataLoader(dsets['train'], batch_size=batch_size,shuffle=False,\n",
    "                                                    num_workers=12),\n",
    "                'val':torch.utils.data.DataLoader(dsets['val'], batch_size=batch_size,shuffle=False,\n",
    "                                                    num_workers=12)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_shape = 'Spiral'\n",
    "data_date = '18_01_31'\n",
    "data_dir = './saved_models_github/' + data_shape + '/' + data_date\n",
    "\n",
    "run_dirs = sorted(os.listdir(data_dir))\n",
    "last_dirs = run_dirs[1::2]\n",
    "ccr1_dirs = last_dirs[:110]\n",
    "mae_dirs = last_dirs[110:]\n",
    "all_dirs = [ccr1_dirs, mae_dirs]\n",
    "\n",
    "\n",
    "'''run_dirs = sorted(os.listdir('./saved_models/test'))\n",
    "last_dirs = run_dirs[1::2]\n",
    "ccr1_dirs = last_dirs[:100]\n",
    "mae_dirs = last_dirs[100:]\n",
    "\n",
    "pure_ccr1 = ['Ordinal_January24  14:10:01_last',\n",
    "             'Ordinal_January24  14:11:08_last',\n",
    "             'Ordinal_January24  14:12:14_last',\n",
    "             'Ordinal_January24  14:13:21_last',\n",
    "             'Ordinal_January24  14:14:27_last',\n",
    "             'Ordinal_January24  14:15:34_last',\n",
    "             'Ordinal_January24  14:16:40_last',\n",
    "             'Ordinal_January24  14:17:47_last',\n",
    "             'Ordinal_January24  14:18:54_last',\n",
    "             'Ordinal_January24  14:20:00_last',]\n",
    "\n",
    "pure_mae = ['Ordinal_January24  14:21:07_last',\n",
    "             'Ordinal_January24  14:22:13_last',\n",
    "             'Ordinal_January24  14:23:19_last',\n",
    "             'Ordinal_January24  14:24:26_last',\n",
    "             'Ordinal_January24  14:25:32_last',\n",
    "             'Ordinal_January24  14:26:38_last',\n",
    "             'Ordinal_January24  14:27:45_last',\n",
    "             'Ordinal_January24  14:28:52_last',\n",
    "             'Ordinal_January24  14:29:58_last',\n",
    "             'Ordinal_January24  14:31:05_last',]'''\n",
    "                \n",
    "len(mae_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.load('./saved_models/ord/Ordinal_January24  10:24:20_last', map_location={'cuda:0': 'cpu'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(model_dir, phase='train'):\n",
    "    if use_gpu:\n",
    "        model = torch.load(model_dir)\n",
    "    else:\n",
    "        model = torch.load(model_dir, map_location={'cuda:0': 'cpu'})\n",
    "    model.train(False)\n",
    "\n",
    "    labels_arr = np.asarray([]);\n",
    "    preds_arr = np.asarray([]);\n",
    "    for data in dset_loaders[phase]:\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            outputs = np.argmax(model(inputs).cpu().data.numpy(), axis=1)\n",
    "            #labels_arr = np.append(labels_arr,labels.cpu().data.numpy())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = np.argmax(model(inputs).data.numpy(), axis=1)\n",
    "            #labels_arr = np.append(labels_arr,labels.data.numpy())\n",
    "          \n",
    "        preds_arr = np.append(preds_arr, outputs)\n",
    "    return preds_arr\n",
    "#pred_tr = validate('./saved_models/ord/Ordinal_January24  10:24:20_last')\n",
    "#print(np.min(label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(18,15))\n",
    "\n",
    "for k in range(5):\n",
    "    plt.subplot(3,5,k+1)\n",
    "    pred_tr = validate(data_dir + '/' + last_dirs[k])\n",
    "    plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "    #plt.scatter(fvec_train[:, 0], fvec_train[:, 1], c=label_train,vmin=-1, vmax=num_classes)\n",
    "    plt.colorbar()\n",
    "    ccr = np.mean(pred_tr==label_train)\n",
    "    ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "    mae = np.mean(np.abs(pred_tr-label_train))\n",
    "    rmse = np.mean((pred_tr-label_train)**2)\n",
    "    plt.title('$CCR$ loss' + \n",
    "              ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "              ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)))\n",
    "    \n",
    "for k in range(5):\n",
    "    plt.subplot(3,5,k+6)\n",
    "    pred_tr = validate(data_dir + '/' + last_dirs[50+k])\n",
    "    plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "    #plt.scatter(fvec_train[:, 0], fvec_train[:, 1], c=label_train,vmin=-1, vmax=num_classes)\n",
    "    plt.colorbar()\n",
    "    ccr = np.mean(pred_tr==label_train)\n",
    "    ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "    mae = np.mean(np.abs(pred_tr-label_train))\n",
    "    rmse = np.mean((pred_tr-label_train)**2)\n",
    "    plt.title('$0.5CCR$ loss + $0.5CCR_1$ loss \\n' + \n",
    "              ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "              ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)))\n",
    "    \n",
    "for k in range(5):\n",
    "    plt.subplot(3,5,k+11)\n",
    "    pred_tr = validate(data_dir + '/' + last_dirs[100+k])\n",
    "    plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "    #plt.scatter(fvec_train[:, 0], fvec_train[:, 1], c=label_train,vmin=-1, vmax=num_classes)\n",
    "    plt.colorbar()\n",
    "    ccr = np.mean(pred_tr==label_train)\n",
    "    ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "    mae = np.mean(np.abs(pred_tr-label_train))\n",
    "    rmse = np.mean((pred_tr-label_train)**2)\n",
    "    plt.title('$CCR_1$ loss ' + \n",
    "              ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "              ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)))\n",
    "    \n",
    "plt.savefig('variance_of_results.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_and_mean(root_dir, sub_dirs, phase='train'):\n",
    "    scores_arr = np.zeros((label_train.shape[0],9))\n",
    "    for sub_dir in sub_dirs:\n",
    "        if use_gpu:\n",
    "            model = torch.load(root_dir + '/' + sub_dir)\n",
    "        else:\n",
    "            model = torch.load(root_dir + '/' + sub_dir, map_location={'cuda:0': 'cpu'})\n",
    "        model.train(False)\n",
    "        #print(model)\n",
    "        score_arr = np.zeros((1,9))\n",
    "        for data in dset_loaders[phase]:\n",
    "            inputs, labels = data\n",
    "            if use_gpu:\n",
    "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                outputs = model(inputs).cpu().data.numpy()\n",
    "                #print(outputs.shape)\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "                outputs = model(inputs).data.numpy()\n",
    "                \n",
    "            score_arr = np.append(score_arr, outputs, axis=0)\n",
    "        scores_arr += score_arr[1:,:]\n",
    "        \n",
    "    return scores_arr\n",
    "#scores_tr = validate_and_mean('./saved_models/test_circular', last_dirs[:10])\n",
    "#pred_tr = np.argmax(scores_tr, axis=1)\n",
    "#print(np.mean(label_train == pred_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dirs = ['./saved_models/ord/Ordinal_January24  10:16:00_last',\n",
    "             './saved_models/ord/Ordinal_January24  10:20:35_last',\n",
    "             './saved_models/ord/Ordinal_January24  10:24:20_last',\n",
    "             './saved_models/ord/Ordinal_January24  10:28:03_last']\n",
    "\n",
    "'''model_dirs = ['./saved_models/ord/Ordinal_January24  13:09:17_last',\n",
    "             './saved_models/ord/Ordinal_January24  13:18:08_last',\n",
    "             './saved_models/ord/Ordinal_January24  13:27:43_last',\n",
    "             './saved_models/ord/Ordinal_January24  13:43:43_last']'''\n",
    "\n",
    "model_dirs = ['./saved_models/ord/Ordinal_January26  00:22:14_last',\n",
    "              './saved_models/ord/Ordinal_January26  00:19:20_last',\n",
    "              './saved_models/ord/Ordinal_January26  00:09:08_last',\n",
    "              './saved_models/ord/Ordinal_January26  00:17:53_last']\n",
    "preds = []\n",
    "\n",
    "for model_dir in model_dirs:\n",
    "    preds.append(validate(model_dir))\n",
    "    \n",
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric = 'CCR1'\n",
    "\n",
    "if metric is 'CCR1':\n",
    "    metric_code = 0\n",
    "elif metric is 'MAE':\n",
    "    metric_code = 1\n",
    "else:\n",
    "    print('Wrong metric')\n",
    "    \n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "plt.subplot(4,3,1)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=label_train, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "plt.title('Ground Truth')\n",
    "\n",
    "metrics = np.zeros((11, 4))\n",
    "for k in range(10):\n",
    "    scores_tr = validate_and_mean(data_dir, all_dirs[metric_code][k*10:(k+1)*10])\n",
    "    pred_tr = np.argmax(scores_tr, axis=1)\n",
    "    plt.subplot(4,3,k+2)\n",
    "    plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "    plt.colorbar()\n",
    "    ccr = np.mean(pred_tr==label_train)\n",
    "    ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "    mae = np.mean(np.abs(pred_tr-label_train))\n",
    "    rmse = np.mean((pred_tr-label_train)**2)\n",
    "    metrics[k,:] = [ccr,ccr1,mae,rmse]\n",
    "    plt.title('$\\lambda$=' + str(np.round(k*.1, decimals=1)) + \n",
    "              ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "              ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)) +\n",
    "              ', $MAE$=' + str(np.round(mae, decimals = 2)) +\n",
    "              ', $RMSE$=' + str(np.round(rmse, decimals = 2)))\n",
    "\n",
    "    \n",
    "#pred_tr = validate('./saved_models/ord/Ordinal_January24  10:20:35_last')\n",
    "#pred_tr = validate('./saved_models/ord/Ordinal_January24  13:18:08_last')\n",
    "\n",
    "scores_tr = validate_and_mean(data_dir, all_dirs[metric_code][100:])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(4,3,12)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "ccr = np.mean(pred_tr==label_train)\n",
    "ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "mae = np.mean(np.abs(pred_tr-label_train))\n",
    "rmse = np.mean((pred_tr-label_train)**2)\n",
    "metrics[10,:] = [ccr,ccr1,mae,rmse]\n",
    "plt.title('$\\lambda$=1.0' + \n",
    "          ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "          ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)) +\n",
    "          ', $MAE$=' + str(np.round(mae, decimals = 2)) +\n",
    "          ', $RMSE$=' + str(np.round(rmse, decimals = 2)))\n",
    "\n",
    "plt_title = data_shape + '_Data_CCR_' + metric + '_tradeoff_' + data_date + '.tiff'\n",
    "plt.savefig(plt_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(metrics)\n",
    "\n",
    "lmbdas = [.1*k for k in range(11)]\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(221)\n",
    "plt.plot(lmbdas, metrics[:,0], 'o-')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('$CCR$')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(lmbdas, metrics[:,1], 'o-')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('$CCR_1$')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(lmbdas, metrics[:,2], 'o-')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('$MAE$')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(lmbdas, metrics[:,3], 'o-')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('$RMSE$')\n",
    "\n",
    "plt.savefig('spiral_plots_ccr1.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=label_train, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "plt.title('Ground Truth')\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/test_circular', ccr1_dirs[:10])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,2)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "ccr = np.mean(pred_tr==label_train)\n",
    "ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "mae = np.mean(np.abs(pred_tr-label_train))\n",
    "rmse = np.mean((pred_tr-label_train)**2)\n",
    "plt.title('$CCR loss$'  + \n",
    "          ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "          ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)) +\n",
    "          ',\\n $MAE$=' + str(np.round(mae, decimals = 2)) +\n",
    "          ', $RMSE$=' + str(np.round(rmse, decimals = 2)))\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/test_circular', ccr1_dirs[100:])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,3)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "ccr = np.mean(pred_tr==label_train)\n",
    "ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "mae = np.mean(np.abs(pred_tr-label_train))\n",
    "rmse = np.mean((pred_tr-label_train)**2)\n",
    "plt.title('$CCR_1 loss$'  + \n",
    "          ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "          ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)) +\n",
    "          ',\\n $MAE$=' + str(np.round(mae, decimals = 2)) +\n",
    "          ', $RMSE$=' + str(np.round(rmse, decimals = 2)))\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/ord', mae_dirs[100:])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,4)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "ccr = np.mean(pred_tr==label_train)\n",
    "ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "mae = np.mean(np.abs(pred_tr-label_train))\n",
    "rmse = np.mean((pred_tr-label_train)**2)\n",
    "plt.title('$MAE loss$'  + \n",
    "          ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "          ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)) +\n",
    "          ',\\n$MAE$=' + str(np.round(mae, decimals = 2)) +\n",
    "          ', $RMSE$=' + str(np.round(rmse, decimals = 2)))\n",
    "\n",
    "plt.savefig('spiral_extreme.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=label_train, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "plt.title('Ground Truth')\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/test_circular', ccr1_dirs[:10])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,2)\n",
    "plt.hist(pred_tr-label_train, bins = np.arange(-4.5,5.5,1))\n",
    "plt.title('$CCR$ loss')\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/test_circular', ccr1_dirs[100:])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,3)\n",
    "plt.hist(pred_tr-label_train, bins = np.arange(-4.5,5.5,1))\n",
    "plt.title('$CCR_1$ loss')\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/test_circular', mae_dirs[100:])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,4)\n",
    "plt.hist(pred_tr-label_train, bins = np.arange(-4.5,5.5,1))\n",
    "plt.title('$MAE$ loss')\n",
    "\n",
    "plt.savefig('circular_extreme_hist.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.subplot(211)\n",
    "img = mpimg.imread('Circular_Data_Extreme_Weights.eps')\n",
    "plt.imshow(img)\n",
    "plt.subplot(212)\n",
    "img = mpimg.imread('Spiral_Data_Extreme_Weights.eps')\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
