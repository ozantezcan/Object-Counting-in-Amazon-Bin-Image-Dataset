{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchsample\n",
    "from torchsample import transforms as ts_transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "import importlib\n",
    "\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "num_classes=5\n",
    "\n",
    "\n",
    "#from torchsample.transforms import RangeNorm\n",
    "\n",
    "import functions.fine_tune as ft\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f0a2e905b00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAD8CAYAAAAylrwMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGAjYAQMGDMiIgYAdMGDAgIz43zi9YYgE0yNZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0aa41cf5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_classes = 10\n",
    "dim = 2\n",
    "\n",
    "def generateLinearData(num_samples = 10000, num_classes = num_classes, dim = dim, bound = 5, sigma_noise = .1,rand_label = False):\n",
    "    \n",
    "    if  rand_label:\n",
    "        rand_classes = np.random.permutation(num_classes)\n",
    "    else:\n",
    "        rand_classes = np.arange(num_classes)\n",
    "        \n",
    "    fvec = np.random.rand(dim, num_samples)*bound*2-bound\n",
    "    label = np.dot((np.random.rand(1,dim)*bound*2-bound).reshape(1,-1),fvec)\n",
    "\n",
    "    sorted_idx = np.argsort(label)\n",
    "    bin_size = label.shape[1]/num_classes\n",
    "\n",
    "    for k in range(0, num_classes):\n",
    "        label[0, sorted_idx[0, np.floor(k*bin_size).astype(int):np.floor((k+1)*bin_size).astype(int)]] = rand_classes[k]\n",
    "\n",
    "    label = label.astype(np.int)\n",
    "    n = sigma_noise * np.random.randn(dim, num_samples)\n",
    "    fvec = fvec + n\n",
    "    \n",
    "    return fvec.T, label.reshape(label.shape[1])\n",
    "\n",
    "fvec, label = generateLinearData(rand_label = False)\n",
    "plt.scatter(fvec[:, 0], fvec[:, 1], c=label)\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateCircularData(num_samples = 10000, num_classes = num_classes, dim = dim,\n",
    "                         bound = 5, sigma_noise = .1, rand_label = False):\n",
    "    \n",
    "    if  rand_label:\n",
    "        rand_classes = np.random.permutation(num_classes)\n",
    "    else:\n",
    "        rand_classes = np.arange(num_classes)\n",
    "        \n",
    "    fvec = np.random.rand(dim, num_samples)*bound*2-bound\n",
    "    \n",
    "    fvec_l = np.sum(fvec**2, axis = 0).reshape(1,-1)\n",
    "    print(fvec_l.shape)\n",
    "    label = fvec_l\n",
    "\n",
    "    sorted_idx = np.argsort(label)\n",
    "    bin_size = label.shape[1]/num_classes\n",
    "\n",
    "    for k in range(0, num_classes):\n",
    "        label[0, sorted_idx[0, np.floor(k*bin_size).astype(int):np.floor((k+1)*bin_size).astype(int)]] = rand_classes[k]\n",
    "\n",
    "    label = label.astype(np.int)\n",
    "    n = sigma_noise * np.random.randn(dim, num_samples)\n",
    "    fvec = fvec + n\n",
    "    \n",
    "    return fvec.T, label.reshape(label.shape[1])\n",
    "\n",
    "fvec, label = generateCircularData(num_classes =5, sigma_noise = 0, rand_label = False)\n",
    "plt.scatter(fvec[:, 0], fvec[:, 1], c=label, cmap = plt.get_cmap('Greys'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generateSpiralData(num_samples = 10000, num_classes = 9, dim = 2,\n",
    "                         bound = 1, sigma_noise = .1, rand_label = False):\n",
    "    \n",
    "    if  rand_label:\n",
    "        rand_classes = np.random.permutation(num_classes)\n",
    "    else:\n",
    "        rand_classes = np.arange(num_classes)\n",
    "    \n",
    "    #rand_classes = [1, 1.5, -1, -1.5]\n",
    "    sample_per_class = int(num_samples/num_classes)\n",
    "    num_samples = sample_per_class*num_classes\n",
    "    fvec = np.zeros((dim, sample_per_class*num_classes))\n",
    "    label = np.zeros((1, sample_per_class*num_classes))\n",
    "    \n",
    "    t = np.linspace(0, 10, sample_per_class)\n",
    "    x = t * np.cos(t)\n",
    "    y = t * np.sin(t)\n",
    "    x = x.reshape(1, -1)\n",
    "    y = y.reshape(1, -1)\n",
    "\n",
    "    cons = .7\n",
    "    for k in range(0, num_classes):\n",
    "        r = np.linspace(0.05, 1, sample_per_class)\n",
    "        t = np.linspace(k*cons, (k+6)*cons, sample_per_class)\n",
    "        x = np.cos(t)\n",
    "        y = np.sin(t)\n",
    "        x = x.reshape(1, -1)\n",
    "        y = y.reshape(1, -1)\n",
    "        label[0, k*sample_per_class:(k+1)*sample_per_class] = rand_classes[k]\n",
    "        fvec[0, k*sample_per_class:(k+1)*sample_per_class] = bound * x * r\n",
    "        fvec[1, k*sample_per_class:(k+1)*sample_per_class] = bound * y * r\n",
    "\n",
    "    label = label.astype(np.int)\n",
    "    n = sigma_noise * np.random.randn(dim, num_samples)\n",
    "    fvec = fvec + n\n",
    "    \n",
    "    return fvec.T, label.reshape(label.shape[1])\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "X, y = generateSpiralData(num_classes = 9, sigma_noise = 0.01, rand_label = False)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap = plt.get_cmap('Set1'))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generateSpiralData(num_samples = 10000, num_classes = num_classes, dim = dim,\n",
    "                         bound = 5, sigma_noise = .1, rand_label = False):\n",
    "    \n",
    "    if  rand_label:\n",
    "        rand_classes = np.random.permutation(num_classes)\n",
    "    else:\n",
    "        rand_classes = np.arange(num_classes)\n",
    "    \n",
    "    #rand_classes = [1, 1.5, -1, -1.5]\n",
    "\n",
    "    cons = 4\n",
    "    N = num_samples # number of points per class\n",
    "    D = dim # dimensionality\n",
    "    K = num_classes # number of classes\n",
    "\n",
    "    X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "    y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "    for j in range(K):\n",
    "      ix = range(N*j,N*(j+1))\n",
    "      r = np.linspace(0.0,1,N) # radius\n",
    "      t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*sigma_noise # theta\n",
    "      X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "      y[ix] = j\n",
    "    \n",
    "    label = y.astype(np.int)\n",
    "    fvec = X\n",
    "    \n",
    "    return fvec, label\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.subplot(121)\n",
    "fvec, label = generateCircularData(num_classes =9, sigma_noise = 0, rand_label = False)\n",
    "plt.scatter(fvec[:, 0], fvec[:, 1], c=label, cmap = plt.get_cmap('Set1'))\n",
    "plt.colorbar()\n",
    "plt.subplot(122)\n",
    "fvec, label = generateSpiralData(num_classes = 3, sigma_noise = 0, rand_label = False)\n",
    "plt.scatter(fvec[:, 0], fvec[:, 1], c=label, cmap = plt.get_cmap('Set1'))\n",
    "plt.colorbar()\n",
    "#plt.savefig('circular_vs_spiral.tiff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinal Regression Benchmark Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "num_bins=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank32nh.data\n",
      "bank8FM.data\n",
      "bostonhousing\n",
      "cal_housing.data\n",
      "cpu_act.data\n",
      "cpu_small.data\n",
      "house_16H.data\n",
      "house_8L.data\n",
      "housing\n",
      "results.csv\n",
      "stock\n",
      "stocksdomain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./dataset/regression\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 0)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-35211db07a8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"feat\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./dataset/regression/housing\", sep=',', header=None)\n",
    "train_df=train_df.drop(train_df.columns[-1],axis=1)\n",
    "print(train_df.shape)\n",
    "\n",
    "columns=[\"feat\"+str(k) for k in range(train_df.shape[1])]\n",
    "columns[-1]=\"label\"\n",
    "train_df.columns=columns\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22784, 17)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./dataset/regression/house_16H.data\", sep=',', header=None)\n",
    "#train_df=train_df.drop(train_df.columns[-1],axis=1)\n",
    "\n",
    "columns=[\"feat\"+str(k) for k in range(train_df.shape[1])]\n",
    "columns[-1]=\"label\"\n",
    "train_df.columns=columns\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat0</th>\n",
       "      <th>feat1</th>\n",
       "      <th>feat2</th>\n",
       "      <th>feat3</th>\n",
       "      <th>feat4</th>\n",
       "      <th>feat5</th>\n",
       "      <th>feat6</th>\n",
       "      <th>feat7</th>\n",
       "      <th>feat8</th>\n",
       "      <th>feat9</th>\n",
       "      <th>feat10</th>\n",
       "      <th>feat11</th>\n",
       "      <th>feat12</th>\n",
       "      <th>feat13</th>\n",
       "      <th>feat14</th>\n",
       "      <th>feat15</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15512.0</td>\n",
       "      <td>0.460869</td>\n",
       "      <td>0.049252</td>\n",
       "      <td>0.226470</td>\n",
       "      <td>0.149827</td>\n",
       "      <td>0.752837</td>\n",
       "      <td>0.010057</td>\n",
       "      <td>0.579729</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.075912</td>\n",
       "      <td>0.625318</td>\n",
       "      <td>0.036613</td>\n",
       "      <td>0.991377</td>\n",
       "      <td>0.260116</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>0.774059</td>\n",
       "      <td>130600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1550.0</td>\n",
       "      <td>0.470968</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>0.137419</td>\n",
       "      <td>0.096341</td>\n",
       "      <td>0.862581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.695142</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.043551</td>\n",
       "      <td>0.064263</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.994975</td>\n",
       "      <td>0.285266</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>40500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4741.0</td>\n",
       "      <td>0.485341</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.189412</td>\n",
       "      <td>0.135656</td>\n",
       "      <td>0.856992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683584</td>\n",
       "      <td>0.004143</td>\n",
       "      <td>0.027965</td>\n",
       "      <td>0.065796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997411</td>\n",
       "      <td>0.315433</td>\n",
       "      <td>0.065116</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>28700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>467.0</td>\n",
       "      <td>0.498929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100642</td>\n",
       "      <td>0.085470</td>\n",
       "      <td>0.907923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.018293</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>310.0</td>\n",
       "      <td>0.474194</td>\n",
       "      <td>0.680645</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.128834</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.756302</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.077519</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.147287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat0     feat1     feat2     feat3     feat4     feat5     feat6  \\\n",
       "0  15512.0  0.460869  0.049252  0.226470  0.149827  0.752837  0.010057   \n",
       "1   1550.0  0.470968  0.002581  0.137419  0.096341  0.862581  0.000000   \n",
       "2   4741.0  0.485341  0.000211  0.189412  0.135656  0.856992  0.000000   \n",
       "3    467.0  0.498929  0.000000  0.100642  0.085470  0.907923  0.000000   \n",
       "4    310.0  0.474194  0.680645  0.225806  0.128834  0.896774  0.000000   \n",
       "\n",
       "      feat7     feat8     feat9    feat10    feat11    feat12    feat13  \\\n",
       "0  0.579729  0.003251  0.075912  0.625318  0.036613  0.991377  0.260116   \n",
       "1  0.695142  0.005025  0.043551  0.064263  0.003350  0.994975  0.285266   \n",
       "2  0.683584  0.004143  0.027965  0.065796  0.000000  0.997411  0.315433   \n",
       "3  0.780488  0.006098  0.018293  0.057471  0.000000  1.000000  0.149425   \n",
       "4  0.756302  0.008403  0.016807  0.077519  0.672269  0.991597  0.147287   \n",
       "\n",
       "     feat14    feat15     label  \n",
       "0  0.052246  0.774059  130600.0  \n",
       "1  0.060606  0.142857   40500.0  \n",
       "2  0.065116  0.687500   28700.0  \n",
       "3  0.139535  1.000000   28500.0  \n",
       "4  0.000000  0.000000   24100.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples per class is 2278.4\n",
      "[0.0, 14999.000099999999, 18400.000199999999, 23400.0003, 28100.000400000001, 33200.000500000002, 39800.000599999999, 49600.000699999997, 65300.000800000002, 102600.0009, 500001.00099999999]\n",
      "[0.0, 14999.000099999999, 18400.000199999999, 23400.0003, 28100.000400000001, 33200.000500000002, 39800.000599999999, 49600.000699999997, 65300.000800000002, 102600.0009, 500002.00099999999]\n",
      "500001.0\n",
      "Unique labels are [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEFCAYAAAAG45eHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a189a94a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_df['label_ord']=train_df['label']\n",
    "label=train_df.label.values\n",
    "sorted_idx=np.argsort(train_df.label.values)\n",
    "num_samples_per_class=train_df.shape[0]/num_bins\n",
    "print('Number of Samples per class is ' + str(num_samples_per_class))\n",
    "bins=[(k*1e-4+label[sorted_idx[np.round(k*num_samples_per_class-1).astype(np.int)]]) for k in range(1,num_bins+1)]\n",
    "bins.insert(0,0.0)\n",
    "print(bins)\n",
    "bins[-1]=bins[-1]+1\n",
    "print(bins)\n",
    "\n",
    "label_ord=label.copy()\n",
    "k = 10\n",
    "\n",
    "print(label[sorted_idx[np.round(k*num_samples_per_class-1).astype(np.int)]])\n",
    "for k in range(num_bins):\n",
    "    #print(np.all([label>=bins[k], label<bins[k+1]],0))\n",
    "    label_ord[np.all([label>=bins[k], label<bins[k+1]],0)]=k\n",
    "    \n",
    "print('Unique labels are ' + str(np.unique(label_ord)))\n",
    "\n",
    "\n",
    "train_df['label_ord']=label_ord\n",
    "#print(train_df.head())\n",
    "\n",
    "plt.scatter(label,label_ord)\n",
    "plt.xlabel('Original Label')\n",
    "plt.ylabel('Ordinal Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat0</th>\n",
       "      <th>feat1</th>\n",
       "      <th>feat2</th>\n",
       "      <th>feat3</th>\n",
       "      <th>feat4</th>\n",
       "      <th>feat5</th>\n",
       "      <th>feat6</th>\n",
       "      <th>feat7</th>\n",
       "      <th>feat8</th>\n",
       "      <th>feat9</th>\n",
       "      <th>feat10</th>\n",
       "      <th>feat11</th>\n",
       "      <th>feat12</th>\n",
       "      <th>feat13</th>\n",
       "      <th>feat14</th>\n",
       "      <th>feat15</th>\n",
       "      <th>label</th>\n",
       "      <th>label_ord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15512.0</td>\n",
       "      <td>0.460869</td>\n",
       "      <td>0.049252</td>\n",
       "      <td>0.226470</td>\n",
       "      <td>0.149827</td>\n",
       "      <td>0.752837</td>\n",
       "      <td>0.010057</td>\n",
       "      <td>0.579729</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.075912</td>\n",
       "      <td>0.625318</td>\n",
       "      <td>0.036613</td>\n",
       "      <td>0.991377</td>\n",
       "      <td>0.260116</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>0.774059</td>\n",
       "      <td>130600.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1550.0</td>\n",
       "      <td>0.470968</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>0.137419</td>\n",
       "      <td>0.096341</td>\n",
       "      <td>0.862581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.695142</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.043551</td>\n",
       "      <td>0.064263</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.994975</td>\n",
       "      <td>0.285266</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>40500.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4741.0</td>\n",
       "      <td>0.485341</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.189412</td>\n",
       "      <td>0.135656</td>\n",
       "      <td>0.856992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683584</td>\n",
       "      <td>0.004143</td>\n",
       "      <td>0.027965</td>\n",
       "      <td>0.065796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997411</td>\n",
       "      <td>0.315433</td>\n",
       "      <td>0.065116</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>28700.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>467.0</td>\n",
       "      <td>0.498929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100642</td>\n",
       "      <td>0.085470</td>\n",
       "      <td>0.907923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.018293</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28500.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>310.0</td>\n",
       "      <td>0.474194</td>\n",
       "      <td>0.680645</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.128834</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.756302</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.077519</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.147287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24100.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat0     feat1     feat2     feat3     feat4     feat5     feat6  \\\n",
       "0  15512.0  0.460869  0.049252  0.226470  0.149827  0.752837  0.010057   \n",
       "1   1550.0  0.470968  0.002581  0.137419  0.096341  0.862581  0.000000   \n",
       "2   4741.0  0.485341  0.000211  0.189412  0.135656  0.856992  0.000000   \n",
       "3    467.0  0.498929  0.000000  0.100642  0.085470  0.907923  0.000000   \n",
       "4    310.0  0.474194  0.680645  0.225806  0.128834  0.896774  0.000000   \n",
       "\n",
       "      feat7     feat8     feat9    feat10    feat11    feat12    feat13  \\\n",
       "0  0.579729  0.003251  0.075912  0.625318  0.036613  0.991377  0.260116   \n",
       "1  0.695142  0.005025  0.043551  0.064263  0.003350  0.994975  0.285266   \n",
       "2  0.683584  0.004143  0.027965  0.065796  0.000000  0.997411  0.315433   \n",
       "3  0.780488  0.006098  0.018293  0.057471  0.000000  1.000000  0.149425   \n",
       "4  0.756302  0.008403  0.016807  0.077519  0.672269  0.991597  0.147287   \n",
       "\n",
       "     feat14    feat15     label  label_ord  \n",
       "0  0.052246  0.774059  130600.0        9.0  \n",
       "1  0.060606  0.142857   40500.0        6.0  \n",
       "2  0.065116  0.687500   28700.0        4.0  \n",
       "3  0.139535  1.000000   28500.0        4.0  \n",
       "4  0.000000  0.000000   24100.0        3.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x109b30eb8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd0AAAF2CAYAAAAr5kJDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYpFV99/93d89Mz0LPMECz48LiV1RQAgI+ImLEIGhi\nXGIMj4oSERSD/h5+LihGvZT4aNRE44ISEVQ0RlCTYBQMoiwKGJQ4RPgqIAEXsBlmH2bp7nr+uGuG\nZuyqrqru2t+v65pr7rrr9F3fOo58+tzLOQOlUglJktR8g+0uQJKkfmHoSpLUIoauJEktYuhKktQi\nhq4kSS1i6EqS1CLz2l2AJEndKiKGgc8B+wNrgTMz8xeV2jvSlSSpcacB6zPzaOCvgI9Xa2zoSpLU\nuCcA3wLIzAQOrtbY0JUkqXG3AM+PiIGIOBrYJyKGKjXu22u6Y2PrZjX/5fLli1m1auNcldNT7Jvq\n7J/K7JvKer1vRkdHBppx3D2vvmXWcx3f96ynVKvtQorR7bXA9cDNmTlRqbEj3QbNm1fxF5m+Z99U\nZ/9UZt9UZt90rKcCV2XmMcBXgbuqNe7bka4kSXPgF8B7I+IdwGrgL6s1NnQlSWpQZj4AHF9re08v\nS5LUIoauJEktYuhKktQihq4kSS1i6EqS1CKGriRJLWLoSpLUIk19Tjcifkyx1BHAL4HzgIuAEnAr\nxRJIkxFxGnA6MA68LzMvj4hFwBeB3YF1wCmZOVae2/Kj5bZXZuZ7yp/1LuB55f1vysybmvndJEmq\nV9NCNyIWAgOZedyUff8KnJuZ34uI84EXRMQPgbOAI4CFwHUR8R3gdcCKzHx3RLwMOBd4I3A+8GKK\nqba+GRGHAQPAM4GjgP2Ayyim5pIkqWM0c6T7ZGBxRFxZ/py3A4cD3y+//y3gj4AJ4PrM3Axsjog7\ngEOBY4APTmn7zohYCgxn5p0AEXEFxUwgmylGvSXgnoiYFxGjmTnWxO8nSX3vkFvv3L694kkHtLGS\n7tDM0N0IfAj4R+AgiuAcKAcjFKeMlwFLgTVTfm66/VP3rd2h7f7AJmDlNMeoGLrLly+e9QTio6Mj\ns/r5XmbfVGf/VGbfVNbpfdPp9XWCZobuz4E7yiH784hYSTHS3WaEYnLoteXtavtnarulwv6KZrtE\n1ujoCGNj62Z1jF5l31Rn/1Rm31TWDX0zm/r6JbCbeffyqcCHASJib4pR6pURcVz5/RMp1h+8CXhG\nRCyMiGUU6xLeSrEu4UlT22bmWmBLRBwQEQPACTy8huEJETEYEY8CBsuTUEuSmujHT9yfHx78WE8t\n16iZI93PAhdFxHUUdyufCjwAXBARC4DbgEszcyIiPkYRnoPAOzJzU0R8Cri4/PNbgJPLxz0DuAQY\noriOeyNARFwL/LB8jDOb+L0kSWVv+p/7uH3TZv5p/30YXTC/3eV0vIFSqTRzqx40NrZuVl+8G071\ntIt9U539U5l9U1mn9s3xt9/NA+MTfOOg/XjM8IKGjzM6OjIwh2Vtt+fVt8w65O571lPmrDbX05Uk\nNez4ZUu4c9NWdp9vnNTCXpIkNexte422u4Su4jSQkiS1iKErSVKLGLqSJLWIoStJatjPN23mmrUb\n2l1G1zB0JUkNe++vx3jzr+7n/q3j7S6lKxi6kqRZGRkcZKfBpjxm23MMXUlSw+YPDDBZgq39Oc9S\n3XxOV5LUsH949F48VJpk51mu2tYvDF1JUsOWDA2yxJOmNbOnJEkNu2btBr66cs3MDQU40pUkzcKH\n7lvJb7du5VnLlrDbPCNlJvaQJKlh92/dypYSDPTpinURMR+4GHgMMAGclpm3V2pv6EqSGjZBsWD6\nIJ35yNAlpRfPwVHurPbmScC8zPxfEfEc4Dyg4ocaupKkhh00PJ8145MsHurbW4R+DsyLiEFgKbC1\nWmNDV5LUsHP22p2NkxMMD/Zt6K6nOLV8O7Ab8Pxqjfu2lyRJs/fu3/yOd/56jE2Tk+0upV3+P+CK\nzHwc8GTg4ohYWKmxI11JUsOOG1nCuolJhgc685puC6zi4VPKDwLzgYozhRi6kqSGnbXnru0uod3+\nDrgwIq4FFgBvz8yKyy4ZupIkNSgz1wMvrbW913QlSWoRQ1eSpBYxdCVJDfv0/Q/ykftWUurTGanq\nZehKkhp24QOrueSB1WydNHRrYehKkhpSKpWYKJWKaSD79omh+nj3siSpYQcvGmaiVGKof5/TrYuh\nK0lqyMDAAJ/ff5/t25qZoStJaphhWx+v6UqSGvb+34zxf38z1u4yuoYjXUlSQ0qlEteu28jgwACl\nUslRbw0MXUlSQwYGBjhiyULmDQwYuDXy9LIkqSFbJyb4t9Xr+ZdV69pdStdwpCtJasiGUonxdhfR\nZQxdSVJDFg8OMTo4wJIhT5rWyp6SJDVkgBJbSiUmnAGyZo50JUkNuWvzFtaUYO34RLtL6RqGriSp\nIbFoIfvNG2Tf+fPbXUrXMHQlSQ25/aHN3Ds+ydjElnaX0jW8pitJashuQ4PMB0YGjZJa2VOSpIZs\nnCwxMABDzotRM0NXktSQkXmDjAwOse8Cr1TWytCVJDVk1fgEayYmmMShbq389USS1JDHDi/g9NHl\nHLJoQbtL6RqGriSpITeu38gnxlYxMjDAD564U7vL6QqeXpYkNeTq1esBWFdySqpaOdKVJDXkWcuW\n8OU169m9gx8ZuvaaV8z6GM/+wzkopKxze0qS1NHGBwYYAA5esrDdpXQNQ1eS1JC95g2x+7whdhsa\nancpXcPQlSQ1ZN1kiXWTkzw44YIHtfKariSpIb/ZOs6GyRL08Y1UEfEq4FXllwuBpwB7Zubq6do3\nNXQjYnfgZuA5wDhwEVACbgXOzMzJiDgNOL38/vsy8/KIWAR8EdgdWAeckpljEXE08NFy2ysz8z3l\nz3kX8Lzy/jdl5k3N/F6SJDhk0TB7zhtin/n9O37LzIsoso2I+ARwYaXAhSaeXo6I+cCngYfKuz4C\nnJuZzwAGgBdExJ7AWcDTgROA90fEMPA6YEW57eeBc8vHOB84GTgGOCoiDouIPwCeCRwFvAz4RLO+\nkyTpYZc+uJb7xie4av3GdpfSdhFxBPDEzPxMtXbNvKb7IYqQ/E359eHA98vb3wKOB44Ers/MzZm5\nBrgDOJQiVL89tW1ELAWGM/POzCwBV5SPcQzFqLeUmfcA8yJitInfS5IEvHD5CI8bXsBLly9tdymd\n4O3Ae2Zq1JTQLZ/jHsvMK6bsHiiHJRSnjJcBS4E1U9pMt3/qvrUztJ26X5LURI8eXsDr91jOS3bp\n7//kRsTOQGTm1TO1bdaJ+FOBUkQcT3FR+fMU12e3GQFWU4ToyAz7Z2q7pcL+qpYvX8y8ebO7zX10\ndGTmRn3KvqnO/qnMvqms0/rm0vse5P/ccz8v2mM5H3/Co9tdTjsdC1xVS8OmhG5mHrttOyK+B5wB\n/G1EHJeZ3wNOBK4GbgLOi4iFwDBwMMVNVtcDJ5XfPxG4NjPXRsSWiDgAuIviGvB7KG6e+mBEfAjY\nFxjMzAdmqnHVqtldgxgdHWFsbN2sjtGr7Jvq7J/K7JvKOrFvFmzcwrKhQSY2j8+6tk77haJOQZFL\nM2rlLWdnAxdExALgNuDSzJyIiI8B11Kc6n5HZm6KiE8BF0fEdRQj2ZPLxzgDuAQYoriOeyNARFwL\n/LB8jDNb+J0kqW+tHJ9kzcQkGycn211KW2Xm39batumhm5nHTXn5zGnevwC4YId9G4E/m6btDcDR\n0+x/N/Du2VUqSarHg+MTTAKb+zx06+GMVJKkhmyeLGaiGu/fuTHqZuhKkhryh0t3Yt/58zh2p8Xt\nLqVrGLqSpIb89a9/x6+2jnP+A6vaXUrXMHQlSQ05aqdFDAAHLJjf7lK6hqErSWrIzzdtpQRs6OMF\nD+pl6EqSGjJZKu5aLnn3cs0MXUlSQw5eNMwQcNiShe0upWsYupKkhnxv7UYmgBs3bG53KV3D0JUk\nNWTBQPH30sGB9hbSRQxdSVJDHjW8AID9h4fbXEn3aOXcy5KkHnLO3rtx0KJh/sL1dGvmSFeS1JCv\nrlzLxWOr+e66De0upWsYupKkhnzpwTWsnpzkwrEZlzBXmaErSWrIyctHWDwwwBt3X97uUrqGoStJ\nashvJ0psLJW4a+t4u0vpGt5IJUlqyHOXLeGB8XFOWLpTu0vpGo50JUkN+e7aDVyzbiM3bXyo3aV0\nDUNXktSQb65exwRw9er17S6laxi6kqSGrJksVhf6xWangayVoStJasi2Bf1Gh5wGslbeSCVJashh\nixbw3w9t4cw9d293KRW9ZtOz213CIxi6kqSGnLrbcr63biOHLVnU7lK6hqErSWrIm391P5tKsGRw\ngDfvPdrucrqC13QlSQ3ZNr49YrGrDNXK0JUkNWSv4QXMB0YXLGh3KV3D0JUkNeTezVvZCvxmy9Z2\nl9I1DF1JUkO2lIqHhpb4xFDNvJFKktSQZUNDPDAxwT4L5re7lLaKiHOAPwEWAJ/MzM9WautIV5LU\nkIcmJ5gEHirN2LRnRcRxwP8Cng48E9ivWntHupKkhiwZGmLz+AR7zB9qdyntdAKwAvg6sBR4c7XG\njnQlSQ1ZNT7BFmBzqY+HurAbcATwZ8AZwCURUfEqt6ErSWrItmUOVo5PtLWONlsJXJGZWzIzgU1A\nxZlCDF1JUkOWDxR3Dh2wsK8nx7gOeG5EDETE3sASiiCelqErSWrIrgvms+v8Ifr53uXMvBz4CXAT\n8G/AmZlZcejvjVSSpIYMMkCpNMA4/R0mmfmWWtv2cz9JkmbhyYuGWTs5yfCAs2PUytPLkqSGfHX1\nOq5Yu4GHJvv67uW6GLqSpLqt3/rwfMsLMHRrZehKkuq2acr2vKG+nhyjLoauJKluN6zb2O4SupKh\nK0mq2/Wr17a7hK5k6EqS6nbvxGS7S+hKhq4kqW4HDvfzlBiNM3QlSXW7cu1D7S6hKxm6kqS6jbe7\ngC5l6EqS6vaEBcVjQrs7GVVdDF1JUt3u3lrcSLXBeTHqYuhKkur2nGVLADh+6aI2V9JdDF1JUt1+\nsaW4qvvbcYe69TB0JUl12/bA0BKv6dal4tJ+EfGiaj+YmV+b+3IkSd1g00SxTvvIkGO3elRbT/ev\nqrxXAgxdSepTt2wuVhn65tqNnNfmWrpJxdDNzGfN5sARMQRcAARFSJ9BsTDFReXXtwJnZuZkRJwG\nnE7x6Nf7MvPyiFgEfBHYHVgHnJKZYxFxNPDRctsrM/M95c97F/C88v43ZeZNs6lfkjQz56WqT7WR\nLgARsSfwWeAg4BjgCxQBeN8MP/rHAJn59Ig4DjgPGADOzczvRcT5wAsi4ofAWcARwELguoj4DvA6\nYEVmvjsiXgacC7wROB94MXAX8M2IOKx83GcCRwH7AZcBT625FyRJddkZWA28erel7S6lqq/88gOz\nPsbZPGMOKinUcjL+k8A3gIeAVcAtFCFcVWZ+A3ht+eWjKf73ORz4fnnft4DjgSOB6zNzc2auAe4A\nDqUI+G9PbRsRS4HhzLwzM0vAFeVjHEMx6i1l5j3AvIgYreG7SZIaUBos7qDa7M3LdZlxpAs8JjMv\niIjXZ+ZW4K0RsaKWg2fmeERcDLwQeAnwnHJYQnHKeBmwFFgz5cem2z9139od2u5Pcdp65TTHGKtU\n2/Lli5k3b3YLL4+Ojszq53uZfVOd/VOZfVNZJ/XN1sniP+U7LVnYUXV1ulpCdzIito+II2KEOh41\nysxTIuKtwI3A1KeoRyhGv2vL29X2z9R2S4X9Fa1aNbsFmEdHRxgbWzerY/Qq+6Y6+6cy+6ayTuub\nk5Yt4fvrH+L5C4fnpK5+Ce5awvNrwCXAsog4Hfgu8M8z/VBEvCIizim/3AhMAv9Zvr4LcCJwLXAT\n8IyIWBgRy4CDKW6yuh44aWrbzFwLbImIAyJiADihfIzrgRMiYjAiHgUMZuYDNXw3SVIDLluzgbGJ\nSb78wJqZG2u7GUe6mfk3EfFKioB+DvAZ4B9rOPbXgM9FxDUUN7i9CbgNuCAiFpS3L83MiYj4GEV4\nDgLvyMxNEfEp4OKIuI5iJHty+bhnUPwSMERxHfdGgIi4Fvhh+Rhn1vTtJUmzspvP6dZloFSa+Sp4\nRMwDHg9sBX6RmZPNLqzZxsbWzeryf6ed6ukk9k119k9l9k1lndY3r7zjXu7aspUrHvcolsyr5Upl\ndaOjI02Z2+rDf/78Wd/qdfZXLp+z2mb8FSUijgLuBL4J/Adwe0QcMlcFSJK6zy2btrBmssSN62d3\nf0y/qeW8wEeB0zLz0Zm5H3A28KnmliVJ6mTbho8Dzr1cl1pCd0FmXrntRWb+G7CkeSVJkjrZ5OTD\nVxgXY+rWo5YT8TdHxEsy81KAiDgJ+HFzy5IkdarxKdtHLt2pbXV0ioj4MQ/PIfHLzHx1pbbVVhla\nR3EGYQj4y4hYBUwAuwH3z125kqRusqk8McYAMNDn55cjYiEwkJnH1dK+2kj3SXNSkSSppyyixCCw\noN2FdIYnA4sj4kqKTH17Zt5QqXG1VYb+Z9t2eVGBnSh+sRkCDqRYQUiS1Gd+umkzkxTz74qNwIco\n5q84CPhWRERmjk/XuJZVhi4AXkAxheOvKQL3OgxdSepLtz+0ud0ldJKfA3eU1xX4eUSsBPYC7p2u\ncS13Lz8HeCzFDFPPA55NkeySpD5005r17S6hk5wKfBggIvamWJjnt5Ua1xK6v83MDcDtwCGZ+X2K\nm6kkSX3olk1b211CJ/kssHN5yuKvAKdWOrUMtT0ytCUijgV+BpwYEVdj6EpS33qw3QV0kMycujbA\njGoZ6b4VOB34d+Aw4AHgiw1VJ0nqevuVnxLq74eFGlPLKkM3ANtufz4qInameF5XktSHHizPATn7\nZQ76T91rMmXmaoq7mCVJfWhD+W+v7Nav0YUQPasgSX1uYbsL6EKNhu6s1yeUJHW35qyA29saDV1J\nUp9b4/CrbtUWPFjB9CPaAYrZqSRJfexx8xzq1qvazWdvaFkVkqSu8/H99213CV2n2oIH329lIZKk\n7nLn5nF2XeBaQ/Xwmq4kqSF7zB9qdwldx9CVJNXs15seXtBvkZd062boSpJqdt3ahxeZ2314uI2V\ndKdqdy//G1Wex83MP2lKRZKkjjW2dUu7S+hq1e5evrRlVUiSusJFqzbM3EgVVbt7+eLp9kfEAHBg\n0yqSJHWsze0uoE4Ll/+fdpfwCDMuEhERpwN/CyyZsnsM2LNZRUmSOtMw3Re8naSWG6neBjwH+CbF\nerp/DXy9mUVJkjrTthuWXdavMbWE7oOZeSNwC7BHZp4HHNncsiRJnWjbA0OTba2ie9USulsjYjnw\nCx4O252aV5IkqdM5D1VjajlD8BngcuCPgVsi4oXA7U2tSpKkHjTjSDczLwT+KDMfBJ4GvBf482YX\nJknqXD6t25ha7l5eDLwkInbh4Wvorwc+0szCJEmd61nDzrvciFpOL38F2BuYur6uSxdLUh9772P2\naXcJXamW0H08cHBmjje7GElS51q1+eEndJcMOdJtRC13L9/b9CokSR3v+2vXb98eHHS9nEbUMtJd\nAVwdEd8GHtq2MzO9pitJfeST969udwldr5bQXQrcgfMtS1Jf+227C+gBM4ZuZr66FYVIktTrqq2n\n+8+Z+dKImHrX8naZeWhTK5MkqUtExO7AzcBzMrPiBFLVRrofKP/9hrksTJKkXhIR84FPM+W+p0qq\nhe5YRDwK+OVcFSZJ6n5Ovv97PgScD5wzU8Nq93z/N3Ar8DOK4P0p8JPy9o2zr1GS1I2OXjS/3SV0\njIh4FTCWmVfU0r5i6GbmSGYuBS4B/ndm7pyZuwIvBL41F8VKkrrD7zZs2L79rr1H21hJxzkVeE5E\nfA94CvD5iNizUuNaHhk6IjNP3/YiM/81It492yolSd3j7Lvv276986JFbayks2Tmsdu2y8F7Rmbe\nV6l9LVOKDEbEcVMO+lxcv1iS+sotzrg/J2oZ6f4V8NWI2EKxytAA8KdNrUqSpC6TmcfN1KaW0N0V\neBRwCMXzuitc/ECSpPrVErp/k5n/Avy42cVIkjqbV3Nnp6YFDyLiHcC1wPYlJjLTEJakPnDW7Xdt\n3z5nj13aWEn3qyV0jyr/ec2UfSVg/6ZUJEnqKFePP3wX1QtHl7exku5Xy4IHj633oOUpsS4EHgMM\nA++jmGTjIorAvhU4MzMnI+I04HRgHHhfZl4eEYuALwK7A+uAUzJzLCKOBj5abntlZr6n/HnvAp5X\n3v+mzLyp3polSWq2qqEbEXtTTGt1DEVYXg98IDN/NcNxXw6szMxXRMQuwC3lP+dm5vci4nzgBRHx\nQ+As4AhgIXBdRHwHeB3FDVvvjoiXAecCb6SYZuvFwF3ANyPiMIq7qZ9JMRrfD7gMeGqd/SBJUtNV\nfE43IvYDbgImgHcC51EE3E0R8egZjvvV8s9Q/plx4HDg++V93wKOB44Ers/MzZm5hmLd3kMpQv7b\nU9tGxFJgODPvzMwScEX5GMdQjHpLmXkPMC8inC5FkuaYkz/OXrWR7vuAczLzC1P2XRYRN5ffe0Wl\nH8zM9QARMQJcSjFS/VA5LKE4ZbwMWAqsmfKj0+2fum/tDm33BzYBK6c5xliV78by5YuZN2+oWpMZ\njY6OzOrne5l9U539U5l9U1k7+ibXrNu+fe3hBzG6dEnLa+gl1UL3DzLzlB13ZubnIuJtMx24PFL+\nOvDJzPxSRHxwytsjwGqKEB2ZYf9MbbdU2F/VqlUbZ2pS1ejoCGNj62Zu2Ifsm+rsn8rsm8ra1TfH\n3Xrn9u0lmyebVkO//LJVbRrIgSrvba520IjYA7gSeGtmXlje/ZMp00meSPEI0k3AMyJiYUQsAw6m\nuMnqeuCkqW0zcy2wJSIOiIgB4ITyMa4HToiIwfJShIOZ+UC1+iRJtXH2x7lVbaQ7HhF7Z+Zvpu4s\n31xVNXSBtwPLgXdGxLZru28EPhYRC4DbgEszcyIiPkYRnoPAOzJzU0R8Crg4Iq6jGMmeXD7GGRSr\nHg1RXMe9sVzTtcAPy8c4s5YvLklSqw2UStP/HhMRZ1As4/dn5VEmEbE78GXgi5n5uZZV2QRjY+tm\n9Qucp8Eqs2+qs38qs28qa1ffHDLl9PKKJx3QtM8ZHR2pdna1Ybc9/uBZD9YPvv22Oaut4kg3M8+P\niAOBX0fEzyhuXDsI+Fi3B64kaWZTB2UHtbGOXlL1Od3M/P8j4u8onoEFuGHH082SpN70n2u2z/zL\nRY+tuC676lDLjFS/Br7WglokSR3kjF/9bvv20iU+KjQXalnEXpLUh7a0u4AeZOhKktQihq4k6ffc\ns3F2EwhpeoauJOn3PO+u327f/g9vopozhq4kqao9vIlqzhi6kqRH2LLFW6iaxdCVJD3C4T+/d/v2\nd/bdtY2V9B5DV5JU0Z4779zuEnqKoStJUosYupKk7SYmJtpdQk8zdCVJ2x11293bt9+/5y7tK6RH\nGbqSpO2mLpb+/N2Wt62OXmXoSpLUIoauJAmAD999z/bt3dtYRy+bcWk/SVJ/uGj91u3bVz3pgDZW\n0j0iYgi4AAigBJyRmbdWau9IV5Kkxv0xQGY+HTgXOK9aY0NXksSRt97Z7hK6UmZ+A3ht+eWjgdXV\n2nt6WZLEQ1O2V3hquS6ZOR4RFwMvBF5Sra0jXUnqc2++3VHubGXmKcDjgAsiouKyTIauJPW5b48/\nvH35Pk6IUY+IeEVEnFN+uRGYLP+ZlqeXJamPrV+//hGvH73cCTHq9DXgcxFxDTAfeFNmPlSpsaEr\nSX3saXffv337a/vt1sZKulNmbgBeWmt7Ty9LkgA4aNmydpfQ8wxdSepTH7773pkbaU4ZupLUpy5a\nv2X7to8JtYahK0l9yHVz28PQlaQ+9JQp6+Y+qX1l9B1DV5L63Jc9tdwyPjIkSX3mzvvvn7lRj3jp\nObOPuRVzUMc2jnQlqc/86djDE2J8Ya+d21hJ/zF0JamPvPe2R86z/JRdd21TJf3J0JWkPlEqlfjn\nKTctnzbcvlr6laErSX3i0P++6xGvzzrIG6hazdCVpD7w5B0WqXcyjPYwdCWpx90/NvaIteY+sevi\nttXS7wxdSepxx9+/9hGvj91rrzZVIkNXknrYIZ5W7iiGriT1iZsft1+7S+h7hq4k9agdR7kLFixo\nUyXaxtCVpD7gaeXOYOhKUg/acZSrzmDoSlKPc5TbOQxdSeoxjnI7l6ErST3k9T4i1NEMXUnqIddO\n2V7StipUiaErST1ix9PKNzjK7TiGriT1gMM9rdwVDF1J6nJ/d+udbGl3EarJvGYePCKOAj6QmcdF\nxIHARUAJuBU4MzMnI+I04HRgHHhfZl4eEYuALwK7A+uAUzJzLCKOBj5abntlZr6n/DnvAp5X3v+m\nzLypmd9LkjrF1q1buXCHfY5yO1fTRroR8RbgH4GF5V0fAc7NzGcAA8ALImJP4Czg6cAJwPsjYhh4\nHbCi3PbzwLnlY5wPnAwcAxwVEYdFxB8AzwSOAl4GfKJZ30mSOs0f3P5LKJWKPxi4na6Zp5fvBF40\n5fXhwPfL298CjgeOBK7PzM2ZuQa4AziUIlS/PbVtRCwFhjPzzswsAVeUj3EMxai3lJn3APMiYrSJ\n30uSOsJPVzwFtq+UO2ngtkFEzI+IL0TEtRFxU0T8SbX2TQvdzLwM2Dpl10A5LKE4ZbwMWAqsmdJm\nuv1T962doe3U/ZLUs3664rkAXMLL+Htey83xmPYW1L9eDqwsn5l9LvDxao2bek13B5NTtkeA1RQh\nOjLD/pnabqmwv6rlyxczb95Qfd9gB6OjIzM36lP2TXX2T2X2TWWP7Jv7tm8dvf/n2WefXVtfkAC+\nClxa3h6guLeoolaG7k8i4rjM/B5wInA1cBNwXkQsBIaBgylusroeOKn8/onAtZm5NiK2RMQBwF0U\n14DfQ/EFPxgRHwL2BQYz84GZilm1auOsvszo6AhjY+tmdYxeZd9UZ/9UZt9UNrVvitPKD1uyZN+u\n77du/WUrM9cDRMQIRfieW619K0P3bOCCiFgA3AZcmpkTEfExiklUBoF3ZOamiPgUcHFEXEcxkj25\nfIwzgEuuKQciAAAL+UlEQVSAIYrruDcCRMS1wA/Lxzizhd9Jklpqx8A99JBb2lSJtomI/YCvA5/M\nzC9VaztQKpWqvd+zxsbWzeqL+xt5ZfZNdfZPZfZNZaOjI1z13UfeKHXw43/E/Pnz21TR3BodHRlo\nxnEPufiQWYfcilNWVKwtIvYAvge8ITOvmulYrRzpSpIatGPg7rXnd3omcLvc24HlwDsj4p3lfSdm\n5kPTNTZ0JanD7XhKed7Qqxgd9cnITpCZbwTeWGt7Q1eSOtSOYQswb+j9POEJJ7ahGs0FQ1eSOtB0\ngbvrLt9kn332aUM1miuGriR1kJ+u+AuKBzwe6dl/eKc3mfUAQ1eSOsR0o1vYl0MPubzltfSKFb+8\np90lPIKhK0ltNn3Y+gxuLzJ0JalNKoUtGLi9ytCVpBYzbPuXoStJLVItbPfb9yqWL3fRgl5n6EpS\nk/3iFz/hoU2vnva9Aw/4AYsXL25xRWoXQ1eSmsibpDSVoStJTVApbIcXfJSIZ7a4GnUKQ1eS5pA3\nSakaQ1eSZumnK04CflPxfcNW2xi6ktSA//qvW7juuqt4+jFfYGAABqZZcXXxoo9z4IHHtL44dSxD\nV5JqtHr1ai655LOP2Ddd2DqyVSWGriRV8YlPfLjq+3f/cmf23Gs1T3nyjQwPD7eoKnUrQ1eSdjBT\n0G4T8SSOP/7sJlejXmLoShK1B+3IyM688pV/2eRq1KsMXUl9q9agPfNMR7OaG4aupL6xYsUtXHPN\nVTW1NWjVDIaupJ51xRX/zh133FZz+zPOeBNDQ0NNrEj9ztCV1BNuuukmfvSja+v+OUe0aiVDV1JX\n2rxhA+vPv4V/mn8DDALTPC87nUMOOYJjj3XuY7WHoSupK2z+8O9POPFPQzMH7stf/hqWLVvWvMKk\nOhi6kjrOdAE7nZdNHM0/Dd5QvBiCP//zU9htt92aWJk0O4aupLaaGrC/qvNndzpsCWf+oddk1T0M\nXUktsfnTt8D62R1j+OzKy+ZJ3cDQlTQnNt/9G7jsd3N2PANWvcjQlVSzf73go6z+rxu2v37xo85m\naHAIBgZqvXl4WtsCdnR0hLGxdbOsUmq9iDgK+EBmHletnaErabt169bx9XNeW3P7ocEhBgYGKFGi\nlmd2HL2qF0XEW4BXABtmamvoSn3k828+Ax5aM2fHW7VhjGWLdn14FqdTH83w8uVzdnypS9wJvAj4\nwkwNDV2pR3zjH97L2vxZ0z/nlR//ctM/Q+ommXlZRDymlraGrtTh/uWLP2JzTv/eplUfmfPPO/KU\nM3n8U4+Z8+NKMnSllrv66qv52VdKbfns/Q5+Gs8686y2fLYkQ1eaM5f+/Y+YXNnqT90FeHD7K0/9\nSp3N0JUqeCATXvOKiu9/9+l/D/PmlW/anc0DMzN76XufWuGdSvsltVJm3g0cPVM7Q1d944GLLoTP\nnj93Bxwqnk+lVKo7c5/x+n3Za6+95q4WSdN6zKYvzfoYd8++jO0MXXW1B178x/C7+9vy2fvd8knu\nPfSMYrQLsA+89IyZR55OACH1L0NXHWXlgyv53ze8hC1srtrusBXjvO3yYrupJ3a/8g1223vvad/a\nDXhaMz9bUs8xdNV07/33v+Zq/mNOj7nkoQZ+aPkuLP/6Nx+eyEGSWszQVcNe/e+v5H+4oy2ffd2R\n81i5dZzXLTueJ73lb9pSgyTVy9DV7/vXj7Dw3o+wEBicsntgyt+HPmofGCy/OzC3J3hP4w38xUkn\nz9zwpDn9WElqOkO333zi6Szlfx7xP/wAla+LTre/3mkd9uOxXHzSJXX+lCT1HkO3Aas3buapb7um\n3WXU5c75JxcD03KKVgrZHQO1NGXf1L9/dM+v+dnSR7HfK37we8fw7lxJmp6h24ANm8fbXULddjwD\nXClMJ4F1wORpd8GCBVWPud+cVihJvc/QbcA+y5fw2qfuxmd+9EC7S6nZwVs+xjULz2Jw0dPg1K+2\nuxxJ6kuGboPe/uKjOO3YbjuF+qJ2FyBJfW1w5iaSJGkuGLqSJLWIoStJUov0zDXdiBgEPgk8GdgM\nvCYz2zNdkiRJ0+ilke6fAgsz82nA24APN+uDNmzYwNlf+gGrVq1q1kdIknpQL4XuMcC3ATLzBuCI\nZn3QceffzGU/XcUfXbiiWR8hSepBPXN6GVgKrJnyeiIi5mXmtDNZLF++mHnzZr/azOjoyKyP0Yvs\nl+rsn8rsm8rsm+7XS6G7Fpj6L3KwUuACrFq1seEP+tHZx26f6tDpDn+f00BWZ/9UZt9U1ut90y+/\nUPTS6eXrKa87ExFHA577lSR1lF4a6X4deE5E/IBiPv9Xt7keSZIeoWdCNzMngTPaXYckSZX00ull\nSZI6mqErSVKLGLqSJLWIoStJUosYupIktYihK0lSi/TMI0OSJLVavSvcOdKVJKlxda1wZ+hKktS4\nula4M3QlSWrctCvcVWrct9d0R0dHBubgGHNRSk+yb6qzfyqzbyqzb+p39/993qz/Wz+Dula4c6Qr\nSVLj6lrhrm9HupIkzYG6VrgbKJVKLalKkqR+5+llSZJaxNCVJKlFDF1JklrEG6nqVO+UX70mIn5M\ncYs8wC+B84CLgBJwK3BmZk5GxGnA6cA48L7MvDwiFgFfBHYH1gGnZOZYi7/CnIuIo4APZOZxEXEg\ns+yP8h2QHy23vTIz39P6bzU3duibw4DLgV+U3/5UZn6l3/omIuYDFwKPAYaB9wE/w383fcGRbv3q\nmvKrl0TEQmAgM48r/3k18BHg3Mx8BsWdey+IiD2Bs4CnAycA74+IYeB1wIpy288D57bli8yhiHgL\n8I/AwvKuueiP84GTKWa6OaocVl1nmr45HPjIlH8/X+nTvnk5sLL83Z4LfBz/3fQNQ7d+dU351WOe\nDCyOiCsj4rvl36wPB75ffv9bwPHAkcD1mbk5M9cAdwCHMqXvprTtdncCL5ryelb9ERFLgeHMvDMz\nS8AVdG8/Tdc3z4uIayLisxExQn/2zVeBd5a3ByhGpv676ROGbv3qmvKrx2wEPkTxW/cZwCUUI99t\nz52tA5bx+3003f5t+7paZl4GbJ2ya7b9sZSHT99P3d91pumbm4A3Z+axwF3Au+jDvsnM9Zm5rvxL\nx6UUI1X/3fQJQ7d+dU351WN+DnwxM0uZ+XNgJbDHlPdHgNX8fh9Nt3/bvl4zOWW7kf6o1LYXfD0z\nb962DRxGn/ZNROwHXA18ITO/hP9u+oahW7+6pvzqMadSvoYdEXtT/HZ9ZUQcV37/ROBaihHNMyJi\nYUQsAw6muDlke99NadtrfjKb/sjMtcCWiDggIgYozir0Sj9dERFHlrefDdxMH/ZNROwBXAm8NTMv\nLO/2302f6JfTonOprim/esxngYsi4jqKuyxPBR4ALoiIBcBtwKWZORERH6P4P/0g8I7M3BQRnwIu\nLv/8FoqbPnrN2cy+P7aduh+iuAv1xpZ/i+Z4HfAPEbEVuA94bWau7cO+eTuwHHhnRGy7tvtG4GP+\nu+l9TgMpSVKLeHpZkqQWMXQlSWoRQ1eSpBYxdCVJahFDV5KkFjF0pQ4REUdExKV1tN8tInz8QOoi\nPqcrdYjM/E/gJe2uQ1LzGLpShyjPSPRx4D8ppvU7BNgPuB14WWauj4gXUSynuBH40Q4//5fA6ynO\nYK0E3kAxded3gJsz8y0RcTzFEnKHZ+b9Lfhakqbw9LLUmQ6nWPbtYGBv4M/K0wdeCLw4Mw8H/mdb\n44h4JnAK8IzMPAz4IPC1zJykWErulRHxAuBzwMkGrtQehq7Umb5dXtJtK8X83rtQLOm2IjN/Vm7z\n6SntnwccCPwgIm6hCN1dImKXzPwtcBrFFKafycxrWvYtJD2Cp5elzvTQlO0SxTzf2/7eZurqVkMU\nK9a8FSAiBilGyKvK7z8RuJ9ijVZJbeJIV+oe1wJPjIgnl1+/asp7VwJ/ERF7lV+fAVwFUF7Z543A\nEcDOEfHG1pQraUeGrtQlMnOMYkWZSyLix8Bjp7x3BfAB4DsR8dNyuxcBOwFfBv4qM39NEdR/HRGH\ntbh8SbjKkCRJLeNIV5KkFjF0JUlqEUNXkqQWMXQlSWoRQ1eSpBYxdCVJahFDV5KkFjF0JUlqkf8H\nmO+Q8RJRex0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109b301d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(train_df.shape[0]), label[sorted_idx],s=3,\n",
    "            c=np.sort(label_ord[sorted_idx]), cmap = plt.get_cmap('tab10'))\n",
    "plt.colorbar()\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('Ordinal Label', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1a1c2deb70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAHfCAYAAACiUkX2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1c305940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''ulimit = np.percentile(train_df.label.values, 98)\n",
    "llimit = np.percentile(train_df.label.values, 2)\n",
    "train_df['label'].ix[train_df['label']>ulimit] = ulimit\n",
    "train_df['label'].ix[train_df['label']<llimit] = llimit'''\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.distplot(train_df.label.values, bins=50, kde=False)\n",
    "plt.xlabel('label', fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_coeff(n, metric, lmbda = 1):\n",
    "    if metric is 'ccr':\n",
    "        return [1]\n",
    "    elif metric is 'ccr1':\n",
    "        return [1, 1, 1]\n",
    "    elif metric is 'mae':\n",
    "        coeff = np.arange(1,n)/(n-1)\n",
    "    elif metric is 'mse':\n",
    "        coeff = np.zeros(n-1)\n",
    "        coeff[0] = 2*n-3\n",
    "        for k in range(1, n-1):\n",
    "            coeff[k] = coeff[k-1] + 2*n - (2*(k+1)+1)\n",
    "        coeff = coeff /((n-1)**2)\n",
    "    else:\n",
    "        print('Undefined Metric: ' + metric)\n",
    "    coeff = np.concatenate((coeff, coeff[::-1][1:]), axis=0)\n",
    "    coeff = coeff * lmbda\n",
    "    coeff[n-2] = 1\n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_type = 'house_16H'\n",
    "num_samples = 10000\n",
    "num_classes = 9\n",
    "nclasses = num_classes\n",
    "dim = 2\n",
    "\n",
    "sigma_noise = 0.01\n",
    "optimizer='sgd' #Optimizer function\n",
    "iter_loc=10 #Number of the first column in the excel file for writing the results.\n",
    "lr=.5 #Initial learning rate\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "batch_size = 256\n",
    "lr_scheduler=ft.exp_lr_scheduler #Learning rate scheduler\n",
    "lr_decay_epoch=10 #Number of epoch for learning rate decay\n",
    "hidden_sizes = [50, 50]\n",
    "dropouts = [0, 0]\n",
    "rand_label = False\n",
    "\n",
    "metric = 'ccr'\n",
    "coeff_lmbda =  1\n",
    "multi_coeff = make_coeff(nclasses, metric, coeff_lmbda)\n",
    "KL = False #KL divergence for porbability measure\n",
    "\n",
    "\n",
    "'''Multipliers for loss functions'''\n",
    "single_loss=1.\n",
    "multi_loss=0.\n",
    "\n",
    "comment=' ' #Additional comments if any\n",
    "\n",
    "algo = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.99786778 -0.1046049  -0.87203641 -0.64262862 -0.52512336  0.68509746\n",
      " -0.95656432  0.25911488 -0.94257411 -0.90573663 -0.7733197  -0.88512628\n",
      "  0.93387451 -0.3900144  -0.78524665 -0.01674871]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "(22784, 16)\n",
      "[6 3 4 ..., 8 4 8]\n",
      "{'train': 18227, 'val': 4556}\n",
      "OR\n",
      "Number of training images 5\n",
      "Number of validation images 5\n",
      "{'train': 18227, 'val': 4556}\n",
      "!!!!! NO CUDA GPUS DETECTED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"inputs, classes = next(iter(dset_loaders['train']))\\nprint(inputs.shape)\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV = 5\n",
    "random_seed = 1\n",
    "\n",
    "if data_type == 'circular':\n",
    "    fvec, label = generateCircularData(num_samples = num_samples, \n",
    "                                       num_classes = num_classes, dim = dim, bound = 5,\n",
    "                                       sigma_noise = sigma_noise, rand_label = rand_label)\n",
    "elif data_type == 'linear':\n",
    "    fvec, label = generateLinearData(num_samples = num_samples, \n",
    "                                     num_classes = num_classes, dim = dim, bound = 5,\n",
    "                                       sigma_noise = sigma_noise, rand_label = rand_label)\n",
    "elif data_type == 'spiral':\n",
    "    fvec, label = generateSpiralData(num_samples = num_samples, \n",
    "                                     num_classes = num_classes, dim = dim, bound = 5,\n",
    "                                       sigma_noise = sigma_noise, rand_label = rand_label)\n",
    "else:\n",
    "    num_classes = num_bins\n",
    "    nclasses = num_classes\n",
    "    \n",
    "    feat=train_df.values[:,:-2]\n",
    "    #Normalize the features\n",
    "\n",
    "    feat_max = np.amax(feat,axis=0)\n",
    "    feat_min = np.amin(feat,axis=0)\n",
    "\n",
    "    feat=(feat-feat_min)/(feat_max-feat_min)\n",
    "    feat=feat*2-1\n",
    "\n",
    "    '''feat_mean = np.mean(feat,axis=0)\n",
    "    feat_std = np.std(feat,axis=0)\n",
    "\n",
    "    feat=(feat-feat_mean)/feat_std\n",
    "    '''\n",
    "    label_ord=train_df.values[:,-1].astype(np.int)\n",
    "\n",
    "    rand_idx = np.random.permutation(len(label_ord))\n",
    "    feat = feat[rand_idx, :]\n",
    "    label = label_ord[rand_idx]\n",
    "\n",
    "\n",
    "    print(np.mean(feat,axis=0))\n",
    "    print(np.min(feat,axis=0))\n",
    "    print(feat.shape)\n",
    "    print(label)\n",
    "\n",
    "    fvec=feat.copy()\n",
    "    dim = feat.shape[1]\n",
    "    \n",
    "    if not CV == 0: \n",
    "        dset_train= torch.utils.data.TensorDataset(torch.from_numpy(fvec).type(torch.FloatTensor),\n",
    "                                                       torch.from_numpy(label).type(torch.LongTensor))\n",
    "        dset_val= torch.utils.data.TensorDataset(torch.from_numpy(fvec).type(torch.FloatTensor),\n",
    "                                                       torch.from_numpy(label).type(torch.LongTensor))\n",
    "\n",
    "        '''Define dataset loaders''''''\n",
    "        dset_loaders = {'train':torch.utils.data.DataLoader(dsets['train'], batch_size=batch_size,shuffle=True,\n",
    "                                                            num_workers=12),\n",
    "                        'val':torch.utils.data.DataLoader(dsets['val'], batch_size=batch_size,shuffle=False,\n",
    "                                                            num_workers=12)}\n",
    "\n",
    "\n",
    "        dset_sizes={'train':len(dsets['train']),'val':len(dsets['val'])}\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "\n",
    "        print(dset_sizes)\n",
    "\n",
    "        if use_gpu:\n",
    "            print('GPU is available')\n",
    "        else:\n",
    "            print('!!!!! NO CUDA GPUS DETECTED')\n",
    "\n",
    "        inputs, classes = next(iter(dset_loaders['train']))\n",
    "        print(inputs.shape)'''\n",
    "        '''dset_train = datasets.ImageFolder(data_dir+'/train_val', data_transforms['train'])\n",
    "        dset_val = datasets.ImageFolder(data_dir+'/train_val', data_transforms['val'])'''\n",
    "\n",
    "        num_train = len(dset_train)\n",
    "        indices = list(range(num_train))\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        splits = (num_train*np.linspace(0,1,CV+1)).astype(int)\n",
    "\n",
    "        val_idx = [indices[splits[k]:splits[k+1]] for k in range(CV)]\n",
    "        train_idx=[np.setdiff1d(indices,val_idx[k]) for k in range(CV)]\n",
    "        '''Sampler functions for validation and training'''\n",
    "        sampler_train = [torch.utils.data.sampler.SubsetRandomSampler(train_idx[k]) for k in range(CV)]\n",
    "        sampler_val = [torch.utils.data.sampler.SubsetRandomSampler(val_idx[k]) for k in range(CV)]\n",
    "\n",
    "        '''Define dataset loaders'''\n",
    "        dset_loaders_arr = [{'train':torch.utils.data.DataLoader(dset_train, batch_size=batch_size,sampler=sampler_train[k],\n",
    "                                                            num_workers=12),\n",
    "                        'val':torch.utils.data.DataLoader(dset_val, batch_size=batch_size,sampler=sampler_val[k],\n",
    "                                                            num_workers=12)} for k in range(CV)]\n",
    "        dset_sizes={'train':int(len(dset_train)*(1-1/CV)),'val':int(len(dset_train)*(1/CV))}\n",
    "\n",
    "        print(dset_sizes)\n",
    "        print('OR')\n",
    "        print('Number of training images '+str(len(val_idx)))\n",
    "        print('Number of validation images '+str(len(train_idx)))\n",
    "    \n",
    "\n",
    "\n",
    "'''rand_idx = np.random.permutation(len(label))\n",
    "fvec_norm = (fvec)/5\n",
    "mid_point = int(len(label)/2)#100*num_classes\n",
    "fvec_test = fvec_norm[rand_idx[:mid_point],:]\n",
    "fvec_train = fvec_norm[rand_idx[mid_point:],:]\n",
    "\n",
    "label_test = label[rand_idx[:mid_point]]\n",
    "label_train = label[rand_idx[mid_point:]]\n",
    "print(np.max(fvec_train))\n",
    "print(np.min(fvec_train))\n",
    "\n",
    "torch.from_numpy(label_train).type(torch.LongTensor)\n",
    "dsets={'train': torch.utils.data.TensorDataset(torch.from_numpy(fvec_train).type(torch.FloatTensor),\n",
    "                                               torch.from_numpy(label_train).type(torch.LongTensor)),\n",
    "       'val': torch.utils.data.TensorDataset(torch.from_numpy(fvec_test).type(torch.FloatTensor),\n",
    "                                             torch.from_numpy(label_test).type(torch.LongTensor))}\n",
    "\n",
    "''''''\n",
    "dset_loaders = {'train':torch.utils.data.DataLoader(dsets['train'], batch_size=batch_size,shuffle=True,\n",
    "                                                    num_workers=12),\n",
    "                'val':torch.utils.data.DataLoader(dsets['val'], batch_size=batch_size,shuffle=False,\n",
    "                                                    num_workers=12)}\n",
    "\n",
    "\n",
    "dset_sizes={'train':len(dsets['train']),'val':len(dsets['val'])}\n",
    "'''\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "print(dset_sizes)\n",
    "\n",
    "if use_gpu:\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    print('!!!!! NO CUDA GPUS DETECTED')\n",
    "\n",
    "'''inputs, classes = next(iter(dset_loaders['train']))\n",
    "print(inputs.shape)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(feat.astype(np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeLog(logname):\n",
    "    '''\n",
    "    Creates a text file named Network_properties.txt inside runs/'logname'\n",
    "    '''\n",
    "    f=open('runs_regression/'+logname+'/Network_properties.txt','w')\n",
    "    f.write('Feature Length: '+str(dim)+'\\n')\n",
    "    f.write('Number of classes: '+str(num_classes)+'\\n')\n",
    "    f.write('Data type: '+data_type+'\\n')\n",
    "    f.write('Random Noise: '+str(sigma_noise)+'\\n')\n",
    "    \n",
    "    f.write('Hidden sizes: '+ str(hidden_sizes)+'\\n')\n",
    "    f.write('Dropouts: '+str(dropouts)+'\\n')\n",
    "    f.write('Batch size: '+str(batch_size)+'\\n')\n",
    "    f.write('Number of samples: '+str(num_samples)+'\\n')\n",
    "    \n",
    "    f.write('Optimizer: ' + optimizer + '\\n')\n",
    "    crt=str(single_loss)+'xsingle + '+str(multi_loss)+'Xmulti'\n",
    "    f.write('Criterion: '+crt+'\\n')\n",
    "    f.write('Learning rate: '+str(lr)+'\\n')\n",
    "    f.write('Momentum: '+str(momentum)+'\\n')\n",
    "    f.write('Leraning Rate Scheduler: '+str(lr_scheduler)+'\\n')\n",
    "    f.write('Leraning Rate Decay Period: '+str(lr_decay_epoch)+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs_regression.xlsx\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "import time\n",
    "\n",
    "def writeLog_xlsx(logname='logs_regression.xlsx',iter_loc=10):\n",
    "    '''\n",
    "    Adds a line to logs.xlsx with the network properties and outcomes.\n",
    "    :param iter_loc: First column to record the outcomes.\n",
    "    '''\n",
    "    \n",
    "    print(logname)\n",
    "    book = openpyxl.load_workbook(logname)\n",
    "    sheet = book.active\n",
    "    crt=str(single_loss)+'xsingle + '+str(multi_loss)+'Xmulti'\n",
    "    if metric:\n",
    "        m_coeff = make_coeff(nclasses, metric, coeff_lmbda)\n",
    "    else:\n",
    "        m_coeff = multi_coeff\n",
    "    specs=(datetime.now().strftime('%B%d  %H:%M:%S'),data_type,str(hidden_sizes),str(dim),str(num_classes),\n",
    "           crt, str(lr), str(m_coeff), str(KL))\n",
    "    sheet.append(specs)\n",
    "    current_row = sheet.max_row\n",
    "    sheet.cell(row=current_row, column=iter_loc+5).value = comment\n",
    "    book.save(logname)\n",
    "writeLog_xlsx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc0): Linear(in_features=2, out_features=50, bias=True)\n",
      "  (relu0): ReLU()\n",
      "  (drop0): Dropout(p=0)\n",
      "  (fc1): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (drop1): Dropout(p=0)\n",
      "  (fc2): Linear(in_features=50, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, dropouts, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.numHidden=len(hidden_sizes)\n",
    "        setattr(self, 'fc0', nn.Linear(input_size, hidden_sizes[0]))\n",
    "        setattr(self, 'relu0', nn.ReLU())\n",
    "        setattr(self, 'drop0', nn.Dropout(p=dropouts[0]))\n",
    "        for k in range(len(hidden_sizes)-1):\n",
    "            setattr(self, 'fc'+str(k+1), nn.Linear(hidden_sizes[k], hidden_sizes[k+1]))\n",
    "            setattr(self, 'relu'+str(k+1), nn.ReLU())\n",
    "            setattr(self, 'drop'+str(k+1), nn.Dropout(p=dropouts[k+1]))\n",
    "        setattr(self, 'fc'+str(len(hidden_sizes)), nn.Linear(hidden_sizes[-1], num_classes))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out=self.fc0(x)\n",
    "        out = self.relu0(out)\n",
    "        out = self.drop0(out)\n",
    "        for k in range(self.numHidden-1):\n",
    "            fc = getattr(self,'fc'+str(k+1))\n",
    "            relu = getattr(self,'relu'+str(k+1))\n",
    "            drop = getattr(self,'drop'+str(k+1))\n",
    "            out = fc(out)\n",
    "            out = relu(out)\n",
    "            out = drop(out)\n",
    "        fc = getattr(self,'fc'+str(self.numHidden))\n",
    "        out = fc(out)\n",
    "        return out\n",
    "    \n",
    "model=Net(2, [50, 50], [0, 0], 2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network_loader(comment=comment,\n",
    "                    optimizer=optimizer,\n",
    "                    iter_loc=iter_loc,\n",
    "                    lr=lr,\n",
    "                    momentum=momentum,\n",
    "                    weight_decay=weight_decay,\n",
    "                    lr_scheduler=lr_scheduler,\n",
    "                    lr_decay_epoch=lr_decay_epoch,\n",
    "                    nclasses=num_classes,\n",
    "                    hidden_sizes = hidden_sizes,\n",
    "                    dropouts = dropouts):\n",
    "    \n",
    "    '''Load the network from pytorch'''\n",
    "    model_ft = Net(dim, hidden_sizes , dropouts, num_classes)\n",
    "\n",
    "    if use_gpu:\n",
    "        model_ft = model_ft.cuda()\n",
    "\n",
    "    '''Define the optimizer function'''\n",
    "    if(optimizer=='adam'):\n",
    "        optimizer_ft = optim.Adam(model_ft.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "    elif(optimizer=='sgd'):\n",
    "        if(end_to_end):\n",
    "            optimizer_ft = optim.SGD(model_ft.parameters(), lr=lr, momentum=momentum)\n",
    "        else:\n",
    "            optimizer_ft = optim.SGD(model_ft.fc.parameters(), lr=lr, momentum=momentum,weight_decay=weight_decay)\n",
    "    return model_ft, optimizer_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'params': [Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.0989  0.0276 -0.2002  0.2364  0.2187  0.2119 -0.1518  0.2126  0.1886  0.2376\n",
      "-0.1236 -0.0579 -0.1949 -0.0178  0.1627  0.1492 -0.0042 -0.1609  0.1058 -0.0118\n",
      " 0.1857  0.0867  0.0413 -0.1223 -0.0814 -0.0646 -0.0002  0.1864 -0.2206 -0.0385\n",
      "-0.0685 -0.1728 -0.0526 -0.0080 -0.1281  0.1239 -0.0421 -0.1162  0.0698  0.1712\n",
      "-0.0983 -0.2350  0.0183  0.2309  0.0525 -0.2041 -0.1387  0.0061 -0.1531  0.0568\n",
      " 0.0293  0.2072 -0.2436  0.1574 -0.1800 -0.0822 -0.0655  0.0933  0.1481  0.2364\n",
      " 0.1060 -0.1060  0.1533  0.1535  0.2004 -0.1161  0.2167  0.0250  0.2023  0.2357\n",
      " 0.2167 -0.1310 -0.0959  0.2040  0.0319 -0.1085 -0.0613 -0.0487 -0.1800 -0.1431\n",
      " 0.1159 -0.2102  0.2266  0.0710 -0.1200  0.0339  0.1792  0.2377 -0.1234 -0.1082\n",
      " 0.0269  0.1652 -0.0606 -0.1809  0.2315  0.2342 -0.0817  0.0290  0.0692  0.1348\n",
      " 0.1353 -0.0651 -0.1571 -0.0406 -0.0071  0.1899 -0.2087  0.1806 -0.2238 -0.1225\n",
      " 0.0041 -0.1201 -0.1526 -0.0879  0.2117  0.0301 -0.0848  0.1570  0.2225  0.1040\n",
      " 0.0044  0.0884  0.0006 -0.0837  0.2484 -0.0302  0.1120 -0.0892 -0.2069 -0.1711\n",
      "-0.0636 -0.1598 -0.0135  0.2430 -0.0612  0.2340 -0.1557 -0.1298  0.0057 -0.1470\n",
      " 0.0774  0.0692 -0.0242  0.0976 -0.1367 -0.0146  0.2042 -0.0059 -0.1841 -0.0959\n",
      "-0.1298  0.2260  0.1972  0.2155  0.1731 -0.2115 -0.0299 -0.0353 -0.1853 -0.2073\n",
      "-0.0652  0.1427 -0.0188  0.0413 -0.1361 -0.1657 -0.0024  0.1943 -0.2295  0.2378\n",
      "-0.1369 -0.2071  0.0168  0.0732 -0.2115 -0.1529  0.0452  0.0652 -0.0743 -0.0384\n",
      " 0.1028 -0.0009  0.2359  0.1414 -0.1017  0.1953 -0.0417  0.1544  0.1082  0.1863\n",
      " 0.0926 -0.1671  0.1624 -0.1546 -0.0672  0.0470  0.0118 -0.2347  0.2206  0.0155\n",
      " 0.1554  0.1408 -0.1131 -0.0592 -0.1664 -0.2056 -0.0665 -0.0382  0.1811 -0.0145\n",
      " 0.0498 -0.0383 -0.2477  0.0681 -0.1656  0.1675 -0.0394  0.0733 -0.0005  0.1387\n",
      " 0.0151  0.1172  0.0021  0.0922  0.0146 -0.1371  0.1280 -0.0325 -0.0354  0.0380\n",
      "-0.2092 -0.1545  0.1426  0.1091  0.0068  0.1498 -0.0564 -0.0414  0.1172  0.0790\n",
      " 0.1503 -0.1921  0.0962 -0.2005 -0.0115  0.0155 -0.2156 -0.1854  0.2457  0.2197\n",
      " 0.1098 -0.0019  0.0623  0.0798 -0.1071  0.1742 -0.1357  0.1624  0.0186 -0.1578\n",
      " 0.1053 -0.1892 -0.1622 -0.1695  0.2300 -0.1978  0.1877  0.1014  0.2192  0.0249\n",
      " 0.2480 -0.1156 -0.0972 -0.0010 -0.0176 -0.0939 -0.1494  0.1571 -0.0534  0.2195\n",
      " 0.1754  0.0876  0.2005 -0.0160 -0.0149 -0.1751  0.2163 -0.1065  0.1430  0.0003\n",
      " 0.0668 -0.0159  0.1540  0.0677  0.0311 -0.1255  0.1869 -0.0321 -0.0599  0.1473\n",
      " 0.1683 -0.0351  0.2150 -0.1712  0.2363 -0.1155  0.2142 -0.2007 -0.0479  0.1703\n",
      "-0.0699  0.0459 -0.0311  0.0507  0.1554  0.0264 -0.2226 -0.1146  0.2181  0.0635\n",
      " 0.2048 -0.1004  0.0066 -0.2418 -0.1587  0.2275 -0.0973  0.0902  0.0193  0.1579\n",
      " 0.1889 -0.2334 -0.0725 -0.2359 -0.0517  0.1283  0.0469 -0.2365  0.0494  0.1518\n",
      " 0.1833 -0.0060 -0.1548  0.0163 -0.1366 -0.1949 -0.0388  0.2319 -0.0506  0.0871\n",
      "-0.0421  0.0737  0.0395 -0.0236  0.0559 -0.1180  0.2287  0.0233 -0.2478 -0.2346\n",
      "-0.1182  0.1226  0.2062 -0.1295 -0.0816 -0.0213 -0.1873 -0.2272  0.0163  0.2242\n",
      " 0.2167 -0.1196 -0.1373  0.0051  0.1649  0.1820  0.0149  0.0522  0.1973 -0.0352\n",
      "-0.2026 -0.1902  0.2030 -0.0290 -0.0880  0.0201  0.2428  0.2104 -0.2392  0.2255\n",
      "-0.0836 -0.0252  0.1226 -0.2338  0.0760 -0.1333  0.2079 -0.0738 -0.1824  0.1276\n",
      " 0.1725 -0.2347  0.0541  0.1941 -0.0509  0.0512  0.2191  0.1534 -0.0165  0.1709\n",
      "-0.0822  0.0645 -0.0136  0.2034 -0.1123  0.0683 -0.0846 -0.2060 -0.1441 -0.0469\n",
      "-0.1413 -0.0763  0.0769 -0.0791  0.2144 -0.1624 -0.1504  0.0477 -0.0414 -0.2176\n",
      " 0.0370 -0.2181  0.1895  0.0073  0.2260 -0.1891 -0.0075  0.1460 -0.2483 -0.0684\n",
      "-0.0535 -0.2102 -0.1583 -0.0232 -0.0934 -0.0967 -0.0547 -0.2240 -0.0899  0.0199\n",
      " 0.0356 -0.0880 -0.1574 -0.2122 -0.0679  0.1584  0.0501 -0.2080 -0.0324  0.0372\n",
      " 0.1478  0.0137 -0.0117 -0.2069 -0.1578 -0.2110 -0.1547  0.1794 -0.0072 -0.1382\n",
      "-0.1353  0.0416 -0.1670 -0.0110 -0.0508  0.2151  0.1410 -0.0949 -0.1421 -0.1459\n",
      "-0.1170 -0.2012  0.0152 -0.0856  0.1820  0.0809  0.1680  0.1725  0.1622 -0.0532\n",
      " 0.0594  0.0355 -0.1572  0.1911  0.1561  0.1226  0.2198 -0.1320  0.1013 -0.2433\n",
      "\n",
      "Columns 10 to 15 \n",
      " 0.2120 -0.0118  0.2485 -0.2296  0.0838  0.0148\n",
      "-0.0254  0.2176  0.1639  0.0925  0.1090 -0.2227\n",
      "-0.2220 -0.0783  0.1464  0.1307 -0.2139 -0.0587\n",
      "-0.1827  0.0425 -0.1780  0.0759 -0.1802  0.2023\n",
      "-0.0499  0.0631 -0.0470  0.1671  0.0675  0.2407\n",
      "-0.2303  0.1110  0.1969 -0.1182 -0.2352 -0.1834\n",
      "-0.2069  0.0097 -0.1959 -0.1907  0.1193  0.2416\n",
      " 0.1294  0.0248 -0.0660 -0.1881 -0.1173  0.1539\n",
      " 0.2133 -0.1134 -0.2253 -0.1696  0.0935  0.1714\n",
      "-0.0813 -0.1398  0.2138  0.1647 -0.1725  0.1607\n",
      "-0.1893  0.2189 -0.0887  0.0378 -0.1480 -0.0388\n",
      " 0.2199  0.1947  0.0709  0.2221 -0.0370  0.1546\n",
      "-0.1906  0.0130  0.1742 -0.0223  0.0300 -0.1473\n",
      " 0.1885  0.0700  0.1706  0.0461 -0.0945  0.0191\n",
      " 0.1735 -0.0598 -0.0102  0.0124 -0.1049  0.0384\n",
      " 0.0020 -0.0560  0.0457  0.1884  0.2219 -0.1218\n",
      "-0.0167 -0.1600 -0.1056 -0.2369  0.1886 -0.1016\n",
      " 0.2154  0.0043 -0.1271  0.1077 -0.1045 -0.1245\n",
      "-0.0419 -0.0590  0.1792  0.1909 -0.1497  0.0731\n",
      "-0.1290 -0.0334 -0.1334 -0.0612  0.2071 -0.1711\n",
      " 0.0244  0.2409 -0.2101 -0.1064 -0.0316 -0.2069\n",
      " 0.1621  0.1301  0.0089  0.0391  0.0247 -0.0021\n",
      " 0.1654  0.1039 -0.2092  0.2359  0.0720  0.0543\n",
      " 0.0575  0.1522  0.0536 -0.1691 -0.1473  0.0859\n",
      " 0.0738 -0.0973  0.2138  0.1608 -0.0174  0.1651\n",
      "-0.2049 -0.2369 -0.2407 -0.1517  0.1637  0.2193\n",
      " 0.1748  0.1255  0.1106 -0.1538  0.1558 -0.1312\n",
      "-0.1913  0.1029 -0.1505 -0.2168  0.0350  0.1013\n",
      " 0.2473 -0.0222 -0.2041  0.0816  0.1470  0.0464\n",
      "-0.1545 -0.1341 -0.1036  0.0575  0.1872  0.0736\n",
      "-0.1864  0.0842 -0.1093  0.0114  0.1089  0.0125\n",
      " 0.0276 -0.1681 -0.2439  0.1569 -0.0474 -0.2453\n",
      "-0.2342 -0.1593  0.0431 -0.1215  0.0838  0.1008\n",
      " 0.0208  0.0934 -0.0054  0.1601 -0.0374 -0.0238\n",
      "-0.0043  0.2384  0.0591  0.1185 -0.0331 -0.1013\n",
      "-0.1834  0.0699 -0.1774  0.1372  0.0828 -0.0379\n",
      "-0.1828 -0.2320 -0.2061  0.1867 -0.1746 -0.0875\n",
      " 0.2214  0.1672 -0.1937 -0.0088 -0.0202  0.0380\n",
      "-0.1328 -0.1460  0.0235  0.0644 -0.0579 -0.1087\n",
      " 0.1324 -0.0831 -0.2499  0.0646 -0.0033  0.0880\n",
      " 0.1304 -0.2090 -0.0020 -0.1187  0.1515  0.2494\n",
      " 0.0411 -0.0081  0.2064  0.1012  0.1308 -0.1166\n",
      "-0.1001 -0.1028 -0.1331  0.2048 -0.2402 -0.2021\n",
      "-0.1771 -0.0595 -0.0574  0.1788 -0.2390  0.1947\n",
      "-0.2253  0.1967 -0.1812 -0.1400  0.1513  0.2245\n",
      "-0.0441 -0.1947  0.2260  0.1255  0.2308  0.1388\n",
      " 0.0088 -0.0308 -0.1196  0.0121 -0.1286  0.2343\n",
      "-0.1825  0.1320 -0.2325 -0.1694 -0.1471  0.1491\n",
      " 0.2442 -0.1418  0.1031  0.0742 -0.0525  0.0802\n",
      " 0.0806 -0.0788 -0.2176  0.1438  0.0943  0.1706\n",
      "[torch.FloatTensor of size 50x16]\n",
      ", Parameter containing:\n",
      " 0.1433\n",
      " 0.0208\n",
      "-0.2030\n",
      "-0.0310\n",
      "-0.1504\n",
      "-0.0258\n",
      " 0.0003\n",
      "-0.1570\n",
      "-0.1882\n",
      "-0.0672\n",
      " 0.0926\n",
      "-0.1561\n",
      "-0.1968\n",
      "-0.1081\n",
      "-0.1404\n",
      " 0.1744\n",
      " 0.0434\n",
      " 0.1500\n",
      " 0.1923\n",
      "-0.1241\n",
      " 0.0928\n",
      " 0.1018\n",
      " 0.0183\n",
      " 0.1133\n",
      " 0.0253\n",
      "-0.2073\n",
      " 0.1337\n",
      " 0.0910\n",
      "-0.0460\n",
      "-0.2370\n",
      "-0.1089\n",
      " 0.0449\n",
      "-0.0235\n",
      " 0.0644\n",
      " 0.0730\n",
      "-0.0784\n",
      "-0.1528\n",
      "-0.1097\n",
      " 0.2224\n",
      "-0.0892\n",
      " 0.0792\n",
      " 0.1041\n",
      "-0.0326\n",
      "-0.1505\n",
      "-0.1074\n",
      "-0.1631\n",
      "-0.2356\n",
      " 0.0687\n",
      " 0.1900\n",
      " 0.1416\n",
      "[torch.FloatTensor of size 50]\n",
      ", Parameter containing:\n",
      "-0.0608 -0.0624  0.0883  ...   0.0584  0.0786 -0.1001\n",
      "-0.1384 -0.0784 -0.1021  ...   0.0682  0.0986 -0.0628\n",
      " 0.0829  0.0019 -0.0245  ...   0.0375 -0.0496 -0.0888\n",
      "          ...             ⋱             ...          \n",
      " 0.0325  0.1208 -0.0907  ...  -0.0532 -0.0787  0.1147\n",
      " 0.0009 -0.0655 -0.0532  ...  -0.0045  0.1292  0.0785\n",
      " 0.0014  0.1203  0.0273  ...  -0.0020  0.0652 -0.1213\n",
      "[torch.FloatTensor of size 50x50]\n",
      ", Parameter containing:\n",
      "-0.0315\n",
      " 0.1000\n",
      " 0.0854\n",
      " 0.0965\n",
      "-0.1019\n",
      " 0.0209\n",
      "-0.0639\n",
      " 0.0733\n",
      " 0.1332\n",
      "-0.1314\n",
      " 0.1371\n",
      " 0.0352\n",
      " 0.1204\n",
      "-0.0090\n",
      "-0.1324\n",
      " 0.0906\n",
      "-0.0503\n",
      "-0.0376\n",
      " 0.1206\n",
      " 0.0589\n",
      "-0.1275\n",
      " 0.0736\n",
      "-0.1344\n",
      " 0.0091\n",
      " 0.0939\n",
      "-0.1002\n",
      " 0.0538\n",
      "-0.1308\n",
      " 0.1086\n",
      " 0.1189\n",
      "-0.0369\n",
      " 0.0519\n",
      "-0.1114\n",
      "-0.0194\n",
      " 0.1388\n",
      " 0.0651\n",
      " 0.0466\n",
      " 0.1288\n",
      "-0.1378\n",
      "-0.0273\n",
      "-0.1385\n",
      " 0.0848\n",
      "-0.0562\n",
      " 0.1042\n",
      "-0.0150\n",
      " 0.1042\n",
      "-0.0201\n",
      " 0.1397\n",
      " 0.1247\n",
      " 0.0806\n",
      "[torch.FloatTensor of size 50]\n",
      ", Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0329 -0.0067 -0.1156 -0.0211 -0.0736  0.0763 -0.1131  0.0383 -0.1252 -0.1185\n",
      " 0.0066 -0.0286 -0.0016 -0.1046 -0.0123  0.0177 -0.0970  0.1029  0.0949 -0.1335\n",
      " 0.1084  0.0392  0.0392 -0.0120  0.0826  0.0193 -0.1142  0.0784 -0.0565  0.1114\n",
      " 0.1332 -0.1128  0.0823  0.1326 -0.1049 -0.0455  0.0362 -0.0552 -0.0745  0.0679\n",
      " 0.0525  0.1040 -0.0751  0.0566  0.0482  0.0652 -0.0102  0.0608  0.0520  0.0654\n",
      " 0.0334  0.0902 -0.1070 -0.0670  0.0849 -0.1386 -0.1073 -0.0226 -0.1144  0.1197\n",
      " 0.0047  0.0800 -0.0335  0.0672 -0.0694 -0.1339 -0.0835  0.0823  0.0580  0.0683\n",
      "-0.1338 -0.0113  0.0406  0.0394 -0.0261  0.1104 -0.1127  0.0014  0.1333 -0.1221\n",
      " 0.0444 -0.0158  0.0983  0.0034  0.0140 -0.1167 -0.0748 -0.0015 -0.1003  0.0821\n",
      " 0.0248  0.0019  0.0335  0.0274  0.1172  0.0062  0.0983 -0.0545 -0.0804 -0.1378\n",
      "\n",
      "Columns 10 to 19 \n",
      "-0.0473 -0.1316  0.0646  0.1066  0.0969  0.0317  0.0980  0.0815  0.0151  0.0410\n",
      "-0.0645  0.0844  0.0746  0.0581 -0.0282 -0.0538  0.1065 -0.1304 -0.1385 -0.0281\n",
      " 0.0965 -0.1310  0.1071 -0.0482  0.0830 -0.0826 -0.0062  0.0510 -0.0875  0.0951\n",
      "-0.0056  0.0457  0.0225  0.1377  0.1346  0.1358 -0.0294 -0.1073 -0.0775  0.1031\n",
      "-0.0019  0.1176  0.1087 -0.0435 -0.0705  0.0804 -0.0422  0.0833  0.0279  0.0080\n",
      " 0.0767 -0.1378  0.0473  0.0245 -0.0265 -0.0596  0.0541  0.0903 -0.1180 -0.0066\n",
      " 0.1031  0.0286  0.1018  0.0048  0.0887 -0.0016  0.0184 -0.1056 -0.0407  0.0388\n",
      "-0.0182 -0.1193 -0.1115  0.0371 -0.0548  0.1263  0.0216  0.1057 -0.1288 -0.0386\n",
      "-0.1324 -0.0845 -0.1346  0.1350  0.0354  0.0440 -0.1354  0.1143  0.1118 -0.0911\n",
      "-0.0488 -0.0418  0.1413 -0.0730  0.0847 -0.1411 -0.1233 -0.0726 -0.0952 -0.0604\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.0586  0.1282  0.1289 -0.1072  0.1000 -0.0063 -0.1025 -0.1381  0.1292  0.0635\n",
      "-0.0319  0.1015  0.0844  0.1102  0.1319  0.0980  0.0027 -0.0175 -0.0738 -0.0592\n",
      " 0.0869  0.0900  0.1031  0.0425 -0.0759 -0.0945  0.0258  0.0176  0.1043  0.0843\n",
      " 0.1102 -0.1054 -0.1293 -0.0958 -0.0266  0.0201  0.1131 -0.1332  0.0847  0.0043\n",
      "-0.1258  0.1129 -0.1219  0.0019 -0.1409 -0.0625  0.1179  0.0958  0.0945 -0.1258\n",
      " 0.0523 -0.0790 -0.0653 -0.0219 -0.0661 -0.0263 -0.0089  0.1134 -0.0413  0.0322\n",
      "-0.1061  0.0722  0.0554  0.1294 -0.0645  0.0486 -0.0977  0.0702  0.0190 -0.1407\n",
      " 0.1107 -0.0247  0.1400  0.0485 -0.0084 -0.0094  0.1123 -0.0776 -0.1212  0.0107\n",
      "-0.0633  0.1133 -0.1238  0.0891  0.0318  0.0849  0.1303 -0.0547  0.0739 -0.0149\n",
      " 0.0734 -0.0613  0.0439 -0.1096  0.0991 -0.0441  0.1010  0.0717  0.1122 -0.0913\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.0442 -0.0446  0.0142 -0.1102  0.0610  0.0744 -0.0003 -0.1013  0.0097  0.0844\n",
      "-0.0330  0.0163  0.0453 -0.0826 -0.0153 -0.1174  0.0233 -0.0747  0.0591 -0.0348\n",
      "-0.0607  0.0996 -0.1086  0.1327  0.0064  0.0676 -0.0233  0.0879  0.1059  0.1063\n",
      "-0.1025 -0.0930  0.0837  0.0930  0.0270 -0.0416 -0.0052 -0.0001 -0.0942 -0.0470\n",
      "-0.0444  0.0167 -0.0594 -0.0572  0.0406  0.1165 -0.0752 -0.0976  0.0186 -0.0161\n",
      "-0.0085 -0.0237  0.0286 -0.1252 -0.0706  0.0522  0.0668  0.1257  0.0031 -0.1115\n",
      "-0.0557 -0.0363 -0.0251 -0.0227  0.1212  0.1379  0.0435 -0.1401  0.0407  0.1234\n",
      "-0.0405 -0.0803 -0.1266  0.1220 -0.0342  0.0727  0.0314  0.1050 -0.0256 -0.1323\n",
      "-0.0260 -0.1011 -0.0025 -0.0967 -0.0493 -0.1027 -0.0802 -0.0528 -0.0008  0.0130\n",
      "-0.1185 -0.0633  0.0962 -0.0036  0.0389  0.0476 -0.0044  0.0816  0.0440 -0.0651\n",
      "\n",
      "Columns 40 to 49 \n",
      "-0.1016  0.1302 -0.1249  0.0534  0.0700  0.0494 -0.0053 -0.0677  0.0096  0.0540\n",
      " 0.1303 -0.0309 -0.1397  0.0490 -0.0409  0.0742  0.0453 -0.1399 -0.0779  0.1362\n",
      " 0.0376 -0.1228  0.1356 -0.1321  0.0718  0.0727  0.0653  0.0204 -0.0057  0.1053\n",
      " 0.0402 -0.0656 -0.0801 -0.0057  0.0665  0.1224 -0.1101 -0.0102 -0.0164  0.0389\n",
      "-0.0698  0.0847 -0.0488  0.1341 -0.0455 -0.0014  0.0326 -0.0545 -0.0184 -0.0598\n",
      " 0.0481  0.1314 -0.0800  0.1409 -0.0907 -0.0937 -0.1073 -0.0683 -0.0717 -0.0848\n",
      "-0.0328  0.0712  0.0848  0.0993  0.0045 -0.0528 -0.0533 -0.0460 -0.0864  0.0827\n",
      " 0.0061  0.0632 -0.1116  0.0387 -0.0427  0.0265  0.0380  0.1236  0.0115 -0.0628\n",
      " 0.0214 -0.1179 -0.0863  0.0952  0.0033  0.1208  0.0125 -0.1249  0.1239 -0.0882\n",
      " 0.0315  0.0164 -0.0085 -0.0450 -0.0789  0.0394 -0.1072 -0.0888  0.0963 -0.0940\n",
      "[torch.FloatTensor of size 10x50]\n",
      ", Parameter containing:\n",
      " 0.0892\n",
      "-0.0820\n",
      " 0.1061\n",
      "-0.0322\n",
      " 0.0517\n",
      "-0.0110\n",
      "-0.0266\n",
      " 0.0679\n",
      " 0.0184\n",
      " 0.1307\n",
      "[torch.FloatTensor of size 10]\n",
      ", Variable containing:\n",
      " 0.7866\n",
      "-0.6991\n",
      " 0.5702\n",
      "-0.5417\n",
      " 0.3867\n",
      "-1.0279\n",
      " 0.0241\n",
      " 0.8079\n",
      "-1.8278\n",
      "-1.6017\n",
      "[torch.FloatTensor of size 10x1]\n",
      "], 'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}]\n"
     ]
    }
   ],
   "source": [
    "model_ft, optimizer_ft = network_loader(comment=comment, #'Tested for three rooms'\n",
    "                                            optimizer=optimizer,\n",
    "                                            iter_loc=iter_loc,\n",
    "                                            lr=lr,\n",
    "                                            momentum=momentum,\n",
    "                                            weight_decay=weight_decay,\n",
    "                                            lr_scheduler=lr_scheduler,\n",
    "                                            lr_decay_epoch=lr_decay_epoch,\n",
    "                                            nclasses=num_classes)\n",
    "a_vec = Variable(torch.randn(10, 1), requires_grad=True)\n",
    "params = optimizer_ft.param_groups\n",
    "params[0]['params'].append(a_vec)\n",
    "optimizer_ft.param_groups = params\n",
    "print(optimizer_ft.param_groups)\n",
    "#optimizer_ft.add_param_group({'params': a_vec})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_epochs(result_log, logname):\n",
    "    print(len(result_log))\n",
    "\n",
    "    wb_tr = openpyxl.Workbook()\n",
    "    ws_tr = wb_tr.active\n",
    "    wb_val = openpyxl.Workbook()\n",
    "    ws_val = wb_val.active\n",
    "    print(logname)\n",
    "\n",
    "    label_arr_tr = np.zeros((100000,1))\n",
    "    probs_arr_tr = np.zeros((100000, num_classes))\n",
    "    label_arr_val = np.zeros((100000,1))\n",
    "    probs_arr_val = np.zeros((100000, num_classes))\n",
    "\n",
    "    prev_epoch = 0\n",
    "    \n",
    "    count_tr = count_val = 0\n",
    "    for result in result_log:\n",
    "        epoch = result[1]\n",
    "        if not epoch == prev_epoch:\n",
    "            label_arr_tr = label_arr_tr[:count_tr]\n",
    "            probs_arr_tr = probs_arr_tr[:count_tr, :]\n",
    "            label_arr_val = label_arr_val[:count_val]\n",
    "            probs_arr_val = probs_arr_val[:count_val, :]\n",
    "            ws_tr.append(['Epoch ' + str(prev_epoch)])\n",
    "            ws_tr.append(label_arr_tr[1:].reshape(-1).tolist())\n",
    "            ws_tr.append(np.argmax(probs_arr_tr[1:,:], axis=1).reshape(-1).tolist())\n",
    "            for probs in probs_arr_tr[1:,:].T.tolist():\n",
    "                ws_tr.append(probs)\n",
    "            #wb_tr.save('./runs_ord/'+logname + '/train.xlsx')\n",
    "            ws_val.append(['Epoch ' + str(prev_epoch)])\n",
    "            ws_val.append(label_arr_val[1:].reshape(-1).tolist())\n",
    "            ws_val.append(np.argmax(probs_arr_val[1:,:], axis=1).reshape(-1).tolist())\n",
    "            for probs in probs_arr_val[1:,:].T.tolist():\n",
    "                ws_val.append(probs)\n",
    "    \n",
    "\n",
    "            label_arr_tr = np.zeros((100000,1))\n",
    "            probs_arr_tr = np.zeros((100000, num_classes))\n",
    "            label_arr_val = np.zeros((100000,1))\n",
    "            probs_arr_val = np.zeros((100000, num_classes)) \n",
    "            count_tr = count_val = 0\n",
    "            prev_epoch = epoch\n",
    "\n",
    "        label = np.asarray(result[2]).reshape(-1,1)\n",
    "        scores = np.asarray(result[3])\n",
    "        exp_scores = np.exp(scores - np.max(scores,axis=1).reshape(-1, 1)*np.ones(num_classes))\n",
    "        probs = np.round(exp_scores/(np.sum(exp_scores,axis=1).reshape(-1, 1)*np.ones(num_classes)), decimals=2)\n",
    "        if result[0] == 'train':\n",
    "            label_arr_tr[count_tr:count_tr + len(label)]  = label\n",
    "            probs_arr_tr[count_tr:count_tr + len(label), :] = probs\n",
    "            count_tr += len(label)\n",
    "        elif result[0] == 'val':\n",
    "            label_arr_val[count_val:count_val + len(label)]  = label\n",
    "            probs_arr_val[count_val:count_val + len(label), :] = probs\n",
    "            count_val += len(label)\n",
    "\n",
    "\n",
    "    \n",
    "    label_arr_tr = label_arr_tr[:count_tr]\n",
    "    probs_arr_tr = probs_arr_tr[:count_tr, :]\n",
    "    label_arr_val = label_arr_val[:count_val]\n",
    "    probs_arr_val = probs_arr_val[:count_val, :]\n",
    "            \n",
    "    ws_tr.append(['Epoch ' + str(epoch)])\n",
    "    ws_tr.append(label_arr_tr[1:].reshape(-1).tolist())\n",
    "    ws_tr.append(np.argmax(probs_arr_tr[1:,:], axis=1).reshape(-1).tolist())\n",
    "    for probs in probs_arr_tr[1:,:].T.tolist():\n",
    "        ws_tr.append(probs)\n",
    "    #wb_tr.save('./runs_ord/'+logname + '/train.xlsx')\n",
    "    ws_val.append(['Epoch ' + str(epoch)])\n",
    "    ws_val.append(label_arr_val[1:].reshape(-1).tolist())\n",
    "    ws_val.append(np.argmax(probs_arr_val[1:,:], axis=1).reshape(-1).tolist())\n",
    "    for probs in probs_arr_val[1:,:].T.tolist():\n",
    "        ws_val.append(probs)\n",
    "    wb_val.save('./runs_regression/'+logname + '/val.xlsx')\n",
    "    label_arr_tr = np.zeros((1,1))\n",
    "    probs_arr_tr = np.zeros((1, num_classes))\n",
    "    label_arr_val = np.zeros((1,1))\n",
    "    probs_arr_val = np.zeros((1, num_classes))\n",
    "    prev_epoch = epoch\n",
    "    print('Finito')\n",
    "    \n",
    "    del label_arr_tr, probs_arr_tr, label_arr_val, probs_arr_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ft)\n",
    "    \n",
    "def run_network():\n",
    "    '''\n",
    "    Cretaes the log files and starts the training\n",
    "    '''\n",
    "    model_ft, optimizer_ft = network_loader(comment=comment, #'Tested for three rooms'\n",
    "                                            optimizer=optimizer,\n",
    "                                            iter_loc=iter_loc,\n",
    "                                            lr=lr,\n",
    "                                            momentum=momentum,\n",
    "                                            weight_decay=weight_decay,\n",
    "                                            lr_scheduler=lr_scheduler,\n",
    "                                            lr_decay_epoch=lr_decay_epoch,\n",
    "                                            nclasses=num_classes)\n",
    "    \n",
    "    \n",
    "    '''Name of the trial'''\n",
    "    crt=str(single_loss)+'xsingle + '+str(multi_loss)+'Xmulti'\n",
    "    logname='Ordinal_'+datetime.now().strftime('%B%d  %H:%M:%S')\n",
    "    writer = SummaryWriter('runs_regression/'+logname) #For tensorboard\n",
    "    writeLog(logname)\n",
    "    writeLog_xlsx()\n",
    "    \n",
    "    '''Start trianing'''\n",
    "    if metric:\n",
    "        m_coeff = make_coeff(nclasses, metric, coeff_lmbda)\n",
    "    else:\n",
    "        m_coeff = multi_coeff\n",
    "    best_model, last_model, result_log = ft.train_model(model_ft,optimizer_ft, lr_scheduler,dset_loaders,\n",
    "                            dset_sizes,writer,use_gpu=use_gpu,num_epochs=100,batch_size=batch_size,num_log=250,\n",
    "                            lr_decay_epoch=lr_decay_epoch,init_lr=lr,regression=False,\n",
    "                            iter_loc=iter_loc,cross_loss=single_loss,multi_loss=multi_loss,numOut=num_classes,\n",
    "                            logname='logs_regression.xlsx',\n",
    "                            multi_coeff = m_coeff, single_coeff = m_coeff, KL = KL, algo = algo)\n",
    "    \n",
    "    '''Save the models'''\n",
    "    torch.save(best_model,'./saved_models/ord/'+logname+'_best')\n",
    "    torch.save(last_model,'./saved_models/ord/'+logname+'_last')\n",
    "    \n",
    "    '''print('Writing results')\n",
    "    write_epochs(result_log, logname)\n",
    "    print('Wrote results')'''\n",
    "    '''Free up the memory'''\n",
    "    del model_ft, result_log\n",
    "    \n",
    "    writer.close\n",
    "    del writer\n",
    "    return last_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22784, 16)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logname' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7a38fb7017e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m run_network()'''\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'logname' is not defined"
     ]
    }
   ],
   "source": [
    "'''hidden_sizes = [50, 50]\n",
    "dropouts = [0, 0]\n",
    "end_to_end = True\n",
    "run_network()'''\n",
    "print(fvec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs_regression.xlsx\n",
      "Multi_coef is [ 0.11111111  0.22222222  0.33333333  0.44444444  0.55555556  0.66666667\n",
      "  0.77777778  0.88888889  1.          0.88888889  0.77777778  0.66666667\n",
      "  0.55555556  0.44444444  0.33333333  0.22222222  0.11111111]\n",
      "Epoch 0/99\n",
      "----------\n",
      "LR is set to 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozan-macbook-air/Desktop/projects/amazon/notebook/functions/fine_tune.py:100: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.3. Note that arange generates values in [start; end), not [start; end].\n",
      "  a_vec = Variable(torch.range(0, numOut - 1).view(numOut, 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0282 Acc: 0.1287 CIR-1: 0.3769 RMSE 2.6845 MAE 2.2135\n",
      "val Loss: 0.0217 Acc: 0.1530 CIR-1: 0.4530 RMSE 2.3654 MAE 1.8984\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.0207 Acc: 0.1671 CIR-1: 0.4733 RMSE 2.3088 MAE 1.8233\n",
      "val Loss: 0.0177 Acc: 0.1863 CIR-1: 0.5219 RMSE 2.1360 MAE 1.6561\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.0170 Acc: 0.1892 CIR-1: 0.5362 RMSE 2.0906 MAE 1.6210\n",
      "val Loss: 0.0153 Acc: 0.2002 CIR-1: 0.5683 RMSE 1.9906 MAE 1.5261\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.0159 Acc: 0.1988 CIR-1: 0.5555 RMSE 2.0266 MAE 1.5616\n",
      "val Loss: 0.0175 Acc: 0.1914 CIR-1: 0.5450 RMSE 2.1255 MAE 1.6293\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.0147 Acc: 0.2056 CIR-1: 0.5927 RMSE 1.9537 MAE 1.4883\n",
      "val Loss: 0.0172 Acc: 0.1866 CIR-1: 0.5768 RMSE 2.1113 MAE 1.5966\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.0151 Acc: 0.2018 CIR-1: 0.5827 RMSE 1.9754 MAE 1.5107\n",
      "val Loss: 0.0144 Acc: 0.2061 CIR-1: 0.5632 RMSE 1.9362 MAE 1.4989\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.0142 Acc: 0.2114 CIR-1: 0.6048 RMSE 1.9117 MAE 1.4530\n",
      "val Loss: 0.0135 Acc: 0.2171 CIR-1: 0.6444 RMSE 1.8543 MAE 1.3883\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.0141 Acc: 0.2106 CIR-1: 0.6140 RMSE 1.9025 MAE 1.4434\n",
      "val Loss: 0.0130 Acc: 0.2316 CIR-1: 0.6302 RMSE 1.8267 MAE 1.3725\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0138 Acc: 0.2105 CIR-1: 0.6176 RMSE 1.8895 MAE 1.4359\n",
      "val Loss: 0.0143 Acc: 0.2155 CIR-1: 0.6023 RMSE 1.9244 MAE 1.4618\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0132 Acc: 0.2240 CIR-1: 0.6309 RMSE 1.8488 MAE 1.3920\n",
      "val Loss: 0.0128 Acc: 0.2366 CIR-1: 0.6501 RMSE 1.8328 MAE 1.3565\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.0132 Acc: 0.2223 CIR-1: 0.6368 RMSE 1.8471 MAE 1.3901\n",
      "val Loss: 0.0126 Acc: 0.2333 CIR-1: 0.6479 RMSE 1.8116 MAE 1.3540\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0133 Acc: 0.2186 CIR-1: 0.6285 RMSE 1.8537 MAE 1.4025\n",
      "val Loss: 0.0124 Acc: 0.2447 CIR-1: 0.6475 RMSE 1.7919 MAE 1.3334\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0124 Acc: 0.2285 CIR-1: 0.6516 RMSE 1.7976 MAE 1.3475\n",
      "val Loss: 0.0121 Acc: 0.2322 CIR-1: 0.6635 RMSE 1.7801 MAE 1.3264\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0126 Acc: 0.2275 CIR-1: 0.6453 RMSE 1.8107 MAE 1.3609\n",
      "val Loss: 0.0123 Acc: 0.2364 CIR-1: 0.6475 RMSE 1.7855 MAE 1.3396\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0124 Acc: 0.2349 CIR-1: 0.6472 RMSE 1.7996 MAE 1.3471\n",
      "val Loss: 0.0142 Acc: 0.2070 CIR-1: 0.6245 RMSE 1.9191 MAE 1.4460\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0125 Acc: 0.2293 CIR-1: 0.6484 RMSE 1.8069 MAE 1.3541\n",
      "val Loss: 0.0127 Acc: 0.2423 CIR-1: 0.6484 RMSE 1.8152 MAE 1.3477\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0121 Acc: 0.2398 CIR-1: 0.6568 RMSE 1.7772 MAE 1.3254\n",
      "val Loss: 0.0117 Acc: 0.2377 CIR-1: 0.6572 RMSE 1.7416 MAE 1.3088\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0123 Acc: 0.2384 CIR-1: 0.6532 RMSE 1.7893 MAE 1.3355\n",
      "val Loss: 0.0137 Acc: 0.2256 CIR-1: 0.6453 RMSE 1.8873 MAE 1.4021\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0122 Acc: 0.2392 CIR-1: 0.6598 RMSE 1.7845 MAE 1.3278\n",
      "val Loss: 0.0121 Acc: 0.2307 CIR-1: 0.6431 RMSE 1.7720 MAE 1.3398\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0119 Acc: 0.2457 CIR-1: 0.6626 RMSE 1.7600 MAE 1.3072\n",
      "val Loss: 0.0114 Acc: 0.2452 CIR-1: 0.6714 RMSE 1.7295 MAE 1.2851\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.0112 Acc: 0.2592 CIR-1: 0.6857 RMSE 1.7116 MAE 1.2557\n",
      "val Loss: 0.0113 Acc: 0.2428 CIR-1: 0.6795 RMSE 1.7077 MAE 1.2713\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0112 Acc: 0.2542 CIR-1: 0.6845 RMSE 1.7062 MAE 1.2578\n",
      "val Loss: 0.0110 Acc: 0.2645 CIR-1: 0.6842 RMSE 1.7048 MAE 1.2480\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0111 Acc: 0.2575 CIR-1: 0.6826 RMSE 1.7089 MAE 1.2579\n",
      "val Loss: 0.0110 Acc: 0.2594 CIR-1: 0.6835 RMSE 1.6932 MAE 1.2487\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0111 Acc: 0.2593 CIR-1: 0.6859 RMSE 1.6981 MAE 1.2488\n",
      "val Loss: 0.0110 Acc: 0.2586 CIR-1: 0.6863 RMSE 1.6883 MAE 1.2441\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0111 Acc: 0.2587 CIR-1: 0.6854 RMSE 1.7034 MAE 1.2524\n",
      "val Loss: 0.0110 Acc: 0.2590 CIR-1: 0.6769 RMSE 1.7082 MAE 1.2599\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0111 Acc: 0.2602 CIR-1: 0.6839 RMSE 1.7020 MAE 1.2513\n",
      "val Loss: 0.0111 Acc: 0.2550 CIR-1: 0.6809 RMSE 1.7034 MAE 1.2577\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0110 Acc: 0.2575 CIR-1: 0.6859 RMSE 1.6994 MAE 1.2509\n",
      "val Loss: 0.0110 Acc: 0.2586 CIR-1: 0.6855 RMSE 1.6973 MAE 1.2491\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0110 Acc: 0.2631 CIR-1: 0.6873 RMSE 1.6987 MAE 1.2454\n",
      "val Loss: 0.0109 Acc: 0.2627 CIR-1: 0.6848 RMSE 1.6939 MAE 1.2436\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0110 Acc: 0.2636 CIR-1: 0.6868 RMSE 1.6960 MAE 1.2431\n",
      "val Loss: 0.0110 Acc: 0.2671 CIR-1: 0.6817 RMSE 1.6966 MAE 1.2436\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0111 Acc: 0.2625 CIR-1: 0.6879 RMSE 1.6964 MAE 1.2442\n",
      "val Loss: 0.0109 Acc: 0.2619 CIR-1: 0.6914 RMSE 1.6844 MAE 1.2355\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0110 Acc: 0.2607 CIR-1: 0.6879 RMSE 1.6931 MAE 1.2440\n",
      "val Loss: 0.0109 Acc: 0.2603 CIR-1: 0.6874 RMSE 1.6844 MAE 1.2392\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0110 Acc: 0.2632 CIR-1: 0.6900 RMSE 1.6934 MAE 1.2401\n",
      "val Loss: 0.0109 Acc: 0.2647 CIR-1: 0.6874 RMSE 1.6859 MAE 1.2362\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0110 Acc: 0.2644 CIR-1: 0.6871 RMSE 1.6969 MAE 1.2432\n",
      "val Loss: 0.0110 Acc: 0.2570 CIR-1: 0.6826 RMSE 1.6884 MAE 1.2476\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0109 Acc: 0.2635 CIR-1: 0.6889 RMSE 1.6917 MAE 1.2400\n",
      "val Loss: 0.0109 Acc: 0.2676 CIR-1: 0.6890 RMSE 1.6855 MAE 1.2320\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0110 Acc: 0.2647 CIR-1: 0.6879 RMSE 1.6935 MAE 1.2404\n",
      "val Loss: 0.0109 Acc: 0.2691 CIR-1: 0.6853 RMSE 1.6903 MAE 1.2368\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0109 Acc: 0.2650 CIR-1: 0.6889 RMSE 1.6908 MAE 1.2378\n",
      "val Loss: 0.0109 Acc: 0.2651 CIR-1: 0.6844 RMSE 1.6886 MAE 1.2395\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0109 Acc: 0.2644 CIR-1: 0.6888 RMSE 1.6937 MAE 1.2404\n",
      "val Loss: 0.0109 Acc: 0.2608 CIR-1: 0.6833 RMSE 1.6859 MAE 1.2432\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0109 Acc: 0.2677 CIR-1: 0.6905 RMSE 1.6846 MAE 1.2319\n",
      "val Loss: 0.0109 Acc: 0.2689 CIR-1: 0.6874 RMSE 1.6817 MAE 1.2311\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0109 Acc: 0.2670 CIR-1: 0.6913 RMSE 1.6854 MAE 1.2328\n",
      "val Loss: 0.0108 Acc: 0.2678 CIR-1: 0.6787 RMSE 1.6932 MAE 1.2434\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0109 Acc: 0.2644 CIR-1: 0.6899 RMSE 1.6894 MAE 1.2371\n",
      "val Loss: 0.0108 Acc: 0.2678 CIR-1: 0.6883 RMSE 1.6836 MAE 1.2322\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "LR is set to 0.00010000000000000002\n",
      "train Loss: 0.0108 Acc: 0.2669 CIR-1: 0.6929 RMSE 1.6797 MAE 1.2287\n",
      "val Loss: 0.0108 Acc: 0.2676 CIR-1: 0.6874 RMSE 1.6792 MAE 1.2302\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0109 Acc: 0.2675 CIR-1: 0.6939 RMSE 1.6801 MAE 1.2278\n",
      "val Loss: 0.0108 Acc: 0.2689 CIR-1: 0.6890 RMSE 1.6788 MAE 1.2281\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2673 CIR-1: 0.6927 RMSE 1.6797 MAE 1.2287\n",
      "val Loss: 0.0108 Acc: 0.2695 CIR-1: 0.6874 RMSE 1.6815 MAE 1.2300\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2683 CIR-1: 0.6941 RMSE 1.6805 MAE 1.2270\n",
      "val Loss: 0.0108 Acc: 0.2693 CIR-1: 0.6885 RMSE 1.6800 MAE 1.2285\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2679 CIR-1: 0.6934 RMSE 1.6802 MAE 1.2276\n",
      "val Loss: 0.0108 Acc: 0.2673 CIR-1: 0.6877 RMSE 1.6801 MAE 1.2309\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2660 CIR-1: 0.6928 RMSE 1.6793 MAE 1.2292\n",
      "val Loss: 0.0108 Acc: 0.2689 CIR-1: 0.6905 RMSE 1.6784 MAE 1.2270\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2670 CIR-1: 0.6940 RMSE 1.6782 MAE 1.2273\n",
      "val Loss: 0.0108 Acc: 0.2695 CIR-1: 0.6888 RMSE 1.6780 MAE 1.2274\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0109 Acc: 0.2678 CIR-1: 0.6940 RMSE 1.6808 MAE 1.2278\n",
      "val Loss: 0.0108 Acc: 0.2669 CIR-1: 0.6885 RMSE 1.6786 MAE 1.2300\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2684 CIR-1: 0.6927 RMSE 1.6788 MAE 1.2269\n",
      "val Loss: 0.0108 Acc: 0.2684 CIR-1: 0.6874 RMSE 1.6771 MAE 1.2291\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2667 CIR-1: 0.6931 RMSE 1.6789 MAE 1.2282\n",
      "val Loss: 0.0108 Acc: 0.2691 CIR-1: 0.6881 RMSE 1.6775 MAE 1.2281\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2696 CIR-1: 0.6931 RMSE 1.6779 MAE 1.2253\n",
      "val Loss: 0.0108 Acc: 0.2695 CIR-1: 0.6894 RMSE 1.6777 MAE 1.2270\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2693 CIR-1: 0.6940 RMSE 1.6793 MAE 1.2258\n",
      "val Loss: 0.0108 Acc: 0.2669 CIR-1: 0.6885 RMSE 1.6771 MAE 1.2294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2675 CIR-1: 0.6928 RMSE 1.6769 MAE 1.2270\n",
      "val Loss: 0.0108 Acc: 0.2709 CIR-1: 0.6888 RMSE 1.6785 MAE 1.2265\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2686 CIR-1: 0.6939 RMSE 1.6791 MAE 1.2264\n",
      "val Loss: 0.0108 Acc: 0.2656 CIR-1: 0.6883 RMSE 1.6790 MAE 1.2313\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2683 CIR-1: 0.6926 RMSE 1.6811 MAE 1.2282\n",
      "val Loss: 0.0108 Acc: 0.2711 CIR-1: 0.6905 RMSE 1.6760 MAE 1.2243\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2708 CIR-1: 0.6935 RMSE 1.6796 MAE 1.2249\n",
      "val Loss: 0.0108 Acc: 0.2709 CIR-1: 0.6905 RMSE 1.6760 MAE 1.2243\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2680 CIR-1: 0.6941 RMSE 1.6777 MAE 1.2261\n",
      "val Loss: 0.0108 Acc: 0.2680 CIR-1: 0.6892 RMSE 1.6777 MAE 1.2281\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2681 CIR-1: 0.6938 RMSE 1.6794 MAE 1.2270\n",
      "val Loss: 0.0107 Acc: 0.2678 CIR-1: 0.6899 RMSE 1.6760 MAE 1.2272\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2691 CIR-1: 0.6934 RMSE 1.6804 MAE 1.2268\n",
      "val Loss: 0.0108 Acc: 0.2660 CIR-1: 0.6881 RMSE 1.6782 MAE 1.2309\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2682 CIR-1: 0.6930 RMSE 1.6775 MAE 1.2264\n",
      "val Loss: 0.0108 Acc: 0.2715 CIR-1: 0.6890 RMSE 1.6772 MAE 1.2252\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "train Loss: 0.0107 Acc: 0.2692 CIR-1: 0.6944 RMSE 1.6782 MAE 1.2251\n",
      "val Loss: 0.0108 Acc: 0.2713 CIR-1: 0.6888 RMSE 1.6771 MAE 1.2254\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2690 CIR-1: 0.6941 RMSE 1.6787 MAE 1.2257\n",
      "val Loss: 0.0107 Acc: 0.2711 CIR-1: 0.6885 RMSE 1.6774 MAE 1.2259\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2687 CIR-1: 0.6944 RMSE 1.6782 MAE 1.2254\n",
      "val Loss: 0.0108 Acc: 0.2702 CIR-1: 0.6890 RMSE 1.6771 MAE 1.2261\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2689 CIR-1: 0.6938 RMSE 1.6799 MAE 1.2265\n",
      "val Loss: 0.0108 Acc: 0.2702 CIR-1: 0.6894 RMSE 1.6763 MAE 1.2254\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2686 CIR-1: 0.6941 RMSE 1.6789 MAE 1.2261\n",
      "val Loss: 0.0108 Acc: 0.2700 CIR-1: 0.6892 RMSE 1.6764 MAE 1.2261\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2686 CIR-1: 0.6938 RMSE 1.6792 MAE 1.2265\n",
      "val Loss: 0.0108 Acc: 0.2704 CIR-1: 0.6896 RMSE 1.6763 MAE 1.2252\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0109 Acc: 0.2685 CIR-1: 0.6941 RMSE 1.6791 MAE 1.2262\n",
      "val Loss: 0.0108 Acc: 0.2700 CIR-1: 0.6892 RMSE 1.6768 MAE 1.2261\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2685 CIR-1: 0.6934 RMSE 1.6794 MAE 1.2268\n",
      "val Loss: 0.0108 Acc: 0.2700 CIR-1: 0.6894 RMSE 1.6771 MAE 1.2261\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2688 CIR-1: 0.6934 RMSE 1.6794 MAE 1.2265\n",
      "val Loss: 0.0108 Acc: 0.2698 CIR-1: 0.6899 RMSE 1.6767 MAE 1.2259\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2687 CIR-1: 0.6940 RMSE 1.6790 MAE 1.2260\n",
      "val Loss: 0.0107 Acc: 0.2698 CIR-1: 0.6903 RMSE 1.6760 MAE 1.2252\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2686 CIR-1: 0.6940 RMSE 1.6788 MAE 1.2260\n",
      "val Loss: 0.0108 Acc: 0.2695 CIR-1: 0.6901 RMSE 1.6756 MAE 1.2252\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2684 CIR-1: 0.6939 RMSE 1.6796 MAE 1.2267\n",
      "val Loss: 0.0108 Acc: 0.2698 CIR-1: 0.6899 RMSE 1.6762 MAE 1.2254\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2683 CIR-1: 0.6938 RMSE 1.6784 MAE 1.2264\n",
      "val Loss: 0.0108 Acc: 0.2693 CIR-1: 0.6896 RMSE 1.6761 MAE 1.2259\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2683 CIR-1: 0.6945 RMSE 1.6788 MAE 1.2261\n",
      "val Loss: 0.0107 Acc: 0.2700 CIR-1: 0.6894 RMSE 1.6766 MAE 1.2259\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2687 CIR-1: 0.6939 RMSE 1.6793 MAE 1.2262\n",
      "val Loss: 0.0108 Acc: 0.2695 CIR-1: 0.6901 RMSE 1.6766 MAE 1.2259\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2686 CIR-1: 0.6938 RMSE 1.6792 MAE 1.2265\n",
      "val Loss: 0.0108 Acc: 0.2698 CIR-1: 0.6901 RMSE 1.6759 MAE 1.2252\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2684 CIR-1: 0.6940 RMSE 1.6789 MAE 1.2262\n",
      "val Loss: 0.0107 Acc: 0.2700 CIR-1: 0.6901 RMSE 1.6762 MAE 1.2252\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2685 CIR-1: 0.6940 RMSE 1.6791 MAE 1.2265\n",
      "val Loss: 0.0108 Acc: 0.2698 CIR-1: 0.6905 RMSE 1.6755 MAE 1.2248\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2687 CIR-1: 0.6941 RMSE 1.6790 MAE 1.2260\n",
      "val Loss: 0.0108 Acc: 0.2700 CIR-1: 0.6903 RMSE 1.6753 MAE 1.2245\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2684 CIR-1: 0.6934 RMSE 1.6788 MAE 1.2265\n",
      "val Loss: 0.0107 Acc: 0.2698 CIR-1: 0.6896 RMSE 1.6762 MAE 1.2256\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-06\n",
      "train Loss: 0.0107 Acc: 0.2689 CIR-1: 0.6938 RMSE 1.6786 MAE 1.2258\n",
      "val Loss: 0.0108 Acc: 0.2698 CIR-1: 0.6896 RMSE 1.6762 MAE 1.2256\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2687 CIR-1: 0.6939 RMSE 1.6784 MAE 1.2258\n",
      "val Loss: 0.0108 Acc: 0.2698 CIR-1: 0.6896 RMSE 1.6762 MAE 1.2256\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2688 CIR-1: 0.6937 RMSE 1.6786 MAE 1.2259\n",
      "val Loss: 0.0108 Acc: 0.2695 CIR-1: 0.6901 RMSE 1.6758 MAE 1.2254\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2686 CIR-1: 0.6938 RMSE 1.6789 MAE 1.2261\n",
      "val Loss: 0.0107 Acc: 0.2698 CIR-1: 0.6903 RMSE 1.6752 MAE 1.2248\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2682 CIR-1: 0.6939 RMSE 1.6790 MAE 1.2265\n",
      "val Loss: 0.0108 Acc: 0.2698 CIR-1: 0.6903 RMSE 1.6752 MAE 1.2248\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2683 CIR-1: 0.6939 RMSE 1.6789 MAE 1.2264\n",
      "val Loss: 0.0108 Acc: 0.2695 CIR-1: 0.6903 RMSE 1.6753 MAE 1.2250\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2683 CIR-1: 0.6940 RMSE 1.6792 MAE 1.2264\n",
      "val Loss: 0.0108 Acc: 0.2695 CIR-1: 0.6903 RMSE 1.6750 MAE 1.2248\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2684 CIR-1: 0.6939 RMSE 1.6789 MAE 1.2263\n",
      "val Loss: 0.0107 Acc: 0.2698 CIR-1: 0.6903 RMSE 1.6752 MAE 1.2248\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2683 CIR-1: 0.6938 RMSE 1.6790 MAE 1.2265\n",
      "val Loss: 0.0108 Acc: 0.2698 CIR-1: 0.6903 RMSE 1.6752 MAE 1.2248\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2685 CIR-1: 0.6939 RMSE 1.6792 MAE 1.2263\n",
      "val Loss: 0.0108 Acc: 0.2698 CIR-1: 0.6905 RMSE 1.6748 MAE 1.2243\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2684 CIR-1: 0.6942 RMSE 1.6792 MAE 1.2263\n",
      "val Loss: 0.0107 Acc: 0.2698 CIR-1: 0.6905 RMSE 1.6752 MAE 1.2245\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2682 CIR-1: 0.6940 RMSE 1.6793 MAE 1.2266\n",
      "val Loss: 0.0108 Acc: 0.2700 CIR-1: 0.6903 RMSE 1.6748 MAE 1.2243\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2684 CIR-1: 0.6939 RMSE 1.6791 MAE 1.2264\n",
      "val Loss: 0.0108 Acc: 0.2698 CIR-1: 0.6903 RMSE 1.6752 MAE 1.2248\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2683 CIR-1: 0.6939 RMSE 1.6790 MAE 1.2264\n",
      "val Loss: 0.0108 Acc: 0.2698 CIR-1: 0.6903 RMSE 1.6756 MAE 1.2250\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2684 CIR-1: 0.6940 RMSE 1.6791 MAE 1.2263\n",
      "val Loss: 0.0108 Acc: 0.2698 CIR-1: 0.6901 RMSE 1.6758 MAE 1.2252\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2683 CIR-1: 0.6938 RMSE 1.6791 MAE 1.2265\n",
      "val Loss: 0.0108 Acc: 0.2698 CIR-1: 0.6903 RMSE 1.6756 MAE 1.2250\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2685 CIR-1: 0.6940 RMSE 1.6789 MAE 1.2261\n",
      "val Loss: 0.0108 Acc: 0.2698 CIR-1: 0.6903 RMSE 1.6756 MAE 1.2250\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2684 CIR-1: 0.6939 RMSE 1.6789 MAE 1.2263\n",
      "val Loss: 0.0108 Acc: 0.2698 CIR-1: 0.6903 RMSE 1.6752 MAE 1.2248\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.2683 CIR-1: 0.6940 RMSE 1.6793 MAE 1.2265\n",
      "val Loss: 0.0107 Acc: 0.2700 CIR-1: 0.6905 RMSE 1.6746 MAE 1.2241\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.2683 CIR-1: 0.6939 RMSE 1.6794 MAE 1.2266\n",
      "val Loss: 0.0108 Acc: 0.2698 CIR-1: 0.6905 RMSE 1.6748 MAE 1.2243\n",
      "\n",
      "Training complete in 3m 12s\n",
      "Best val RMSE: 1.674644\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './saved_models/ord/Ordinal_February13  00:21:29_best'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-8bbcfa7209bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0malgo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fix_a'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdset_loaders\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdset_loaders_arr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mrun_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-1d854c7fac37>\u001b[0m in \u001b[0;36mrun_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;34m'''Save the models'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'./saved_models/ord/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlogname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_best'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'./saved_models/ord/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlogname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ozan-macbook-air/anaconda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0mto\u001b[0m \u001b[0moverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \"\"\"\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ozan-macbook-air/anaconda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './saved_models/ord/Ordinal_February13  00:21:29_best'"
     ]
    }
   ],
   "source": [
    "end_to_end = True\n",
    "optimizer='sgd' #Optimizer function\n",
    "lr=.01 #Initial learning rate\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "lr_scheduler=ft.exp_lr_scheduler #Learning rate scheduler\n",
    "lr_decay_epoch=20 #Number of epoch for learning rate decay\n",
    "\n",
    "hidden_sizes = [64, 64, 128, 128, 256, 512, 256, 128, 64, 32, 16]\n",
    "dropouts = [0, 0, 0, 0, 0, .5, .5, .5, 0, 0, 0]\n",
    "\n",
    "'''hidden_size = [64, 64, 128, 64, 32]\n",
    "dropouts = [0, 0, 0, 0, 0]'''\n",
    "\n",
    "single_loss=0.\n",
    "multi_loss =1.\n",
    "\n",
    "metric = 'mae'\n",
    "algo = 'fix_a'\n",
    "for dset_loaders in dset_loaders_arr:\n",
    "    run_network()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "end_to_end = True\n",
    "optimizer='sgd' #Optimizer function\n",
    "lr=1 #Initial learning rate\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "lr_scheduler=ft.exp_lr_scheduler #Learning rate scheduler\n",
    "lr_decay_epoch=40 #Number of epoch for learning rate decay\n",
    "\n",
    "hidden_sizes = [64, 64, 128, 128, 256, 512, 256, 128, 64, 32, 16]#8, 16, 8, 4, 4]\n",
    "dropouts = [0, 0, .5, .5, .5, .5, .5, .5, .5, 0, 0]#.5, .5, .5]\n",
    "\n",
    "for kk in range(3):\n",
    "    single_loss=1.\n",
    "    multi_loss =0.\n",
    "    KL = True\n",
    "    metric = 'ccr'\n",
    "    for dset_loaders in dset_loaders_arr:\n",
    "        run_network()\n",
    "\n",
    "    metric = 'ccr1'\n",
    "    for dset_loaders in dset_loaders_arr:\n",
    "        run_network()\n",
    "\n",
    "    metric = 'mae'\n",
    "    for dset_loaders in dset_loaders_arr:\n",
    "        run_network()\n",
    "\n",
    "    single_loss=0.\n",
    "    multi_loss =1.\n",
    "    metric = 'ccr'\n",
    "    for dset_loaders in dset_loaders_arr:\n",
    "        run_network()\n",
    "\n",
    "    metric = 'ccr1'\n",
    "    for dset_loaders in dset_loaders_arr:\n",
    "        run_network()\n",
    "\n",
    "    metric = 'mae'\n",
    "    for dset_loaders in dset_loaders_arr:\n",
    "        run_network()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm is learn_a\n",
      "Multi_coef is [ 0.  1.  0.]\n",
      "Epoch 0/99\n",
      "----------\n",
      "LR is set to 0.05\n",
      "Variable containing:\n",
      "-2.0074\n",
      "-0.1140\n",
      "-0.1974\n",
      " 0.3888\n",
      "-0.6736\n",
      " 0.2404\n",
      "-1.6391\n",
      " 0.1642\n",
      "-0.1674\n",
      " 0.6457\n",
      "[torch.cuda.FloatTensor of size 10x1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1025  0.0933  0.0941  ...   0.0983  0.0927  0.1111\n",
      " 0.1047  0.0948  0.0944  ...   0.1025  0.0950  0.1072\n",
      " 0.1021  0.0927  0.0965  ...   0.1045  0.0949  0.1064\n",
      "          ...             ⋱             ...          \n",
      " 0.1040  0.0948  0.0939  ...   0.1008  0.0922  0.1090\n",
      " 0.0991  0.0941  0.0972  ...   0.1027  0.0940  0.1044\n",
      " 0.1043  0.0955  0.0946  ...   0.1007  0.0952  0.1086\n",
      "[torch.cuda.FloatTensor of size 256x10 (GPU 0)]\n",
      "\n",
      "Preds is Variable containing:\n",
      "-0.3485\n",
      "-0.3582\n",
      "-0.3581\n",
      "-0.3557\n",
      "-0.3569\n",
      "-0.3565\n",
      "-0.3537\n",
      "-0.3562\n",
      "-0.3538\n",
      "-0.3593\n",
      "-0.3631\n",
      "-0.3548\n",
      "-0.3626\n",
      "-0.3539\n",
      "-0.3641\n",
      "-0.3569\n",
      "-0.3429\n",
      "-0.3500\n",
      "-0.3584\n",
      "-0.3521\n",
      "-0.3549\n",
      "-0.3500\n",
      "-0.3553\n",
      "-0.3625\n",
      "-0.3547\n",
      "-0.3440\n",
      "-0.3671\n",
      "-0.3595\n",
      "-0.3572\n",
      "-0.3566\n",
      "-0.3550\n",
      "-0.3568\n",
      "-0.3552\n",
      "-0.3466\n",
      "-0.3515\n",
      "-0.3580\n",
      "-0.3507\n",
      "-0.3622\n",
      "-0.3522\n",
      "-0.3573\n",
      "-0.3533\n",
      "-0.3606\n",
      "-0.3545\n",
      "-0.3552\n",
      "-0.3532\n",
      "-0.3560\n",
      "-0.3558\n",
      "-0.3596\n",
      "-0.3607\n",
      "-0.3568\n",
      "-0.3500\n",
      "-0.3619\n",
      "-0.3456\n",
      "-0.3521\n",
      "-0.3497\n",
      "-0.3502\n",
      "-0.3576\n",
      "-0.3547\n",
      "-0.3538\n",
      "-0.3626\n",
      "-0.3592\n",
      "-0.3575\n",
      "-0.3506\n",
      "-0.3596\n",
      "-0.3520\n",
      "-0.3583\n",
      "-0.3605\n",
      "-0.3510\n",
      "-0.3510\n",
      "-0.3536\n",
      "-0.3537\n",
      "-0.3552\n",
      "-0.3535\n",
      "-0.3489\n",
      "-0.3636\n",
      "-0.3608\n",
      "-0.3519\n",
      "-0.3540\n",
      "-0.3575\n",
      "-0.3590\n",
      "-0.3533\n",
      "-0.3562\n",
      "-0.3479\n",
      "-0.3565\n",
      "-0.3520\n",
      "-0.3475\n",
      "-0.3591\n",
      "-0.3483\n",
      "-0.3597\n",
      "-0.3521\n",
      "-0.3544\n",
      "-0.3516\n",
      "-0.3426\n",
      "-0.3565\n",
      "-0.3523\n",
      "-0.3527\n",
      "-0.3632\n",
      "-0.3593\n",
      "-0.3567\n",
      "-0.3530\n",
      "-0.3582\n",
      "-0.3545\n",
      "-0.3480\n",
      "-0.3613\n",
      "-0.3507\n",
      "-0.3571\n",
      "-0.3498\n",
      "-0.3582\n",
      "-0.3532\n",
      "-0.3574\n",
      "-0.3580\n",
      "-0.3658\n",
      "-0.3562\n",
      "-0.3550\n",
      "-0.3553\n",
      "-0.3565\n",
      "-0.3539\n",
      "-0.3554\n",
      "-0.3443\n",
      "-0.3432\n",
      "-0.3557\n",
      "-0.3527\n",
      "-0.3523\n",
      "-0.3609\n",
      "-0.3617\n",
      "-0.3550\n",
      "-0.3551\n",
      "-0.3572\n",
      "-0.3582\n",
      "-0.3643\n",
      "-0.3462\n",
      "-0.3582\n",
      "-0.3487\n",
      "-0.3535\n",
      "-0.3621\n",
      "-0.3536\n",
      "-0.3597\n",
      "-0.3553\n",
      "-0.3498\n",
      "-0.3624\n",
      "-0.3547\n",
      "-0.3630\n",
      "-0.3564\n",
      "-0.3522\n",
      "-0.3460\n",
      "-0.3576\n",
      "-0.3596\n",
      "-0.3566\n",
      "-0.3567\n",
      "-0.3543\n",
      "-0.3546\n",
      "-0.3643\n",
      "-0.3516\n",
      "-0.3510\n",
      "-0.3547\n",
      "-0.3558\n",
      "-0.3575\n",
      "-0.3450\n",
      "-0.3524\n",
      "-0.3457\n",
      "-0.3568\n",
      "-0.3531\n",
      "-0.3567\n",
      "-0.3608\n",
      "-0.3522\n",
      "-0.3484\n",
      "-0.3672\n",
      "-0.3596\n",
      "-0.3602\n",
      "-0.3528\n",
      "-0.3526\n",
      "-0.3599\n",
      "-0.3584\n",
      "-0.3516\n",
      "-0.3520\n",
      "-0.3522\n",
      "-0.3599\n",
      "-0.3585\n",
      "-0.3533\n",
      "-0.3580\n",
      "-0.3579\n",
      "-0.3564\n",
      "-0.3524\n",
      "-0.3535\n",
      "-0.3665\n",
      "-0.3569\n",
      "-0.3565\n",
      "-0.3593\n",
      "-0.3604\n",
      "-0.3484\n",
      "-0.3562\n",
      "-0.3601\n",
      "-0.3568\n",
      "-0.3592\n",
      "-0.3587\n",
      "-0.3563\n",
      "-0.3555\n",
      "-0.3611\n",
      "-0.3581\n",
      "-0.3522\n",
      "-0.3583\n",
      "-0.3568\n",
      "-0.3759\n",
      "-0.3512\n",
      "-0.3576\n",
      "-0.3538\n",
      "-0.3520\n",
      "-0.3598\n",
      "-0.3633\n",
      "-0.3543\n",
      "-0.3587\n",
      "-0.3593\n",
      "-0.3570\n",
      "-0.3562\n",
      "-0.3649\n",
      "-0.3607\n",
      "-0.3645\n",
      "-0.3560\n",
      "-0.3551\n",
      "-0.3641\n",
      "-0.3504\n",
      "-0.3484\n",
      "-0.3557\n",
      "-0.3465\n",
      "-0.3590\n",
      "-0.3532\n",
      "-0.3570\n",
      "-0.3538\n",
      "-0.3473\n",
      "-0.3618\n",
      "-0.3533\n",
      "-0.3521\n",
      "-0.3554\n",
      "-0.3630\n",
      "-0.3565\n",
      "-0.3499\n",
      "-0.3602\n",
      "-0.3556\n",
      "-0.3522\n",
      "-0.3576\n",
      "-0.3543\n",
      "-0.3543\n",
      "-0.3561\n",
      "-0.3528\n",
      "-0.3560\n",
      "-0.3582\n",
      "-0.3500\n",
      "-0.3585\n",
      "-0.3539\n",
      "-0.3585\n",
      "-0.3560\n",
      "-0.3517\n",
      "-0.3571\n",
      "-0.3544\n",
      "-0.3531\n",
      "-0.3544\n",
      "[torch.cuda.FloatTensor of size 256x1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 8\n",
      " 0\n",
      " 6\n",
      " 0\n",
      " 9\n",
      " 0\n",
      " 7\n",
      " 5\n",
      " 9\n",
      " 4\n",
      " 3\n",
      " 6\n",
      " 0\n",
      " 8\n",
      " 6\n",
      " 2\n",
      " 7\n",
      " 8\n",
      " 2\n",
      " 6\n",
      " 2\n",
      " 7\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 9\n",
      " 0\n",
      " 6\n",
      " 5\n",
      " 7\n",
      " 9\n",
      " 6\n",
      " 6\n",
      " 0\n",
      " 8\n",
      " 0\n",
      " 2\n",
      " 8\n",
      " 6\n",
      " 3\n",
      " 6\n",
      " 1\n",
      " 4\n",
      " 2\n",
      " 7\n",
      " 6\n",
      " 0\n",
      " 2\n",
      " 3\n",
      " 8\n",
      " 9\n",
      " 3\n",
      " 4\n",
      " 4\n",
      " 9\n",
      " 8\n",
      " 7\n",
      " 9\n",
      " 1\n",
      " 7\n",
      " 1\n",
      " 6\n",
      " 8\n",
      " 6\n",
      " 9\n",
      " 2\n",
      " 0\n",
      " 7\n",
      " 9\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 9\n",
      " 4\n",
      " 1\n",
      " 3\n",
      " 7\n",
      " 9\n",
      " 2\n",
      " 0\n",
      " 3\n",
      " 2\n",
      " 5\n",
      " 2\n",
      " 3\n",
      " 8\n",
      " 2\n",
      " 3\n",
      " 6\n",
      " 6\n",
      " 4\n",
      " 7\n",
      " 6\n",
      " 5\n",
      " 8\n",
      " 8\n",
      " 4\n",
      " 4\n",
      " 7\n",
      " 6\n",
      " 4\n",
      " 5\n",
      " 2\n",
      " 5\n",
      " 9\n",
      " 0\n",
      " 0\n",
      " 5\n",
      " 8\n",
      " 5\n",
      " 5\n",
      " 4\n",
      " 2\n",
      " 8\n",
      " 9\n",
      " 8\n",
      " 5\n",
      " 0\n",
      " 9\n",
      " 5\n",
      " 5\n",
      " 9\n",
      " 4\n",
      " 3\n",
      " 0\n",
      " 0\n",
      " 8\n",
      " 5\n",
      " 4\n",
      " 1\n",
      " 9\n",
      " 2\n",
      " 5\n",
      " 8\n",
      " 3\n",
      " 8\n",
      " 6\n",
      " 3\n",
      " 7\n",
      " 2\n",
      " 8\n",
      " 1\n",
      " 1\n",
      " 8\n",
      " 8\n",
      " 3\n",
      " 2\n",
      " 0\n",
      " 6\n",
      " 7\n",
      " 5\n",
      " 1\n",
      " 9\n",
      " 4\n",
      " 8\n",
      " 7\n",
      " 0\n",
      " 8\n",
      " 9\n",
      " 0\n",
      " 3\n",
      " 0\n",
      " 5\n",
      " 1\n",
      " 8\n",
      " 7\n",
      " 3\n",
      " 0\n",
      " 4\n",
      " 5\n",
      " 9\n",
      " 3\n",
      " 3\n",
      " 1\n",
      " 6\n",
      " 2\n",
      " 0\n",
      " 3\n",
      " 5\n",
      " 1\n",
      " 3\n",
      " 2\n",
      " 3\n",
      " 3\n",
      " 0\n",
      " 2\n",
      " 2\n",
      " 0\n",
      " 2\n",
      " 6\n",
      " 2\n",
      " 1\n",
      " 6\n",
      " 4\n",
      " 0\n",
      " 4\n",
      " 6\n",
      " 4\n",
      " 6\n",
      " 6\n",
      " 5\n",
      " 6\n",
      " 7\n",
      " 0\n",
      " 8\n",
      " 5\n",
      " 3\n",
      " 2\n",
      " 7\n",
      " 5\n",
      " 0\n",
      " 0\n",
      " 2\n",
      " 0\n",
      " 4\n",
      " 2\n",
      " 5\n",
      " 3\n",
      " 3\n",
      " 2\n",
      " 2\n",
      " 8\n",
      " 6\n",
      " 9\n",
      " 6\n",
      " 2\n",
      " 6\n",
      " 9\n",
      " 7\n",
      " 8\n",
      " 6\n",
      " 8\n",
      " 8\n",
      " 0\n",
      " 8\n",
      " 0\n",
      " 4\n",
      " 1\n",
      " 7\n",
      " 0\n",
      " 3\n",
      " 5\n",
      " 4\n",
      " 7\n",
      " 4\n",
      " 3\n",
      " 1\n",
      " 4\n",
      " 8\n",
      " 3\n",
      " 2\n",
      " 6\n",
      " 6\n",
      " 7\n",
      " 4\n",
      " 2\n",
      "[torch.cuda.FloatTensor of size 256 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Variable' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-5a1e68e99e0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmulti_coeff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_coeff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mrun_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlmbda_mae\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-1d854c7fac37>\u001b[0m in \u001b[0;36mrun_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                             \u001b[0miter_loc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter_loc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcross_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msingle_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmulti_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumOut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                             \u001b[0mlogname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logs_regression.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                             multi_coeff = m_coeff, single_coeff = m_coeff, KL = KL, algo = algo)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;34m'''Save the models'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mtezcan/New Volume/amazon/notebook/functions/fine_tune.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, lr_scheduler, dset_loaders, dset_sizes, writer, use_gpu, num_epochs, batch_size, num_log, init_lr, lr_decay_epoch, regression, learn_a, cross_loss, multi_loss, numOut, logname, iter_loc, multi_coeff, single_coeff, KL, algo)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mtezcan/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fallthrough_methods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Variable' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "end_to_end = True\n",
    "optimizer='adam' #Optimizer function\n",
    "lr=0.05 #Initial learning rate\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "lr_scheduler=ft.exp_lr_scheduler #Learning rate scheduler\n",
    "lr_decay_epoch=10 #Number of epoch for learning rate decay\n",
    "\n",
    "\n",
    "hidden_sizes = [16, 16, 32, 32, 16, 16]#8, 16, 8, 4, 4]\n",
    "dropouts = []#.5, .5, .5]\n",
    "single_loss=1.0\n",
    "multi_loss =0.0\n",
    "\n",
    "KL = True\n",
    "metric = None\n",
    "\n",
    "for lmbda_mae in [.1*k for k in range(11)]:\n",
    "    multi_coeff = lmbda_mae * np.asarray(make_coeff(nclasses, 'ccr1', coeff_lmbda))\n",
    "    multi_coeff[int((len(multi_coeff)-1)/2)] = 1.\n",
    "    for k in range(10):\n",
    "        run_network()\n",
    "        \n",
    "for lmbda_mae in [.1*k for k in range(11)]:\n",
    "    multi_coeff = lmbda_mae * np.asarray(make_coeff(nclasses, 'mae', coeff_lmbda))\n",
    "    multi_coeff[int((len(multi_coeff)-1)/2)] = 1.\n",
    "    for k in range(10):\n",
    "        run_network()\n",
    "    \n",
    "'''KL = True\n",
    "metric = 'ccr'\n",
    "for k in range(10):\n",
    "    run_network()\n",
    "    \n",
    "metric = 'ccr1'\n",
    "for k in range(10):\n",
    "    run_network()\n",
    "    \n",
    "metric = 'mae'\n",
    "for k in range(10):\n",
    "    run_network()\n",
    "    \n",
    "metric = 'mse'\n",
    "for k in range(10):\n",
    "    run_network()'''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloader again, this time without shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fvec_norm = (fvec)/5\n",
    "mid_point = int(len(label)/2)#100*num_classes\n",
    "fvec_test = fvec_norm[rand_idx[:mid_point],:]\n",
    "fvec_train = fvec_norm[rand_idx[mid_point:],:]\n",
    "\n",
    "label_test = label[rand_idx[:mid_point]]\n",
    "label_train = label[rand_idx[mid_point:]]\n",
    "print(np.max(fvec_train))\n",
    "print(np.min(fvec_train))\n",
    "\n",
    "torch.from_numpy(label_train).type(torch.LongTensor)\n",
    "dsets={'train': torch.utils.data.TensorDataset(torch.from_numpy(fvec_train).type(torch.FloatTensor),\n",
    "                                               torch.from_numpy(label_train).type(torch.LongTensor)),\n",
    "       'val': torch.utils.data.TensorDataset(torch.from_numpy(fvec_test).type(torch.FloatTensor),\n",
    "                                             torch.from_numpy(label_test).type(torch.LongTensor))}\n",
    "\n",
    "'''Define dataset loaders'''\n",
    "dset_loaders = {'train':torch.utils.data.DataLoader(dsets['train'], batch_size=batch_size,shuffle=False,\n",
    "                                                    num_workers=12),\n",
    "                'val':torch.utils.data.DataLoader(dsets['val'], batch_size=batch_size,shuffle=False,\n",
    "                                                    num_workers=12)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_shape = 'Spiral'\n",
    "data_date = '18_01_31'\n",
    "data_dir = './saved_models_github/' + data_shape + '/' + data_date\n",
    "\n",
    "run_dirs = sorted(os.listdir(data_dir))\n",
    "last_dirs = run_dirs[1::2]\n",
    "ccr1_dirs = last_dirs[:110]\n",
    "mae_dirs = last_dirs[110:]\n",
    "all_dirs = [ccr1_dirs, mae_dirs]\n",
    "\n",
    "\n",
    "'''run_dirs = sorted(os.listdir('./saved_models/test'))\n",
    "last_dirs = run_dirs[1::2]\n",
    "ccr1_dirs = last_dirs[:100]\n",
    "mae_dirs = last_dirs[100:]\n",
    "\n",
    "pure_ccr1 = ['Ordinal_January24  14:10:01_last',\n",
    "             'Ordinal_January24  14:11:08_last',\n",
    "             'Ordinal_January24  14:12:14_last',\n",
    "             'Ordinal_January24  14:13:21_last',\n",
    "             'Ordinal_January24  14:14:27_last',\n",
    "             'Ordinal_January24  14:15:34_last',\n",
    "             'Ordinal_January24  14:16:40_last',\n",
    "             'Ordinal_January24  14:17:47_last',\n",
    "             'Ordinal_January24  14:18:54_last',\n",
    "             'Ordinal_January24  14:20:00_last',]\n",
    "\n",
    "pure_mae = ['Ordinal_January24  14:21:07_last',\n",
    "             'Ordinal_January24  14:22:13_last',\n",
    "             'Ordinal_January24  14:23:19_last',\n",
    "             'Ordinal_January24  14:24:26_last',\n",
    "             'Ordinal_January24  14:25:32_last',\n",
    "             'Ordinal_January24  14:26:38_last',\n",
    "             'Ordinal_January24  14:27:45_last',\n",
    "             'Ordinal_January24  14:28:52_last',\n",
    "             'Ordinal_January24  14:29:58_last',\n",
    "             'Ordinal_January24  14:31:05_last',]'''\n",
    "                \n",
    "len(mae_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.load('./saved_models/ord/Ordinal_January24  10:24:20_last', map_location={'cuda:0': 'cpu'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(model_dir, phase='train'):\n",
    "    if use_gpu:\n",
    "        model = torch.load(model_dir)\n",
    "    else:\n",
    "        model = torch.load(model_dir, map_location={'cuda:0': 'cpu'})\n",
    "    model.train(False)\n",
    "\n",
    "    labels_arr = np.asarray([]);\n",
    "    preds_arr = np.asarray([]);\n",
    "    for data in dset_loaders[phase]:\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            outputs = np.argmax(model(inputs).cpu().data.numpy(), axis=1)\n",
    "            #labels_arr = np.append(labels_arr,labels.cpu().data.numpy())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = np.argmax(model(inputs).data.numpy(), axis=1)\n",
    "            #labels_arr = np.append(labels_arr,labels.data.numpy())\n",
    "          \n",
    "        preds_arr = np.append(preds_arr, outputs)\n",
    "    return preds_arr\n",
    "#pred_tr = validate('./saved_models/ord/Ordinal_January24  10:24:20_last')\n",
    "#print(np.min(label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(18,15))\n",
    "\n",
    "for k in range(5):\n",
    "    plt.subplot(3,5,k+1)\n",
    "    pred_tr = validate(data_dir + '/' + last_dirs[k])\n",
    "    plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "    #plt.scatter(fvec_train[:, 0], fvec_train[:, 1], c=label_train,vmin=-1, vmax=num_classes)\n",
    "    plt.colorbar()\n",
    "    ccr = np.mean(pred_tr==label_train)\n",
    "    ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "    mae = np.mean(np.abs(pred_tr-label_train))\n",
    "    rmse = np.mean((pred_tr-label_train)**2)\n",
    "    plt.title('$CCR$ loss' + \n",
    "              ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "              ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)))\n",
    "    \n",
    "for k in range(5):\n",
    "    plt.subplot(3,5,k+6)\n",
    "    pred_tr = validate(data_dir + '/' + last_dirs[50+k])\n",
    "    plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "    #plt.scatter(fvec_train[:, 0], fvec_train[:, 1], c=label_train,vmin=-1, vmax=num_classes)\n",
    "    plt.colorbar()\n",
    "    ccr = np.mean(pred_tr==label_train)\n",
    "    ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "    mae = np.mean(np.abs(pred_tr-label_train))\n",
    "    rmse = np.mean((pred_tr-label_train)**2)\n",
    "    plt.title('$0.5CCR$ loss + $0.5CCR_1$ loss \\n' + \n",
    "              ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "              ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)))\n",
    "    \n",
    "for k in range(5):\n",
    "    plt.subplot(3,5,k+11)\n",
    "    pred_tr = validate(data_dir + '/' + last_dirs[100+k])\n",
    "    plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "    #plt.scatter(fvec_train[:, 0], fvec_train[:, 1], c=label_train,vmin=-1, vmax=num_classes)\n",
    "    plt.colorbar()\n",
    "    ccr = np.mean(pred_tr==label_train)\n",
    "    ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "    mae = np.mean(np.abs(pred_tr-label_train))\n",
    "    rmse = np.mean((pred_tr-label_train)**2)\n",
    "    plt.title('$CCR_1$ loss ' + \n",
    "              ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "              ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)))\n",
    "    \n",
    "plt.savefig('variance_of_results.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_and_mean(root_dir, sub_dirs, phase='train'):\n",
    "    scores_arr = np.zeros((label_train.shape[0],9))\n",
    "    for sub_dir in sub_dirs:\n",
    "        if use_gpu:\n",
    "            model = torch.load(root_dir + '/' + sub_dir)\n",
    "        else:\n",
    "            model = torch.load(root_dir + '/' + sub_dir, map_location={'cuda:0': 'cpu'})\n",
    "        model.train(False)\n",
    "        #print(model)\n",
    "        score_arr = np.zeros((1,9))\n",
    "        for data in dset_loaders[phase]:\n",
    "            inputs, labels = data\n",
    "            if use_gpu:\n",
    "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                outputs = model(inputs).cpu().data.numpy()\n",
    "                #print(outputs.shape)\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "                outputs = model(inputs).data.numpy()\n",
    "                \n",
    "            score_arr = np.append(score_arr, outputs, axis=0)\n",
    "        scores_arr += score_arr[1:,:]\n",
    "        \n",
    "    return scores_arr\n",
    "#scores_tr = validate_and_mean('./saved_models/test_circular', last_dirs[:10])\n",
    "#pred_tr = np.argmax(scores_tr, axis=1)\n",
    "#print(np.mean(label_train == pred_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dirs = ['./saved_models/ord/Ordinal_January24  10:16:00_last',\n",
    "             './saved_models/ord/Ordinal_January24  10:20:35_last',\n",
    "             './saved_models/ord/Ordinal_January24  10:24:20_last',\n",
    "             './saved_models/ord/Ordinal_January24  10:28:03_last']\n",
    "\n",
    "'''model_dirs = ['./saved_models/ord/Ordinal_January24  13:09:17_last',\n",
    "             './saved_models/ord/Ordinal_January24  13:18:08_last',\n",
    "             './saved_models/ord/Ordinal_January24  13:27:43_last',\n",
    "             './saved_models/ord/Ordinal_January24  13:43:43_last']'''\n",
    "\n",
    "model_dirs = ['./saved_models/ord/Ordinal_January26  00:22:14_last',\n",
    "              './saved_models/ord/Ordinal_January26  00:19:20_last',\n",
    "              './saved_models/ord/Ordinal_January26  00:09:08_last',\n",
    "              './saved_models/ord/Ordinal_January26  00:17:53_last']\n",
    "preds = []\n",
    "\n",
    "for model_dir in model_dirs:\n",
    "    preds.append(validate(model_dir))\n",
    "    \n",
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric = 'CCR1'\n",
    "\n",
    "if metric is 'CCR1':\n",
    "    metric_code = 0\n",
    "elif metric is 'MAE':\n",
    "    metric_code = 1\n",
    "else:\n",
    "    print('Wrong metric')\n",
    "    \n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "plt.subplot(4,3,1)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=label_train, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "plt.title('Ground Truth')\n",
    "\n",
    "metrics = np.zeros((11, 4))\n",
    "for k in range(10):\n",
    "    scores_tr = validate_and_mean(data_dir, all_dirs[metric_code][k*10:(k+1)*10])\n",
    "    pred_tr = np.argmax(scores_tr, axis=1)\n",
    "    plt.subplot(4,3,k+2)\n",
    "    plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "    plt.colorbar()\n",
    "    ccr = np.mean(pred_tr==label_train)\n",
    "    ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "    mae = np.mean(np.abs(pred_tr-label_train))\n",
    "    rmse = np.mean((pred_tr-label_train)**2)\n",
    "    metrics[k,:] = [ccr,ccr1,mae,rmse]\n",
    "    plt.title('$\\lambda$=' + str(np.round(k*.1, decimals=1)) + \n",
    "              ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "              ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)) +\n",
    "              ', $MAE$=' + str(np.round(mae, decimals = 2)) +\n",
    "              ', $RMSE$=' + str(np.round(rmse, decimals = 2)))\n",
    "\n",
    "    \n",
    "#pred_tr = validate('./saved_models/ord/Ordinal_January24  10:20:35_last')\n",
    "#pred_tr = validate('./saved_models/ord/Ordinal_January24  13:18:08_last')\n",
    "\n",
    "scores_tr = validate_and_mean(data_dir, all_dirs[metric_code][100:])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(4,3,12)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "ccr = np.mean(pred_tr==label_train)\n",
    "ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "mae = np.mean(np.abs(pred_tr-label_train))\n",
    "rmse = np.mean((pred_tr-label_train)**2)\n",
    "metrics[10,:] = [ccr,ccr1,mae,rmse]\n",
    "plt.title('$\\lambda$=1.0' + \n",
    "          ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "          ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)) +\n",
    "          ', $MAE$=' + str(np.round(mae, decimals = 2)) +\n",
    "          ', $RMSE$=' + str(np.round(rmse, decimals = 2)))\n",
    "\n",
    "plt_title = data_shape + '_Data_CCR_' + metric + '_tradeoff_' + data_date + '.tiff'\n",
    "plt.savefig(plt_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(metrics)\n",
    "\n",
    "lmbdas = [.1*k for k in range(11)]\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(221)\n",
    "plt.plot(lmbdas, metrics[:,0], 'o-')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('$CCR$')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(lmbdas, metrics[:,1], 'o-')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('$CCR_1$')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(lmbdas, metrics[:,2], 'o-')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('$MAE$')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(lmbdas, metrics[:,3], 'o-')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('$RMSE$')\n",
    "\n",
    "plt.savefig('spiral_plots_ccr1.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=label_train, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "plt.title('Ground Truth')\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/test_circular', ccr1_dirs[:10])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,2)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "ccr = np.mean(pred_tr==label_train)\n",
    "ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "mae = np.mean(np.abs(pred_tr-label_train))\n",
    "rmse = np.mean((pred_tr-label_train)**2)\n",
    "plt.title('$CCR loss$'  + \n",
    "          ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "          ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)) +\n",
    "          ',\\n $MAE$=' + str(np.round(mae, decimals = 2)) +\n",
    "          ', $RMSE$=' + str(np.round(rmse, decimals = 2)))\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/test_circular', ccr1_dirs[100:])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,3)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "ccr = np.mean(pred_tr==label_train)\n",
    "ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "mae = np.mean(np.abs(pred_tr-label_train))\n",
    "rmse = np.mean((pred_tr-label_train)**2)\n",
    "plt.title('$CCR_1 loss$'  + \n",
    "          ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "          ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)) +\n",
    "          ',\\n $MAE$=' + str(np.round(mae, decimals = 2)) +\n",
    "          ', $RMSE$=' + str(np.round(rmse, decimals = 2)))\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/ord', mae_dirs[100:])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,4)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=pred_tr, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "ccr = np.mean(pred_tr==label_train)\n",
    "ccr1 = np.mean(np.abs(pred_tr-label_train)<=1)\n",
    "mae = np.mean(np.abs(pred_tr-label_train))\n",
    "rmse = np.mean((pred_tr-label_train)**2)\n",
    "plt.title('$MAE loss$'  + \n",
    "          ', $CCR$=' + str(np.round(ccr, decimals = 2)) +\n",
    "          ', $CCR_1$=' + str(np.round(ccr1, decimals = 2)) +\n",
    "          ',\\n$MAE$=' + str(np.round(mae, decimals = 2)) +\n",
    "          ', $RMSE$=' + str(np.round(rmse, decimals = 2)))\n",
    "\n",
    "plt.savefig('spiral_extreme.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.scatter(fvec_train[:, 0], fvec_train[:, 1], s=15, c=label_train, cmap = plt.get_cmap('Set1'),vmin=0, vmax=num_classes-1)\n",
    "plt.colorbar()\n",
    "plt.title('Ground Truth')\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/test_circular', ccr1_dirs[:10])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,2)\n",
    "plt.hist(pred_tr-label_train, bins = np.arange(-4.5,5.5,1))\n",
    "plt.title('$CCR$ loss')\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/test_circular', ccr1_dirs[100:])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,3)\n",
    "plt.hist(pred_tr-label_train, bins = np.arange(-4.5,5.5,1))\n",
    "plt.title('$CCR_1$ loss')\n",
    "\n",
    "scores_tr = validate_and_mean('./saved_models/test_circular', mae_dirs[100:])\n",
    "pred_tr = np.argmax(scores_tr, axis=1)\n",
    "plt.subplot(1,4,4)\n",
    "plt.hist(pred_tr-label_train, bins = np.arange(-4.5,5.5,1))\n",
    "plt.title('$MAE$ loss')\n",
    "\n",
    "plt.savefig('circular_extreme_hist.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.subplot(211)\n",
    "img = mpimg.imread('Circular_Data_Extreme_Weights.eps')\n",
    "plt.imshow(img)\n",
    "plt.subplot(212)\n",
    "img = mpimg.imread('Spiral_Data_Extreme_Weights.eps')\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
